{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "water_gan_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FXS8l4uurbuO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FHffsU9Qrs-K",
        "colab_type": "code",
        "outputId": "7bd89837-8310-4a81-ef05-d61a9255f178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import pickle\n",
        "import copy\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.ConfigProto()\n",
        "#config.gpu_options.allow_growth = True\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "session = tf.Session(config=config)\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.backend.tensorflow_backend import set_session"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "r1ZL-FDtx67z",
        "colab_type": "code",
        "outputId": "633043dd-35ac-47c3-e76d-b2ef1246e4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "py4crw0errbL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GAN():\n",
        "    def __init__(self,dataset_name='',load_model_name=''):\n",
        "        \n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        \n",
        "        if (load_model_name == ''):\n",
        "            #X_train = self.load_gan_data(dataset_name)\n",
        "            \n",
        "            print(\"Loading dataset...\")\n",
        "            \n",
        "            ndata = 200\n",
        "            patch_size = 32\n",
        "            \n",
        "            self.X_tr = np.zeros((ndata, patch_size, patch_size, 3))\n",
        "            \n",
        "            for i in range(ndata):\n",
        "              if(i%10==0):\n",
        "                print(\"Image \",i+1)\n",
        "              im = Image.open(\"drive/My Drive/texture/patchset3/patchno\"+str(i)+\".jpg\")\n",
        "              self.X_tr[i,:,:,:] = np.array(im)\n",
        "            \n",
        "            print(\"Dataset loaded.\")\n",
        "            \n",
        "            # default parameters for mnist \n",
        "            self.img_rows = self.X_tr.shape[1]\n",
        "            self.img_cols = self.X_tr.shape[2]\n",
        "            self.img_channels = self.X_tr.shape[3]\n",
        "            self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
        "            self.z_dim = 49\n",
        "            self.iter_count = 0\n",
        "            self.dataset_name = dataset_name\n",
        "            self.model_file = 'models/'+self.dataset_name+'_gan_model.pickle'#\n",
        "\n",
        "            # Build and compile the discriminator and discriminator loss\n",
        "            self.discriminator = self.build_discriminator()\n",
        "            # set discriminator loss\n",
        "            # BEGIN INSERT CODE\n",
        "            self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            # END INSERT CODE\n",
        "\n",
        "            # Build the generator\n",
        "            self.generator = self.build_generator()\n",
        "\n",
        "        else:\n",
        "            #load gan class and models (generator, discriminator and stacked model)\n",
        "            self.load_gan_model(load_model_name)\n",
        "\n",
        "        # Create the stacked model\n",
        "        #first, create the random vector z in the latent space\n",
        "        z = Input(shape=(self.z_dim,))\n",
        "        #create generated (fake) image\n",
        "        img = self.generator(z)\n",
        "\n",
        "        #indicate that for the stacked model, the weights are not trained\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and gives a probability of whether it is a true or\n",
        "        #false image\n",
        "        p_true = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # In this model, we train the generator only\n",
        "        self.stacked_gen_disc = Model(z, p_true)\n",
        "\n",
        "        # loss\n",
        "        # START INSERT CODE HERE\n",
        "        generator_loss = K.mean(K.log(1 - p_true))\n",
        "        # END INSERT CODE HERE\n",
        "        self.stacked_gen_disc.add_loss(generator_loss)\n",
        "        self.stacked_gen_disc.compile(optimizer=optimizer)\n",
        "\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        z_rand = Input(shape=(self.z_dim,))\n",
        "\n",
        "        # START INSERT CODE HERE\n",
        "        out = Dense(256)(z_rand)\n",
        "        out = LeakyReLU(0.2)(out)\n",
        "        out = Dense(512)(out)\n",
        "        out = LeakyReLU(0.2)(out)\n",
        "        out = Dense(3072)(out)\n",
        "        out = Activation('tanh')(out)\n",
        "        out = Reshape((32,32,3))(out)\n",
        "        output_img = out\n",
        "        # END INSERT CODE HERE\n",
        "\n",
        "        model_generator = Model(z_rand, output_img)\n",
        "        model_generator.summary()\n",
        "\n",
        "        return model_generator\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        input_img = Input(shape=self.img_shape)\n",
        "        \n",
        "        out = Flatten()(input_img)\n",
        "        out = Dense(512)(out)\n",
        "        out = LeakyReLU(0.2)(out)\n",
        "        out = Dense(256)(out)\n",
        "        out = LeakyReLU(0.2)(out)\n",
        "        out = Dense(1)(out)\n",
        "        out = Activation('sigmoid')(out)\n",
        "        p_true = out\n",
        "        \n",
        "        model_discriminator = Model(input_img, p_true)\n",
        "        model_discriminator.summary()\n",
        "\n",
        "        return model_discriminator\n",
        "\n",
        "    def load_gan_data(self,dataset_name):\n",
        "        # Load the dataset\n",
        "        if(dataset_name == 'mnist'):\n",
        "            (X_train, _), (_, _) = mnist.load_data()\n",
        "        elif(dataset_name == 'cifar'):\n",
        "            from keras.datasets import cifar10\n",
        "            (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "        else:\n",
        "            print('Error, unknown database')\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        #add a channel dimension, if need be (for mnist data)\n",
        "        if(X_train.ndim ==3):\n",
        "            X_train = np.expand_dims(X_train, axis=3)\n",
        "        return X_train\n",
        "\n",
        "    def save_gan_model(self, model_file):\n",
        "\n",
        "        #save the GAN class instance\n",
        "        gan_temp = GAN(self.dataset_name,'')\n",
        "        gan_temp.generator = self.generator\n",
        "        gan_temp.discriminator = self.discriminator\n",
        "        gan_temp.stacked_gen_disc = []\n",
        "        gan_temp.iter_count = self.iter_count\n",
        "        with open(model_file,'wb') as file_class:\n",
        "            pickle.dump(gan_temp,file_class,-1)\n",
        "\n",
        "    def load_gan_model(self, model_file):\n",
        "\n",
        "        #load GAN class instance\n",
        "        gan_temp = pickle.load(open(model_file,\"rb\",-1))\n",
        "        #copy parameters\n",
        "        self.img_rows = gan_temp.img_rows \n",
        "        self.img_cols = gan_temp.img_cols \n",
        "        self.img_channels = gan_temp.img_channels \n",
        "        self.img_shape = gan_temp.img_shape\n",
        "        self.z_dim = gan_temp.z_dim\n",
        "        self.iter_count = gan_temp.iter_count\n",
        "        self.model_file = gan_temp.model_file\n",
        "        self.dataset_name = gan_temp.dataset_name\n",
        "\n",
        "        #copy models\n",
        "        self.generator = gan_temp.generator\n",
        "        self.discriminator = gan_temp.discriminator\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        k=1    #number of internal loops\n",
        "\n",
        "        #load dataset\n",
        "        X_train = self.X_tr\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        d_output_true = np.ones((batch_size, 1))\n",
        "        d_output_false = np.zeros((batch_size, 1))\n",
        "\n",
        "        first_iter =self.iter_count\n",
        "\n",
        "        for epoch in range(first_iter,epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the discriminator\n",
        "            for i in range(0,k):\n",
        "                # Select a random batch of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "                imgs = X_train[idx]\n",
        "\n",
        "                z_random = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
        "\n",
        "                # Generate a batch of new (fake) images\n",
        "                gen_imgs = self.generator.predict(z_random)\n",
        "                \n",
        "                # START INSERT CODE\n",
        "                d_loss_real = self.discriminator.train_on_batch(imgs, d_output_true)\n",
        "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, d_output_false)\n",
        "                # END INSERT CODE\n",
        "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            \n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            z_random = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
        "\n",
        "            # Generate a batch of new (fake) images\n",
        "            gen_imgs = self.generator.predict(z_random)\n",
        "            # Generator training : try to make generated images be classified as true by the discriminator\n",
        "            g_loss = self.stacked_gen_disc.train_on_batch(z_random,None)\n",
        "\n",
        "            # increase epoch counter\n",
        "            self.iter_count = self.iter_count+1\n",
        "            # Plot the losses\n",
        "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # Save some random generated images and the models at every sample_interval iterations\n",
        "            #if (epoch % sample_interval == 0):\n",
        "            #    self.sample_images('images/'+self.dataset_name+'_sample_%06d.png' % epoch)\n",
        "            #    self.save_gan_model(self.model_file)\n",
        "\n",
        "    def sample_images(self, image_filename, rand_seed=30):\n",
        "        np.random.seed(rand_seed)\n",
        "\n",
        "        r, c = 5, 5\n",
        "        z_random = np.random.normal(0, 1, (r * c, self.z_dim))\n",
        "        gen_imgs = self.generator.predict(z_random)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                #black and white images\n",
        "                if(gen_imgs.shape[3] == 1):\n",
        "                    axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                elif(gen_imgs.shape[3] == 3):   #colour images\n",
        "                    axs[i,j].imshow(gen_imgs[cnt, :,:])\n",
        "                else:\n",
        "                    print('Error, unsupported channel size. Dude, I don''t know what you want me to do.\\\n",
        "                            I can''t handle this data. You''ve made me very sad ...')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(image_filename)\n",
        "        plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQHAcAS-r3Fq",
        "colab_type": "code",
        "outputId": "0e8a8293-d6f8-4402-ba94-0f5a15ceed5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15710
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    #create the output image and model directories\n",
        "    if (os.path.isdir('images')==0):\n",
        "        os.mkdir('images')\n",
        "    if (os.path.isdir('models')==0):\n",
        "        os.mkdir('models')\n",
        "\n",
        "    #choose dataset\n",
        "    dataset_name = 'mnist'#\n",
        "\n",
        "    #create GAN model\n",
        "    set_session(session)\n",
        "\n",
        "    #create GAN model\n",
        "    model_file = ''#'models/'+dataset_name+'_gan_model.pickle'#\n",
        "    gan = GAN()#,\n",
        "    is_training = 1\n",
        "\n",
        "    if (is_training ==1):\n",
        "        gan.train(epochs=5001, batch_size=64, sample_interval=500)\n",
        "    else:\n",
        "        gan.sample_images('images/test_images.png')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Loading dataset...\n",
            "Image  1\n",
            "Image  11\n",
            "Image  21\n",
            "Image  31\n",
            "Image  41\n",
            "Image  51\n",
            "Image  61\n",
            "Image  71\n",
            "Image  81\n",
            "Image  91\n",
            "Image  101\n",
            "Image  111\n",
            "Image  121\n",
            "Image  131\n",
            "Image  141\n",
            "Image  151\n",
            "Image  161\n",
            "Image  171\n",
            "Image  181\n",
            "Image  191\n",
            "Dataset loaded.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 1,704,961\n",
            "Trainable params: 1,704,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 49)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               12800     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3072)              1575936   \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 32, 32, 3)         0         \n",
            "=================================================================\n",
            "Total params: 1,720,320\n",
            "Trainable params: 1,720,320\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 8.390385, acc.: 39.84%] [G loss: -0.471939]\n",
            "1 [D loss: 8.333504, acc.: 50.00%] [G loss: -0.393427]\n",
            "2 [D loss: 8.311960, acc.: 50.00%] [G loss: -0.348705]\n",
            "3 [D loss: 8.302606, acc.: 50.00%] [G loss: -0.336402]\n",
            "4 [D loss: 8.294056, acc.: 50.00%] [G loss: -0.316614]\n",
            "5 [D loss: 8.293243, acc.: 50.00%] [G loss: -0.303787]\n",
            "6 [D loss: 8.285602, acc.: 50.00%] [G loss: -0.304732]\n",
            "7 [D loss: 8.290751, acc.: 50.00%] [G loss: -0.294390]\n",
            "8 [D loss: 8.289156, acc.: 50.00%] [G loss: -0.295535]\n",
            "9 [D loss: 8.303288, acc.: 50.00%] [G loss: -0.297687]\n",
            "10 [D loss: 8.305524, acc.: 50.00%] [G loss: -0.286905]\n",
            "11 [D loss: 8.302393, acc.: 49.22%] [G loss: -0.271613]\n",
            "12 [D loss: 8.293241, acc.: 50.00%] [G loss: -0.241936]\n",
            "13 [D loss: 8.254276, acc.: 50.00%] [G loss: -0.213398]\n",
            "14 [D loss: 8.231875, acc.: 50.00%] [G loss: -0.178448]\n",
            "15 [D loss: 8.207863, acc.: 50.00%] [G loss: -0.167121]\n",
            "16 [D loss: 8.198278, acc.: 50.00%] [G loss: -0.147365]\n",
            "17 [D loss: 8.186814, acc.: 50.00%] [G loss: -0.141727]\n",
            "18 [D loss: 8.188154, acc.: 50.00%] [G loss: -0.135273]\n",
            "19 [D loss: 8.179141, acc.: 50.00%] [G loss: -0.144112]\n",
            "20 [D loss: 8.182681, acc.: 50.00%] [G loss: -0.161401]\n",
            "21 [D loss: 8.190277, acc.: 50.00%] [G loss: -0.178464]\n",
            "22 [D loss: 8.212935, acc.: 50.00%] [G loss: -0.176984]\n",
            "23 [D loss: 8.225046, acc.: 50.00%] [G loss: -0.196282]\n",
            "24 [D loss: 8.225865, acc.: 50.00%] [G loss: -0.202301]\n",
            "25 [D loss: 8.219391, acc.: 50.00%] [G loss: -0.199386]\n",
            "26 [D loss: 8.208995, acc.: 50.00%] [G loss: -0.175831]\n",
            "27 [D loss: 8.195568, acc.: 50.00%] [G loss: -0.191259]\n",
            "28 [D loss: 8.191326, acc.: 50.00%] [G loss: -0.177489]\n",
            "29 [D loss: 8.188593, acc.: 50.00%] [G loss: -0.182811]\n",
            "30 [D loss: 8.181757, acc.: 50.00%] [G loss: -0.178505]\n",
            "31 [D loss: 8.195457, acc.: 50.00%] [G loss: -0.179130]\n",
            "32 [D loss: 8.183447, acc.: 50.00%] [G loss: -0.161450]\n",
            "33 [D loss: 8.176486, acc.: 50.00%] [G loss: -0.162928]\n",
            "34 [D loss: 8.169794, acc.: 50.00%] [G loss: -0.159804]\n",
            "35 [D loss: 8.168107, acc.: 50.00%] [G loss: -0.154521]\n",
            "36 [D loss: 8.169205, acc.: 50.00%] [G loss: -0.166384]\n",
            "37 [D loss: 8.161727, acc.: 50.00%] [G loss: -0.156842]\n",
            "38 [D loss: 8.163988, acc.: 50.00%] [G loss: -0.165274]\n",
            "39 [D loss: 8.170694, acc.: 50.00%] [G loss: -0.157622]\n",
            "40 [D loss: 8.163331, acc.: 50.00%] [G loss: -0.153262]\n",
            "41 [D loss: 8.167070, acc.: 50.00%] [G loss: -0.144694]\n",
            "42 [D loss: 8.154993, acc.: 50.00%] [G loss: -0.140562]\n",
            "43 [D loss: 8.145515, acc.: 50.00%] [G loss: -0.117139]\n",
            "44 [D loss: 8.141099, acc.: 50.00%] [G loss: -0.108457]\n",
            "45 [D loss: 8.134727, acc.: 50.00%] [G loss: -0.103377]\n",
            "46 [D loss: 8.131849, acc.: 50.00%] [G loss: -0.112495]\n",
            "47 [D loss: 8.133497, acc.: 50.00%] [G loss: -0.113390]\n",
            "48 [D loss: 8.134609, acc.: 50.00%] [G loss: -0.105670]\n",
            "49 [D loss: 8.137289, acc.: 50.00%] [G loss: -0.112153]\n",
            "50 [D loss: 8.133492, acc.: 50.00%] [G loss: -0.101906]\n",
            "51 [D loss: 8.132073, acc.: 50.00%] [G loss: -0.100411]\n",
            "52 [D loss: 8.133995, acc.: 50.00%] [G loss: -0.104032]\n",
            "53 [D loss: 8.131749, acc.: 50.00%] [G loss: -0.094778]\n",
            "54 [D loss: 8.125572, acc.: 50.00%] [G loss: -0.089343]\n",
            "55 [D loss: 8.122566, acc.: 50.00%] [G loss: -0.082917]\n",
            "56 [D loss: 8.117219, acc.: 50.00%] [G loss: -0.077308]\n",
            "57 [D loss: 8.114400, acc.: 50.00%] [G loss: -0.078004]\n",
            "58 [D loss: 8.110878, acc.: 50.00%] [G loss: -0.077336]\n",
            "59 [D loss: 8.112036, acc.: 50.00%] [G loss: -0.080714]\n",
            "60 [D loss: 8.114637, acc.: 50.00%] [G loss: -0.084980]\n",
            "61 [D loss: 8.117844, acc.: 50.00%] [G loss: -0.080552]\n",
            "62 [D loss: 8.112757, acc.: 50.00%] [G loss: -0.080339]\n",
            "63 [D loss: 8.115030, acc.: 50.00%] [G loss: -0.078689]\n",
            "64 [D loss: 8.114576, acc.: 50.00%] [G loss: -0.074435]\n",
            "65 [D loss: 8.108105, acc.: 50.00%] [G loss: -0.061328]\n",
            "66 [D loss: 8.100608, acc.: 50.00%] [G loss: -0.059693]\n",
            "67 [D loss: 8.101086, acc.: 50.00%] [G loss: -0.053883]\n",
            "68 [D loss: 8.095427, acc.: 50.00%] [G loss: -0.053953]\n",
            "69 [D loss: 8.093436, acc.: 50.00%] [G loss: -0.050130]\n",
            "70 [D loss: 8.094531, acc.: 50.00%] [G loss: -0.048467]\n",
            "71 [D loss: 8.094069, acc.: 50.00%] [G loss: -0.048682]\n",
            "72 [D loss: 8.089636, acc.: 50.00%] [G loss: -0.047372]\n",
            "73 [D loss: 8.092126, acc.: 50.00%] [G loss: -0.045957]\n",
            "74 [D loss: 8.091135, acc.: 50.00%] [G loss: -0.046562]\n",
            "75 [D loss: 8.090316, acc.: 50.00%] [G loss: -0.047080]\n",
            "76 [D loss: 8.089317, acc.: 50.00%] [G loss: -0.043939]\n",
            "77 [D loss: 8.087957, acc.: 50.00%] [G loss: -0.038204]\n",
            "78 [D loss: 8.086473, acc.: 50.00%] [G loss: -0.037946]\n",
            "79 [D loss: 8.083586, acc.: 50.00%] [G loss: -0.035391]\n",
            "80 [D loss: 8.083455, acc.: 50.00%] [G loss: -0.035997]\n",
            "81 [D loss: 8.082974, acc.: 50.00%] [G loss: -0.033982]\n",
            "82 [D loss: 8.079789, acc.: 50.00%] [G loss: -0.035414]\n",
            "83 [D loss: 8.080690, acc.: 50.00%] [G loss: -0.031819]\n",
            "84 [D loss: 8.080606, acc.: 50.00%] [G loss: -0.033575]\n",
            "85 [D loss: 8.081034, acc.: 50.00%] [G loss: -0.032431]\n",
            "86 [D loss: 8.079625, acc.: 50.00%] [G loss: -0.029490]\n",
            "87 [D loss: 8.077346, acc.: 50.00%] [G loss: -0.034733]\n",
            "88 [D loss: 8.081980, acc.: 50.00%] [G loss: -0.026295]\n",
            "89 [D loss: 8.078246, acc.: 50.00%] [G loss: -0.031866]\n",
            "90 [D loss: 8.079135, acc.: 50.00%] [G loss: -0.029462]\n",
            "91 [D loss: 8.077150, acc.: 50.00%] [G loss: -0.034505]\n",
            "92 [D loss: 8.077261, acc.: 50.00%] [G loss: -0.030476]\n",
            "93 [D loss: 8.077174, acc.: 50.00%] [G loss: -0.024902]\n",
            "94 [D loss: 8.075832, acc.: 50.00%] [G loss: -0.027673]\n",
            "95 [D loss: 8.075210, acc.: 50.00%] [G loss: -0.026849]\n",
            "96 [D loss: 8.074095, acc.: 50.00%] [G loss: -0.021447]\n",
            "97 [D loss: 8.072934, acc.: 50.00%] [G loss: -0.021957]\n",
            "98 [D loss: 8.073212, acc.: 50.00%] [G loss: -0.019467]\n",
            "99 [D loss: 8.071126, acc.: 50.00%] [G loss: -0.017051]\n",
            "100 [D loss: 8.069432, acc.: 50.00%] [G loss: -0.017678]\n",
            "101 [D loss: 8.068527, acc.: 50.00%] [G loss: -0.015867]\n",
            "102 [D loss: 8.068550, acc.: 50.00%] [G loss: -0.013868]\n",
            "103 [D loss: 8.067871, acc.: 50.00%] [G loss: -0.014988]\n",
            "104 [D loss: 8.067555, acc.: 50.00%] [G loss: -0.014518]\n",
            "105 [D loss: 8.067643, acc.: 50.00%] [G loss: -0.015422]\n",
            "106 [D loss: 8.068118, acc.: 50.00%] [G loss: -0.013194]\n",
            "107 [D loss: 8.065929, acc.: 50.00%] [G loss: -0.014412]\n",
            "108 [D loss: 8.067112, acc.: 50.00%] [G loss: -0.014334]\n",
            "109 [D loss: 8.066594, acc.: 50.00%] [G loss: -0.011495]\n",
            "110 [D loss: 8.066866, acc.: 50.00%] [G loss: -0.013567]\n",
            "111 [D loss: 8.066608, acc.: 50.00%] [G loss: -0.013285]\n",
            "112 [D loss: 8.067779, acc.: 50.00%] [G loss: -0.015165]\n",
            "113 [D loss: 8.066440, acc.: 50.00%] [G loss: -0.014074]\n",
            "114 [D loss: 8.067656, acc.: 50.00%] [G loss: -0.012754]\n",
            "115 [D loss: 8.067116, acc.: 50.00%] [G loss: -0.013104]\n",
            "116 [D loss: 8.065862, acc.: 50.00%] [G loss: -0.014064]\n",
            "117 [D loss: 8.068301, acc.: 50.00%] [G loss: -0.013463]\n",
            "118 [D loss: 8.067389, acc.: 50.00%] [G loss: -0.014810]\n",
            "119 [D loss: 8.067569, acc.: 50.00%] [G loss: -0.013244]\n",
            "120 [D loss: 8.067830, acc.: 50.00%] [G loss: -0.014332]\n",
            "121 [D loss: 8.066820, acc.: 50.00%] [G loss: -0.013495]\n",
            "122 [D loss: 8.067276, acc.: 50.00%] [G loss: -0.015182]\n",
            "123 [D loss: 8.068782, acc.: 50.00%] [G loss: -0.012892]\n",
            "124 [D loss: 8.067120, acc.: 50.00%] [G loss: -0.013569]\n",
            "125 [D loss: 8.066268, acc.: 50.00%] [G loss: -0.012066]\n",
            "126 [D loss: 8.066031, acc.: 50.00%] [G loss: -0.013009]\n",
            "127 [D loss: 8.067307, acc.: 50.00%] [G loss: -0.011717]\n",
            "128 [D loss: 8.066017, acc.: 50.00%] [G loss: -0.012859]\n",
            "129 [D loss: 8.066618, acc.: 50.00%] [G loss: -0.012566]\n",
            "130 [D loss: 8.066606, acc.: 50.00%] [G loss: -0.011899]\n",
            "131 [D loss: 8.065612, acc.: 50.00%] [G loss: -0.011839]\n",
            "132 [D loss: 8.065914, acc.: 50.00%] [G loss: -0.012027]\n",
            "133 [D loss: 8.065301, acc.: 50.00%] [G loss: -0.010297]\n",
            "134 [D loss: 8.064889, acc.: 50.00%] [G loss: -0.011096]\n",
            "135 [D loss: 8.065270, acc.: 50.00%] [G loss: -0.009935]\n",
            "136 [D loss: 8.064877, acc.: 50.00%] [G loss: -0.008752]\n",
            "137 [D loss: 8.065388, acc.: 50.00%] [G loss: -0.010267]\n",
            "138 [D loss: 8.064104, acc.: 50.00%] [G loss: -0.010557]\n",
            "139 [D loss: 8.064412, acc.: 50.00%] [G loss: -0.009137]\n",
            "140 [D loss: 8.064254, acc.: 50.00%] [G loss: -0.009198]\n",
            "141 [D loss: 8.063941, acc.: 50.00%] [G loss: -0.008808]\n",
            "142 [D loss: 8.064122, acc.: 50.00%] [G loss: -0.009024]\n",
            "143 [D loss: 8.064232, acc.: 50.00%] [G loss: -0.008855]\n",
            "144 [D loss: 8.064014, acc.: 50.00%] [G loss: -0.008009]\n",
            "145 [D loss: 8.063871, acc.: 50.00%] [G loss: -0.009539]\n",
            "146 [D loss: 8.063244, acc.: 50.00%] [G loss: -0.007787]\n",
            "147 [D loss: 8.063734, acc.: 50.00%] [G loss: -0.007784]\n",
            "148 [D loss: 8.063346, acc.: 50.00%] [G loss: -0.008874]\n",
            "149 [D loss: 8.064282, acc.: 50.00%] [G loss: -0.007765]\n",
            "150 [D loss: 8.063442, acc.: 50.00%] [G loss: -0.007335]\n",
            "151 [D loss: 8.063516, acc.: 50.00%] [G loss: -0.007886]\n",
            "152 [D loss: 8.063691, acc.: 50.00%] [G loss: -0.007496]\n",
            "153 [D loss: 8.063399, acc.: 50.00%] [G loss: -0.008640]\n",
            "154 [D loss: 8.063113, acc.: 50.00%] [G loss: -0.008383]\n",
            "155 [D loss: 8.063391, acc.: 50.00%] [G loss: -0.007419]\n",
            "156 [D loss: 8.062915, acc.: 50.00%] [G loss: -0.007334]\n",
            "157 [D loss: 8.063068, acc.: 50.00%] [G loss: -0.007536]\n",
            "158 [D loss: 8.063406, acc.: 50.00%] [G loss: -0.007275]\n",
            "159 [D loss: 8.063230, acc.: 50.00%] [G loss: -0.006931]\n",
            "160 [D loss: 8.062940, acc.: 50.00%] [G loss: -0.007435]\n",
            "161 [D loss: 8.062899, acc.: 50.00%] [G loss: -0.007286]\n",
            "162 [D loss: 8.062934, acc.: 50.00%] [G loss: -0.006939]\n",
            "163 [D loss: 8.063062, acc.: 50.00%] [G loss: -0.006358]\n",
            "164 [D loss: 8.062922, acc.: 50.00%] [G loss: -0.006839]\n",
            "165 [D loss: 8.062524, acc.: 50.00%] [G loss: -0.006953]\n",
            "166 [D loss: 8.062462, acc.: 50.00%] [G loss: -0.006399]\n",
            "167 [D loss: 8.062669, acc.: 50.00%] [G loss: -0.006631]\n",
            "168 [D loss: 8.062641, acc.: 50.00%] [G loss: -0.006309]\n",
            "169 [D loss: 8.062832, acc.: 50.00%] [G loss: -0.006355]\n",
            "170 [D loss: 8.062229, acc.: 50.00%] [G loss: -0.006180]\n",
            "171 [D loss: 8.062169, acc.: 50.00%] [G loss: -0.005957]\n",
            "172 [D loss: 8.062025, acc.: 50.00%] [G loss: -0.005241]\n",
            "173 [D loss: 8.062600, acc.: 50.00%] [G loss: -0.005613]\n",
            "174 [D loss: 8.062170, acc.: 50.00%] [G loss: -0.005230]\n",
            "175 [D loss: 8.062201, acc.: 50.00%] [G loss: -0.006240]\n",
            "176 [D loss: 8.062152, acc.: 50.00%] [G loss: -0.005170]\n",
            "177 [D loss: 8.061786, acc.: 50.00%] [G loss: -0.006095]\n",
            "178 [D loss: 8.061959, acc.: 50.00%] [G loss: -0.005578]\n",
            "179 [D loss: 8.062250, acc.: 50.00%] [G loss: -0.005317]\n",
            "180 [D loss: 8.061979, acc.: 50.00%] [G loss: -0.005262]\n",
            "181 [D loss: 8.061800, acc.: 50.00%] [G loss: -0.005321]\n",
            "182 [D loss: 8.061894, acc.: 50.00%] [G loss: -0.004987]\n",
            "183 [D loss: 8.061909, acc.: 50.00%] [G loss: -0.004692]\n",
            "184 [D loss: 8.061691, acc.: 50.00%] [G loss: -0.005323]\n",
            "185 [D loss: 8.061478, acc.: 50.00%] [G loss: -0.004773]\n",
            "186 [D loss: 8.061760, acc.: 50.00%] [G loss: -0.005050]\n",
            "187 [D loss: 8.061700, acc.: 50.00%] [G loss: -0.005370]\n",
            "188 [D loss: 8.061902, acc.: 50.00%] [G loss: -0.004755]\n",
            "189 [D loss: 8.061749, acc.: 50.00%] [G loss: -0.005143]\n",
            "190 [D loss: 8.061966, acc.: 50.00%] [G loss: -0.005025]\n",
            "191 [D loss: 8.061674, acc.: 50.00%] [G loss: -0.005218]\n",
            "192 [D loss: 8.061501, acc.: 50.00%] [G loss: -0.005021]\n",
            "193 [D loss: 8.061872, acc.: 50.00%] [G loss: -0.004795]\n",
            "194 [D loss: 8.061773, acc.: 50.00%] [G loss: -0.005339]\n",
            "195 [D loss: 8.061667, acc.: 50.00%] [G loss: -0.004812]\n",
            "196 [D loss: 8.061902, acc.: 50.00%] [G loss: -0.004641]\n",
            "197 [D loss: 8.061500, acc.: 50.00%] [G loss: -0.004794]\n",
            "198 [D loss: 8.061828, acc.: 50.00%] [G loss: -0.005893]\n",
            "199 [D loss: 8.061753, acc.: 50.00%] [G loss: -0.005170]\n",
            "200 [D loss: 8.061994, acc.: 50.00%] [G loss: -0.005747]\n",
            "201 [D loss: 8.062080, acc.: 50.00%] [G loss: -0.004683]\n",
            "202 [D loss: 8.061632, acc.: 50.00%] [G loss: -0.004707]\n",
            "203 [D loss: 8.061705, acc.: 50.00%] [G loss: -0.005149]\n",
            "204 [D loss: 8.061902, acc.: 50.00%] [G loss: -0.005378]\n",
            "205 [D loss: 8.061827, acc.: 50.00%] [G loss: -0.004639]\n",
            "206 [D loss: 8.061800, acc.: 50.00%] [G loss: -0.005499]\n",
            "207 [D loss: 8.061907, acc.: 50.00%] [G loss: -0.005510]\n",
            "208 [D loss: 8.061754, acc.: 50.00%] [G loss: -0.005406]\n",
            "209 [D loss: 8.061994, acc.: 50.00%] [G loss: -0.004690]\n",
            "210 [D loss: 8.062050, acc.: 50.00%] [G loss: -0.004908]\n",
            "211 [D loss: 8.061687, acc.: 50.00%] [G loss: -0.005379]\n",
            "212 [D loss: 8.061768, acc.: 50.00%] [G loss: -0.005369]\n",
            "213 [D loss: 8.062023, acc.: 50.00%] [G loss: -0.005271]\n",
            "214 [D loss: 8.062119, acc.: 50.00%] [G loss: -0.005108]\n",
            "215 [D loss: 8.061728, acc.: 50.00%] [G loss: -0.005162]\n",
            "216 [D loss: 8.061794, acc.: 50.00%] [G loss: -0.004742]\n",
            "217 [D loss: 8.061769, acc.: 50.00%] [G loss: -0.004796]\n",
            "218 [D loss: 8.061673, acc.: 50.00%] [G loss: -0.004841]\n",
            "219 [D loss: 8.061830, acc.: 50.00%] [G loss: -0.004536]\n",
            "220 [D loss: 8.061662, acc.: 50.00%] [G loss: -0.004959]\n",
            "221 [D loss: 8.061572, acc.: 50.00%] [G loss: -0.005145]\n",
            "222 [D loss: 8.061795, acc.: 50.00%] [G loss: -0.005038]\n",
            "223 [D loss: 8.061810, acc.: 50.00%] [G loss: -0.004715]\n",
            "224 [D loss: 8.061616, acc.: 50.00%] [G loss: -0.004830]\n",
            "225 [D loss: 8.061654, acc.: 50.00%] [G loss: -0.004796]\n",
            "226 [D loss: 8.061545, acc.: 50.00%] [G loss: -0.004568]\n",
            "227 [D loss: 8.061588, acc.: 50.00%] [G loss: -0.005133]\n",
            "228 [D loss: 8.061380, acc.: 50.00%] [G loss: -0.004601]\n",
            "229 [D loss: 8.061888, acc.: 50.00%] [G loss: -0.004871]\n",
            "230 [D loss: 8.061542, acc.: 50.00%] [G loss: -0.004752]\n",
            "231 [D loss: 8.061528, acc.: 50.00%] [G loss: -0.004661]\n",
            "232 [D loss: 8.061538, acc.: 50.00%] [G loss: -0.004637]\n",
            "233 [D loss: 8.061739, acc.: 50.00%] [G loss: -0.004675]\n",
            "234 [D loss: 8.061323, acc.: 50.00%] [G loss: -0.004629]\n",
            "235 [D loss: 8.061411, acc.: 50.00%] [G loss: -0.004536]\n",
            "236 [D loss: 8.061491, acc.: 50.00%] [G loss: -0.004382]\n",
            "237 [D loss: 8.061466, acc.: 50.00%] [G loss: -0.004353]\n",
            "238 [D loss: 8.061356, acc.: 50.00%] [G loss: -0.004591]\n",
            "239 [D loss: 8.061461, acc.: 50.00%] [G loss: -0.004611]\n",
            "240 [D loss: 8.061401, acc.: 50.00%] [G loss: -0.004240]\n",
            "241 [D loss: 8.061296, acc.: 50.00%] [G loss: -0.004197]\n",
            "242 [D loss: 8.061522, acc.: 50.00%] [G loss: -0.004262]\n",
            "243 [D loss: 8.061140, acc.: 50.00%] [G loss: -0.004311]\n",
            "244 [D loss: 8.061279, acc.: 50.00%] [G loss: -0.004314]\n",
            "245 [D loss: 8.061143, acc.: 50.00%] [G loss: -0.004352]\n",
            "246 [D loss: 8.061385, acc.: 50.00%] [G loss: -0.004170]\n",
            "247 [D loss: 8.061349, acc.: 50.00%] [G loss: -0.004006]\n",
            "248 [D loss: 8.061318, acc.: 50.00%] [G loss: -0.003938]\n",
            "249 [D loss: 8.061431, acc.: 50.00%] [G loss: -0.004305]\n",
            "250 [D loss: 8.061201, acc.: 50.00%] [G loss: -0.004029]\n",
            "251 [D loss: 8.061116, acc.: 50.00%] [G loss: -0.004030]\n",
            "252 [D loss: 8.061193, acc.: 50.00%] [G loss: -0.004086]\n",
            "253 [D loss: 8.061201, acc.: 50.00%] [G loss: -0.003807]\n",
            "254 [D loss: 8.061159, acc.: 50.00%] [G loss: -0.003847]\n",
            "255 [D loss: 8.061191, acc.: 50.00%] [G loss: -0.004004]\n",
            "256 [D loss: 8.061088, acc.: 50.00%] [G loss: -0.004046]\n",
            "257 [D loss: 8.061199, acc.: 50.00%] [G loss: -0.003748]\n",
            "258 [D loss: 8.060966, acc.: 50.00%] [G loss: -0.003873]\n",
            "259 [D loss: 8.061138, acc.: 50.00%] [G loss: -0.003931]\n",
            "260 [D loss: 8.060856, acc.: 50.00%] [G loss: -0.003611]\n",
            "261 [D loss: 8.061049, acc.: 50.00%] [G loss: -0.004028]\n",
            "262 [D loss: 8.061110, acc.: 50.00%] [G loss: -0.003731]\n",
            "263 [D loss: 8.061175, acc.: 50.00%] [G loss: -0.003458]\n",
            "264 [D loss: 8.060989, acc.: 50.00%] [G loss: -0.003686]\n",
            "265 [D loss: 8.060883, acc.: 50.00%] [G loss: -0.003646]\n",
            "266 [D loss: 8.060877, acc.: 50.00%] [G loss: -0.003464]\n",
            "267 [D loss: 8.060935, acc.: 50.00%] [G loss: -0.003473]\n",
            "268 [D loss: 8.061004, acc.: 50.00%] [G loss: -0.003823]\n",
            "269 [D loss: 8.060907, acc.: 50.00%] [G loss: -0.003402]\n",
            "270 [D loss: 8.060904, acc.: 50.00%] [G loss: -0.003546]\n",
            "271 [D loss: 8.061023, acc.: 50.00%] [G loss: -0.003558]\n",
            "272 [D loss: 8.060820, acc.: 50.00%] [G loss: -0.003637]\n",
            "273 [D loss: 8.060781, acc.: 50.00%] [G loss: -0.003333]\n",
            "274 [D loss: 8.060815, acc.: 50.00%] [G loss: -0.002928]\n",
            "275 [D loss: 8.060791, acc.: 50.00%] [G loss: -0.003246]\n",
            "276 [D loss: 8.060641, acc.: 50.00%] [G loss: -0.003283]\n",
            "277 [D loss: 8.060691, acc.: 50.00%] [G loss: -0.003428]\n",
            "278 [D loss: 8.060696, acc.: 50.00%] [G loss: -0.003152]\n",
            "279 [D loss: 8.060621, acc.: 50.00%] [G loss: -0.003047]\n",
            "280 [D loss: 8.060690, acc.: 50.00%] [G loss: -0.003043]\n",
            "281 [D loss: 8.060543, acc.: 50.00%] [G loss: -0.002862]\n",
            "282 [D loss: 8.060534, acc.: 50.00%] [G loss: -0.002931]\n",
            "283 [D loss: 8.060545, acc.: 50.00%] [G loss: -0.002569]\n",
            "284 [D loss: 8.060546, acc.: 50.00%] [G loss: -0.002816]\n",
            "285 [D loss: 8.060528, acc.: 50.00%] [G loss: -0.002573]\n",
            "286 [D loss: 8.060370, acc.: 50.00%] [G loss: -0.002541]\n",
            "287 [D loss: 8.060419, acc.: 50.00%] [G loss: -0.002699]\n",
            "288 [D loss: 8.060229, acc.: 50.00%] [G loss: -0.002720]\n",
            "289 [D loss: 8.060326, acc.: 50.00%] [G loss: -0.002458]\n",
            "290 [D loss: 8.060173, acc.: 50.00%] [G loss: -0.002583]\n",
            "291 [D loss: 8.060350, acc.: 50.00%] [G loss: -0.002246]\n",
            "292 [D loss: 8.060213, acc.: 50.00%] [G loss: -0.002368]\n",
            "293 [D loss: 8.060255, acc.: 50.00%] [G loss: -0.002099]\n",
            "294 [D loss: 8.060274, acc.: 50.00%] [G loss: -0.001988]\n",
            "295 [D loss: 8.059991, acc.: 50.00%] [G loss: -0.001996]\n",
            "296 [D loss: 8.060097, acc.: 50.00%] [G loss: -0.002056]\n",
            "297 [D loss: 8.060121, acc.: 50.00%] [G loss: -0.001795]\n",
            "298 [D loss: 8.060035, acc.: 50.00%] [G loss: -0.001926]\n",
            "299 [D loss: 8.060045, acc.: 50.00%] [G loss: -0.001795]\n",
            "300 [D loss: 8.059932, acc.: 50.00%] [G loss: -0.001821]\n",
            "301 [D loss: 8.059991, acc.: 50.00%] [G loss: -0.001726]\n",
            "302 [D loss: 8.059918, acc.: 50.00%] [G loss: -0.001811]\n",
            "303 [D loss: 8.060009, acc.: 50.00%] [G loss: -0.001617]\n",
            "304 [D loss: 8.059887, acc.: 50.00%] [G loss: -0.001661]\n",
            "305 [D loss: 8.059910, acc.: 50.00%] [G loss: -0.001550]\n",
            "306 [D loss: 8.059834, acc.: 50.00%] [G loss: -0.001453]\n",
            "307 [D loss: 8.059801, acc.: 50.00%] [G loss: -0.001535]\n",
            "308 [D loss: 8.059817, acc.: 50.00%] [G loss: -0.001521]\n",
            "309 [D loss: 8.059773, acc.: 50.00%] [G loss: -0.001529]\n",
            "310 [D loss: 8.059866, acc.: 50.00%] [G loss: -0.001513]\n",
            "311 [D loss: 8.059821, acc.: 50.00%] [G loss: -0.001448]\n",
            "312 [D loss: 8.059772, acc.: 50.00%] [G loss: -0.001492]\n",
            "313 [D loss: 8.059765, acc.: 50.00%] [G loss: -0.001483]\n",
            "314 [D loss: 8.059827, acc.: 50.00%] [G loss: -0.001411]\n",
            "315 [D loss: 8.059691, acc.: 50.00%] [G loss: -0.001439]\n",
            "316 [D loss: 8.059740, acc.: 50.00%] [G loss: -0.001345]\n",
            "317 [D loss: 8.059740, acc.: 50.00%] [G loss: -0.001273]\n",
            "318 [D loss: 8.059752, acc.: 50.00%] [G loss: -0.001296]\n",
            "319 [D loss: 8.059705, acc.: 50.00%] [G loss: -0.001281]\n",
            "320 [D loss: 8.059677, acc.: 50.00%] [G loss: -0.001280]\n",
            "321 [D loss: 8.059622, acc.: 50.00%] [G loss: -0.001146]\n",
            "322 [D loss: 8.059625, acc.: 50.00%] [G loss: -0.001213]\n",
            "323 [D loss: 8.059610, acc.: 50.00%] [G loss: -0.001168]\n",
            "324 [D loss: 8.059667, acc.: 50.00%] [G loss: -0.001172]\n",
            "325 [D loss: 8.059626, acc.: 50.00%] [G loss: -0.001231]\n",
            "326 [D loss: 8.059686, acc.: 50.00%] [G loss: -0.001213]\n",
            "327 [D loss: 8.059683, acc.: 50.00%] [G loss: -0.001128]\n",
            "328 [D loss: 8.059621, acc.: 50.00%] [G loss: -0.001104]\n",
            "329 [D loss: 8.059595, acc.: 50.00%] [G loss: -0.001163]\n",
            "330 [D loss: 8.059631, acc.: 50.00%] [G loss: -0.001074]\n",
            "331 [D loss: 8.059624, acc.: 50.00%] [G loss: -0.001156]\n",
            "332 [D loss: 8.059618, acc.: 50.00%] [G loss: -0.001080]\n",
            "333 [D loss: 8.059632, acc.: 50.00%] [G loss: -0.001040]\n",
            "334 [D loss: 8.059645, acc.: 50.00%] [G loss: -0.001110]\n",
            "335 [D loss: 8.059577, acc.: 50.00%] [G loss: -0.001056]\n",
            "336 [D loss: 8.059597, acc.: 50.00%] [G loss: -0.001022]\n",
            "337 [D loss: 8.059572, acc.: 50.00%] [G loss: -0.001091]\n",
            "338 [D loss: 8.059563, acc.: 50.00%] [G loss: -0.000963]\n",
            "339 [D loss: 8.059545, acc.: 50.00%] [G loss: -0.000872]\n",
            "340 [D loss: 8.059518, acc.: 50.00%] [G loss: -0.000991]\n",
            "341 [D loss: 8.059540, acc.: 50.00%] [G loss: -0.000972]\n",
            "342 [D loss: 8.059542, acc.: 50.00%] [G loss: -0.000880]\n",
            "343 [D loss: 8.059534, acc.: 50.00%] [G loss: -0.000954]\n",
            "344 [D loss: 8.059543, acc.: 50.00%] [G loss: -0.000998]\n",
            "345 [D loss: 8.059510, acc.: 50.00%] [G loss: -0.000949]\n",
            "346 [D loss: 8.059530, acc.: 50.00%] [G loss: -0.000869]\n",
            "347 [D loss: 8.059511, acc.: 50.00%] [G loss: -0.001057]\n",
            "348 [D loss: 8.059518, acc.: 50.00%] [G loss: -0.000879]\n",
            "349 [D loss: 8.059522, acc.: 50.00%] [G loss: -0.000873]\n",
            "350 [D loss: 8.059514, acc.: 50.00%] [G loss: -0.000935]\n",
            "351 [D loss: 8.059498, acc.: 50.00%] [G loss: -0.000957]\n",
            "352 [D loss: 8.059501, acc.: 50.00%] [G loss: -0.000873]\n",
            "353 [D loss: 8.059520, acc.: 50.00%] [G loss: -0.000998]\n",
            "354 [D loss: 8.059434, acc.: 50.00%] [G loss: -0.000865]\n",
            "355 [D loss: 8.059490, acc.: 50.00%] [G loss: -0.000888]\n",
            "356 [D loss: 8.059478, acc.: 50.00%] [G loss: -0.000875]\n",
            "357 [D loss: 8.059484, acc.: 50.00%] [G loss: -0.000776]\n",
            "358 [D loss: 8.059497, acc.: 50.00%] [G loss: -0.000895]\n",
            "359 [D loss: 8.059464, acc.: 50.00%] [G loss: -0.000844]\n",
            "360 [D loss: 8.059474, acc.: 50.00%] [G loss: -0.000944]\n",
            "361 [D loss: 8.059459, acc.: 50.00%] [G loss: -0.000810]\n",
            "362 [D loss: 8.059476, acc.: 50.00%] [G loss: -0.000834]\n",
            "363 [D loss: 8.059493, acc.: 50.00%] [G loss: -0.000796]\n",
            "364 [D loss: 8.059451, acc.: 50.00%] [G loss: -0.000791]\n",
            "365 [D loss: 8.059488, acc.: 50.00%] [G loss: -0.000758]\n",
            "366 [D loss: 8.059472, acc.: 50.00%] [G loss: -0.000813]\n",
            "367 [D loss: 8.059422, acc.: 50.00%] [G loss: -0.000773]\n",
            "368 [D loss: 8.059459, acc.: 50.00%] [G loss: -0.000861]\n",
            "369 [D loss: 8.059448, acc.: 50.00%] [G loss: -0.000898]\n",
            "370 [D loss: 8.059449, acc.: 50.00%] [G loss: -0.000735]\n",
            "371 [D loss: 8.059456, acc.: 50.00%] [G loss: -0.000858]\n",
            "372 [D loss: 8.059438, acc.: 50.00%] [G loss: -0.000877]\n",
            "373 [D loss: 8.059484, acc.: 50.00%] [G loss: -0.000845]\n",
            "374 [D loss: 8.059446, acc.: 50.00%] [G loss: -0.000767]\n",
            "375 [D loss: 8.059426, acc.: 50.00%] [G loss: -0.000757]\n",
            "376 [D loss: 8.059443, acc.: 50.00%] [G loss: -0.000761]\n",
            "377 [D loss: 8.059429, acc.: 50.00%] [G loss: -0.000743]\n",
            "378 [D loss: 8.059446, acc.: 50.00%] [G loss: -0.000809]\n",
            "379 [D loss: 8.059422, acc.: 50.00%] [G loss: -0.000770]\n",
            "380 [D loss: 8.059405, acc.: 50.00%] [G loss: -0.000755]\n",
            "381 [D loss: 8.059452, acc.: 50.00%] [G loss: -0.000787]\n",
            "382 [D loss: 8.059404, acc.: 50.00%] [G loss: -0.000821]\n",
            "383 [D loss: 8.059406, acc.: 50.00%] [G loss: -0.000695]\n",
            "384 [D loss: 8.059436, acc.: 50.00%] [G loss: -0.000673]\n",
            "385 [D loss: 8.059410, acc.: 50.00%] [G loss: -0.000699]\n",
            "386 [D loss: 8.059420, acc.: 50.00%] [G loss: -0.000755]\n",
            "387 [D loss: 8.059410, acc.: 50.00%] [G loss: -0.000664]\n",
            "388 [D loss: 8.059415, acc.: 50.00%] [G loss: -0.000728]\n",
            "389 [D loss: 8.059403, acc.: 50.00%] [G loss: -0.000634]\n",
            "390 [D loss: 8.059400, acc.: 50.00%] [G loss: -0.000678]\n",
            "391 [D loss: 8.059439, acc.: 50.00%] [G loss: -0.000727]\n",
            "392 [D loss: 8.059403, acc.: 50.00%] [G loss: -0.000637]\n",
            "393 [D loss: 8.059429, acc.: 50.00%] [G loss: -0.000645]\n",
            "394 [D loss: 8.059418, acc.: 50.00%] [G loss: -0.000590]\n",
            "395 [D loss: 8.059395, acc.: 50.00%] [G loss: -0.000696]\n",
            "396 [D loss: 8.059391, acc.: 50.00%] [G loss: -0.000648]\n",
            "397 [D loss: 8.059404, acc.: 50.00%] [G loss: -0.000626]\n",
            "398 [D loss: 8.059420, acc.: 50.00%] [G loss: -0.000705]\n",
            "399 [D loss: 8.059380, acc.: 50.00%] [G loss: -0.000792]\n",
            "400 [D loss: 8.059385, acc.: 50.00%] [G loss: -0.000620]\n",
            "401 [D loss: 8.059369, acc.: 50.00%] [G loss: -0.000721]\n",
            "402 [D loss: 8.059383, acc.: 50.00%] [G loss: -0.000691]\n",
            "403 [D loss: 8.059380, acc.: 50.00%] [G loss: -0.000653]\n",
            "404 [D loss: 8.059388, acc.: 50.00%] [G loss: -0.000693]\n",
            "405 [D loss: 8.059363, acc.: 50.00%] [G loss: -0.000576]\n",
            "406 [D loss: 8.059407, acc.: 50.00%] [G loss: -0.000678]\n",
            "407 [D loss: 8.059371, acc.: 50.00%] [G loss: -0.000674]\n",
            "408 [D loss: 8.059374, acc.: 50.00%] [G loss: -0.000704]\n",
            "409 [D loss: 8.059403, acc.: 50.00%] [G loss: -0.000681]\n",
            "410 [D loss: 8.059382, acc.: 50.00%] [G loss: -0.000657]\n",
            "411 [D loss: 8.059384, acc.: 50.00%] [G loss: -0.000637]\n",
            "412 [D loss: 8.059381, acc.: 50.00%] [G loss: -0.000646]\n",
            "413 [D loss: 8.059378, acc.: 50.00%] [G loss: -0.000676]\n",
            "414 [D loss: 8.059377, acc.: 50.00%] [G loss: -0.000697]\n",
            "415 [D loss: 8.059356, acc.: 50.00%] [G loss: -0.000648]\n",
            "416 [D loss: 8.059418, acc.: 50.00%] [G loss: -0.000579]\n",
            "417 [D loss: 8.059339, acc.: 50.00%] [G loss: -0.000646]\n",
            "418 [D loss: 8.059384, acc.: 50.00%] [G loss: -0.000648]\n",
            "419 [D loss: 8.059344, acc.: 50.00%] [G loss: -0.000676]\n",
            "420 [D loss: 8.059378, acc.: 50.00%] [G loss: -0.000629]\n",
            "421 [D loss: 8.059355, acc.: 50.00%] [G loss: -0.000645]\n",
            "422 [D loss: 8.059361, acc.: 50.00%] [G loss: -0.000679]\n",
            "423 [D loss: 8.059391, acc.: 50.00%] [G loss: -0.000685]\n",
            "424 [D loss: 8.059378, acc.: 50.00%] [G loss: -0.000618]\n",
            "425 [D loss: 8.059389, acc.: 50.00%] [G loss: -0.000652]\n",
            "426 [D loss: 8.059383, acc.: 50.00%] [G loss: -0.000665]\n",
            "427 [D loss: 8.059356, acc.: 50.00%] [G loss: -0.000640]\n",
            "428 [D loss: 8.059355, acc.: 50.00%] [G loss: -0.000620]\n",
            "429 [D loss: 8.059378, acc.: 50.00%] [G loss: -0.000608]\n",
            "430 [D loss: 8.059349, acc.: 50.00%] [G loss: -0.000661]\n",
            "431 [D loss: 8.059381, acc.: 50.00%] [G loss: -0.000676]\n",
            "432 [D loss: 8.059356, acc.: 50.00%] [G loss: -0.000591]\n",
            "433 [D loss: 8.059366, acc.: 50.00%] [G loss: -0.000620]\n",
            "434 [D loss: 8.059347, acc.: 50.00%] [G loss: -0.000577]\n",
            "435 [D loss: 8.059346, acc.: 50.00%] [G loss: -0.000669]\n",
            "436 [D loss: 8.059356, acc.: 50.00%] [G loss: -0.000596]\n",
            "437 [D loss: 8.059361, acc.: 50.00%] [G loss: -0.000650]\n",
            "438 [D loss: 8.059373, acc.: 50.00%] [G loss: -0.000549]\n",
            "439 [D loss: 8.059366, acc.: 50.00%] [G loss: -0.000591]\n",
            "440 [D loss: 8.059365, acc.: 50.00%] [G loss: -0.000601]\n",
            "441 [D loss: 8.059349, acc.: 50.00%] [G loss: -0.000621]\n",
            "442 [D loss: 8.059347, acc.: 50.00%] [G loss: -0.000646]\n",
            "443 [D loss: 8.059342, acc.: 50.00%] [G loss: -0.000621]\n",
            "444 [D loss: 8.059324, acc.: 50.00%] [G loss: -0.000618]\n",
            "445 [D loss: 8.059346, acc.: 50.00%] [G loss: -0.000573]\n",
            "446 [D loss: 8.059367, acc.: 50.00%] [G loss: -0.000581]\n",
            "447 [D loss: 8.059352, acc.: 50.00%] [G loss: -0.000548]\n",
            "448 [D loss: 8.059337, acc.: 50.00%] [G loss: -0.000576]\n",
            "449 [D loss: 8.059348, acc.: 50.00%] [G loss: -0.000598]\n",
            "450 [D loss: 8.059354, acc.: 50.00%] [G loss: -0.000593]\n",
            "451 [D loss: 8.059342, acc.: 50.00%] [G loss: -0.000552]\n",
            "452 [D loss: 8.059358, acc.: 50.00%] [G loss: -0.000619]\n",
            "453 [D loss: 8.059313, acc.: 50.00%] [G loss: -0.000583]\n",
            "454 [D loss: 8.059354, acc.: 50.00%] [G loss: -0.000565]\n",
            "455 [D loss: 8.059343, acc.: 50.00%] [G loss: -0.000548]\n",
            "456 [D loss: 8.059365, acc.: 50.00%] [G loss: -0.000604]\n",
            "457 [D loss: 8.059319, acc.: 50.00%] [G loss: -0.000524]\n",
            "458 [D loss: 8.059369, acc.: 50.00%] [G loss: -0.000525]\n",
            "459 [D loss: 8.059324, acc.: 50.00%] [G loss: -0.000583]\n",
            "460 [D loss: 8.059300, acc.: 50.00%] [G loss: -0.000561]\n",
            "461 [D loss: 8.059309, acc.: 50.00%] [G loss: -0.000562]\n",
            "462 [D loss: 8.059343, acc.: 50.00%] [G loss: -0.000535]\n",
            "463 [D loss: 8.059331, acc.: 50.00%] [G loss: -0.000539]\n",
            "464 [D loss: 8.059328, acc.: 50.00%] [G loss: -0.000526]\n",
            "465 [D loss: 8.059339, acc.: 50.00%] [G loss: -0.000608]\n",
            "466 [D loss: 8.059303, acc.: 50.00%] [G loss: -0.000542]\n",
            "467 [D loss: 8.059353, acc.: 50.00%] [G loss: -0.000579]\n",
            "468 [D loss: 8.059346, acc.: 50.00%] [G loss: -0.000541]\n",
            "469 [D loss: 8.059364, acc.: 50.00%] [G loss: -0.000577]\n",
            "470 [D loss: 8.059323, acc.: 50.00%] [G loss: -0.000541]\n",
            "471 [D loss: 8.059322, acc.: 50.00%] [G loss: -0.000510]\n",
            "472 [D loss: 8.059304, acc.: 50.00%] [G loss: -0.000532]\n",
            "473 [D loss: 8.059330, acc.: 50.00%] [G loss: -0.000528]\n",
            "474 [D loss: 8.059307, acc.: 50.00%] [G loss: -0.000548]\n",
            "475 [D loss: 8.059354, acc.: 50.00%] [G loss: -0.000521]\n",
            "476 [D loss: 8.059322, acc.: 50.00%] [G loss: -0.000551]\n",
            "477 [D loss: 8.059317, acc.: 50.00%] [G loss: -0.000545]\n",
            "478 [D loss: 8.059313, acc.: 50.00%] [G loss: -0.000532]\n",
            "479 [D loss: 8.059313, acc.: 50.00%] [G loss: -0.000485]\n",
            "480 [D loss: 8.059313, acc.: 50.00%] [G loss: -0.000558]\n",
            "481 [D loss: 8.059311, acc.: 50.00%] [G loss: -0.000474]\n",
            "482 [D loss: 8.059302, acc.: 50.00%] [G loss: -0.000491]\n",
            "483 [D loss: 8.059340, acc.: 50.00%] [G loss: -0.000556]\n",
            "484 [D loss: 8.059299, acc.: 50.00%] [G loss: -0.000505]\n",
            "485 [D loss: 8.059320, acc.: 50.00%] [G loss: -0.000532]\n",
            "486 [D loss: 8.059319, acc.: 50.00%] [G loss: -0.000532]\n",
            "487 [D loss: 8.059308, acc.: 50.00%] [G loss: -0.000475]\n",
            "488 [D loss: 8.059280, acc.: 50.00%] [G loss: -0.000495]\n",
            "489 [D loss: 8.059304, acc.: 50.00%] [G loss: -0.000520]\n",
            "490 [D loss: 8.059327, acc.: 50.00%] [G loss: -0.000532]\n",
            "491 [D loss: 8.059317, acc.: 50.00%] [G loss: -0.000495]\n",
            "492 [D loss: 8.059323, acc.: 50.00%] [G loss: -0.000525]\n",
            "493 [D loss: 8.059312, acc.: 50.00%] [G loss: -0.000494]\n",
            "494 [D loss: 8.059311, acc.: 50.00%] [G loss: -0.000522]\n",
            "495 [D loss: 8.059288, acc.: 50.00%] [G loss: -0.000484]\n",
            "496 [D loss: 8.059284, acc.: 50.00%] [G loss: -0.000560]\n",
            "497 [D loss: 8.059315, acc.: 50.00%] [G loss: -0.000486]\n",
            "498 [D loss: 8.059284, acc.: 50.00%] [G loss: -0.000479]\n",
            "499 [D loss: 8.059290, acc.: 50.00%] [G loss: -0.000526]\n",
            "500 [D loss: 8.059319, acc.: 50.00%] [G loss: -0.000480]\n",
            "501 [D loss: 8.059313, acc.: 50.00%] [G loss: -0.000473]\n",
            "502 [D loss: 8.059323, acc.: 50.00%] [G loss: -0.000494]\n",
            "503 [D loss: 8.059270, acc.: 50.00%] [G loss: -0.000527]\n",
            "504 [D loss: 8.059292, acc.: 50.00%] [G loss: -0.000438]\n",
            "505 [D loss: 8.059284, acc.: 50.00%] [G loss: -0.000491]\n",
            "506 [D loss: 8.059298, acc.: 50.00%] [G loss: -0.000486]\n",
            "507 [D loss: 8.059290, acc.: 50.00%] [G loss: -0.000525]\n",
            "508 [D loss: 8.059282, acc.: 50.00%] [G loss: -0.000523]\n",
            "509 [D loss: 8.059299, acc.: 50.00%] [G loss: -0.000500]\n",
            "510 [D loss: 8.059302, acc.: 50.00%] [G loss: -0.000516]\n",
            "511 [D loss: 8.059317, acc.: 50.00%] [G loss: -0.000501]\n",
            "512 [D loss: 8.059285, acc.: 50.00%] [G loss: -0.000495]\n",
            "513 [D loss: 8.059300, acc.: 50.00%] [G loss: -0.000456]\n",
            "514 [D loss: 8.059299, acc.: 50.00%] [G loss: -0.000437]\n",
            "515 [D loss: 8.059305, acc.: 50.00%] [G loss: -0.000443]\n",
            "516 [D loss: 8.059308, acc.: 50.00%] [G loss: -0.000482]\n",
            "517 [D loss: 8.059278, acc.: 50.00%] [G loss: -0.000507]\n",
            "518 [D loss: 8.059270, acc.: 50.00%] [G loss: -0.000485]\n",
            "519 [D loss: 8.059291, acc.: 50.00%] [G loss: -0.000487]\n",
            "520 [D loss: 8.059246, acc.: 50.00%] [G loss: -0.000471]\n",
            "521 [D loss: 8.059274, acc.: 50.00%] [G loss: -0.000460]\n",
            "522 [D loss: 8.059274, acc.: 50.00%] [G loss: -0.000430]\n",
            "523 [D loss: 8.059285, acc.: 50.00%] [G loss: -0.000482]\n",
            "524 [D loss: 8.059275, acc.: 50.00%] [G loss: -0.000428]\n",
            "525 [D loss: 8.059277, acc.: 50.00%] [G loss: -0.000397]\n",
            "526 [D loss: 8.059295, acc.: 50.00%] [G loss: -0.000454]\n",
            "527 [D loss: 8.059318, acc.: 50.00%] [G loss: -0.000457]\n",
            "528 [D loss: 8.059275, acc.: 50.00%] [G loss: -0.000499]\n",
            "529 [D loss: 8.059283, acc.: 50.00%] [G loss: -0.000442]\n",
            "530 [D loss: 8.059291, acc.: 50.00%] [G loss: -0.000423]\n",
            "531 [D loss: 8.059264, acc.: 50.00%] [G loss: -0.000425]\n",
            "532 [D loss: 8.059268, acc.: 50.00%] [G loss: -0.000464]\n",
            "533 [D loss: 8.059279, acc.: 50.00%] [G loss: -0.000415]\n",
            "534 [D loss: 8.059258, acc.: 50.00%] [G loss: -0.000491]\n",
            "535 [D loss: 8.059249, acc.: 50.00%] [G loss: -0.000433]\n",
            "536 [D loss: 8.059273, acc.: 50.00%] [G loss: -0.000456]\n",
            "537 [D loss: 8.059285, acc.: 50.00%] [G loss: -0.000425]\n",
            "538 [D loss: 8.059294, acc.: 50.00%] [G loss: -0.000522]\n",
            "539 [D loss: 8.059242, acc.: 50.00%] [G loss: -0.000450]\n",
            "540 [D loss: 8.059273, acc.: 50.00%] [G loss: -0.000440]\n",
            "541 [D loss: 8.059276, acc.: 50.00%] [G loss: -0.000457]\n",
            "542 [D loss: 8.059268, acc.: 50.00%] [G loss: -0.000413]\n",
            "543 [D loss: 8.059279, acc.: 50.00%] [G loss: -0.000426]\n",
            "544 [D loss: 8.059280, acc.: 50.00%] [G loss: -0.000444]\n",
            "545 [D loss: 8.059252, acc.: 50.00%] [G loss: -0.000393]\n",
            "546 [D loss: 8.059265, acc.: 50.00%] [G loss: -0.000467]\n",
            "547 [D loss: 8.059292, acc.: 50.00%] [G loss: -0.000431]\n",
            "548 [D loss: 8.059263, acc.: 50.00%] [G loss: -0.000426]\n",
            "549 [D loss: 8.059247, acc.: 50.00%] [G loss: -0.000390]\n",
            "550 [D loss: 8.059262, acc.: 50.00%] [G loss: -0.000459]\n",
            "551 [D loss: 8.059275, acc.: 50.00%] [G loss: -0.000402]\n",
            "552 [D loss: 8.059266, acc.: 50.00%] [G loss: -0.000437]\n",
            "553 [D loss: 8.059267, acc.: 50.00%] [G loss: -0.000423]\n",
            "554 [D loss: 8.059258, acc.: 50.00%] [G loss: -0.000482]\n",
            "555 [D loss: 8.059257, acc.: 50.00%] [G loss: -0.000441]\n",
            "556 [D loss: 8.059247, acc.: 50.00%] [G loss: -0.000388]\n",
            "557 [D loss: 8.059260, acc.: 50.00%] [G loss: -0.000413]\n",
            "558 [D loss: 8.059280, acc.: 50.00%] [G loss: -0.000408]\n",
            "559 [D loss: 8.059267, acc.: 50.00%] [G loss: -0.000408]\n",
            "560 [D loss: 8.059260, acc.: 50.00%] [G loss: -0.000397]\n",
            "561 [D loss: 8.059254, acc.: 50.00%] [G loss: -0.000433]\n",
            "562 [D loss: 8.059281, acc.: 50.00%] [G loss: -0.000430]\n",
            "563 [D loss: 8.059265, acc.: 50.00%] [G loss: -0.000416]\n",
            "564 [D loss: 8.059259, acc.: 50.00%] [G loss: -0.000453]\n",
            "565 [D loss: 8.059256, acc.: 50.00%] [G loss: -0.000421]\n",
            "566 [D loss: 8.059273, acc.: 50.00%] [G loss: -0.000393]\n",
            "567 [D loss: 8.059266, acc.: 50.00%] [G loss: -0.000436]\n",
            "568 [D loss: 8.059236, acc.: 50.00%] [G loss: -0.000420]\n",
            "569 [D loss: 8.059251, acc.: 50.00%] [G loss: -0.000422]\n",
            "570 [D loss: 8.059244, acc.: 50.00%] [G loss: -0.000392]\n",
            "571 [D loss: 8.059246, acc.: 50.00%] [G loss: -0.000415]\n",
            "572 [D loss: 8.059270, acc.: 50.00%] [G loss: -0.000382]\n",
            "573 [D loss: 8.059253, acc.: 50.00%] [G loss: -0.000409]\n",
            "574 [D loss: 8.059256, acc.: 50.00%] [G loss: -0.000397]\n",
            "575 [D loss: 8.059269, acc.: 50.00%] [G loss: -0.000409]\n",
            "576 [D loss: 8.059263, acc.: 50.00%] [G loss: -0.000416]\n",
            "577 [D loss: 8.059252, acc.: 50.00%] [G loss: -0.000438]\n",
            "578 [D loss: 8.059254, acc.: 50.00%] [G loss: -0.000399]\n",
            "579 [D loss: 8.059265, acc.: 50.00%] [G loss: -0.000408]\n",
            "580 [D loss: 8.059254, acc.: 50.00%] [G loss: -0.000371]\n",
            "581 [D loss: 8.059259, acc.: 50.00%] [G loss: -0.000376]\n",
            "582 [D loss: 8.059258, acc.: 50.00%] [G loss: -0.000403]\n",
            "583 [D loss: 8.059258, acc.: 50.00%] [G loss: -0.000379]\n",
            "584 [D loss: 8.059240, acc.: 50.00%] [G loss: -0.000436]\n",
            "585 [D loss: 8.059260, acc.: 50.00%] [G loss: -0.000395]\n",
            "586 [D loss: 8.059253, acc.: 50.00%] [G loss: -0.000408]\n",
            "587 [D loss: 8.059260, acc.: 50.00%] [G loss: -0.000391]\n",
            "588 [D loss: 8.059253, acc.: 50.00%] [G loss: -0.000401]\n",
            "589 [D loss: 8.059250, acc.: 50.00%] [G loss: -0.000413]\n",
            "590 [D loss: 8.059266, acc.: 50.00%] [G loss: -0.000392]\n",
            "591 [D loss: 8.059253, acc.: 50.00%] [G loss: -0.000389]\n",
            "592 [D loss: 8.059252, acc.: 50.00%] [G loss: -0.000404]\n",
            "593 [D loss: 8.059246, acc.: 50.00%] [G loss: -0.000380]\n",
            "594 [D loss: 8.059238, acc.: 50.00%] [G loss: -0.000391]\n",
            "595 [D loss: 8.059229, acc.: 50.00%] [G loss: -0.000391]\n",
            "596 [D loss: 8.059267, acc.: 50.00%] [G loss: -0.000354]\n",
            "597 [D loss: 8.059254, acc.: 50.00%] [G loss: -0.000359]\n",
            "598 [D loss: 8.059233, acc.: 50.00%] [G loss: -0.000402]\n",
            "599 [D loss: 8.059240, acc.: 50.00%] [G loss: -0.000399]\n",
            "600 [D loss: 8.059242, acc.: 50.00%] [G loss: -0.000358]\n",
            "601 [D loss: 8.059250, acc.: 50.00%] [G loss: -0.000373]\n",
            "602 [D loss: 8.059241, acc.: 50.00%] [G loss: -0.000392]\n",
            "603 [D loss: 8.059244, acc.: 50.00%] [G loss: -0.000400]\n",
            "604 [D loss: 8.059243, acc.: 50.00%] [G loss: -0.000387]\n",
            "605 [D loss: 8.059226, acc.: 50.00%] [G loss: -0.000375]\n",
            "606 [D loss: 8.059235, acc.: 50.00%] [G loss: -0.000408]\n",
            "607 [D loss: 8.059241, acc.: 50.00%] [G loss: -0.000396]\n",
            "608 [D loss: 8.059232, acc.: 50.00%] [G loss: -0.000400]\n",
            "609 [D loss: 8.059251, acc.: 50.00%] [G loss: -0.000375]\n",
            "610 [D loss: 8.059244, acc.: 50.00%] [G loss: -0.000360]\n",
            "611 [D loss: 8.059244, acc.: 50.00%] [G loss: -0.000411]\n",
            "612 [D loss: 8.059250, acc.: 50.00%] [G loss: -0.000406]\n",
            "613 [D loss: 8.059247, acc.: 50.00%] [G loss: -0.000416]\n",
            "614 [D loss: 8.059248, acc.: 50.00%] [G loss: -0.000381]\n",
            "615 [D loss: 8.059234, acc.: 50.00%] [G loss: -0.000393]\n",
            "616 [D loss: 8.059252, acc.: 50.00%] [G loss: -0.000380]\n",
            "617 [D loss: 8.059248, acc.: 50.00%] [G loss: -0.000368]\n",
            "618 [D loss: 8.059248, acc.: 50.00%] [G loss: -0.000359]\n",
            "619 [D loss: 8.059246, acc.: 50.00%] [G loss: -0.000364]\n",
            "620 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000390]\n",
            "621 [D loss: 8.059241, acc.: 50.00%] [G loss: -0.000366]\n",
            "622 [D loss: 8.059238, acc.: 50.00%] [G loss: -0.000341]\n",
            "623 [D loss: 8.059232, acc.: 50.00%] [G loss: -0.000399]\n",
            "624 [D loss: 8.059265, acc.: 50.00%] [G loss: -0.000344]\n",
            "625 [D loss: 8.059250, acc.: 50.00%] [G loss: -0.000392]\n",
            "626 [D loss: 8.059238, acc.: 50.00%] [G loss: -0.000393]\n",
            "627 [D loss: 8.059243, acc.: 50.00%] [G loss: -0.000384]\n",
            "628 [D loss: 8.059229, acc.: 50.00%] [G loss: -0.000407]\n",
            "629 [D loss: 8.059253, acc.: 50.00%] [G loss: -0.000350]\n",
            "630 [D loss: 8.059233, acc.: 50.00%] [G loss: -0.000391]\n",
            "631 [D loss: 8.059242, acc.: 50.00%] [G loss: -0.000375]\n",
            "632 [D loss: 8.059226, acc.: 50.00%] [G loss: -0.000385]\n",
            "633 [D loss: 8.059232, acc.: 50.00%] [G loss: -0.000380]\n",
            "634 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000375]\n",
            "635 [D loss: 8.059247, acc.: 50.00%] [G loss: -0.000391]\n",
            "636 [D loss: 8.059236, acc.: 50.00%] [G loss: -0.000385]\n",
            "637 [D loss: 8.059249, acc.: 50.00%] [G loss: -0.000365]\n",
            "638 [D loss: 8.059212, acc.: 50.00%] [G loss: -0.000375]\n",
            "639 [D loss: 8.059226, acc.: 50.00%] [G loss: -0.000389]\n",
            "640 [D loss: 8.059254, acc.: 50.00%] [G loss: -0.000387]\n",
            "641 [D loss: 8.059234, acc.: 50.00%] [G loss: -0.000347]\n",
            "642 [D loss: 8.059241, acc.: 50.00%] [G loss: -0.000358]\n",
            "643 [D loss: 8.059230, acc.: 50.00%] [G loss: -0.000397]\n",
            "644 [D loss: 8.059266, acc.: 50.00%] [G loss: -0.000344]\n",
            "645 [D loss: 8.059236, acc.: 50.00%] [G loss: -0.000382]\n",
            "646 [D loss: 8.059236, acc.: 50.00%] [G loss: -0.000390]\n",
            "647 [D loss: 8.059246, acc.: 50.00%] [G loss: -0.000359]\n",
            "648 [D loss: 8.059242, acc.: 50.00%] [G loss: -0.000381]\n",
            "649 [D loss: 8.059234, acc.: 50.00%] [G loss: -0.000402]\n",
            "650 [D loss: 8.059225, acc.: 50.00%] [G loss: -0.000358]\n",
            "651 [D loss: 8.059219, acc.: 50.00%] [G loss: -0.000366]\n",
            "652 [D loss: 8.059244, acc.: 50.00%] [G loss: -0.000351]\n",
            "653 [D loss: 8.059226, acc.: 50.00%] [G loss: -0.000383]\n",
            "654 [D loss: 8.059231, acc.: 50.00%] [G loss: -0.000353]\n",
            "655 [D loss: 8.059234, acc.: 50.00%] [G loss: -0.000385]\n",
            "656 [D loss: 8.059239, acc.: 50.00%] [G loss: -0.000375]\n",
            "657 [D loss: 8.059239, acc.: 50.00%] [G loss: -0.000391]\n",
            "658 [D loss: 8.059242, acc.: 50.00%] [G loss: -0.000416]\n",
            "659 [D loss: 8.059225, acc.: 50.00%] [G loss: -0.000357]\n",
            "660 [D loss: 8.059219, acc.: 50.00%] [G loss: -0.000366]\n",
            "661 [D loss: 8.059232, acc.: 50.00%] [G loss: -0.000354]\n",
            "662 [D loss: 8.059243, acc.: 50.00%] [G loss: -0.000390]\n",
            "663 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000359]\n",
            "664 [D loss: 8.059227, acc.: 50.00%] [G loss: -0.000364]\n",
            "665 [D loss: 8.059241, acc.: 50.00%] [G loss: -0.000373]\n",
            "666 [D loss: 8.059230, acc.: 50.00%] [G loss: -0.000368]\n",
            "667 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000381]\n",
            "668 [D loss: 8.059229, acc.: 50.00%] [G loss: -0.000340]\n",
            "669 [D loss: 8.059241, acc.: 50.00%] [G loss: -0.000363]\n",
            "670 [D loss: 8.059234, acc.: 50.00%] [G loss: -0.000386]\n",
            "671 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000374]\n",
            "672 [D loss: 8.059256, acc.: 50.00%] [G loss: -0.000367]\n",
            "673 [D loss: 8.059233, acc.: 50.00%] [G loss: -0.000363]\n",
            "674 [D loss: 8.059233, acc.: 50.00%] [G loss: -0.000367]\n",
            "675 [D loss: 8.059234, acc.: 50.00%] [G loss: -0.000388]\n",
            "676 [D loss: 8.059240, acc.: 50.00%] [G loss: -0.000385]\n",
            "677 [D loss: 8.059230, acc.: 50.00%] [G loss: -0.000386]\n",
            "678 [D loss: 8.059236, acc.: 50.00%] [G loss: -0.000393]\n",
            "679 [D loss: 8.059244, acc.: 50.00%] [G loss: -0.000360]\n",
            "680 [D loss: 8.059240, acc.: 50.00%] [G loss: -0.000379]\n",
            "681 [D loss: 8.059238, acc.: 50.00%] [G loss: -0.000389]\n",
            "682 [D loss: 8.059221, acc.: 50.00%] [G loss: -0.000356]\n",
            "683 [D loss: 8.059229, acc.: 50.00%] [G loss: -0.000373]\n",
            "684 [D loss: 8.059236, acc.: 50.00%] [G loss: -0.000388]\n",
            "685 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000392]\n",
            "686 [D loss: 8.059250, acc.: 50.00%] [G loss: -0.000372]\n",
            "687 [D loss: 8.059240, acc.: 50.00%] [G loss: -0.000384]\n",
            "688 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000366]\n",
            "689 [D loss: 8.059244, acc.: 50.00%] [G loss: -0.000386]\n",
            "690 [D loss: 8.059254, acc.: 50.00%] [G loss: -0.000361]\n",
            "691 [D loss: 8.059252, acc.: 50.00%] [G loss: -0.000375]\n",
            "692 [D loss: 8.059245, acc.: 50.00%] [G loss: -0.000375]\n",
            "693 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000373]\n",
            "694 [D loss: 8.059233, acc.: 50.00%] [G loss: -0.000372]\n",
            "695 [D loss: 8.059234, acc.: 50.00%] [G loss: -0.000372]\n",
            "696 [D loss: 8.059244, acc.: 50.00%] [G loss: -0.000373]\n",
            "697 [D loss: 8.059233, acc.: 50.00%] [G loss: -0.000387]\n",
            "698 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000418]\n",
            "699 [D loss: 8.059255, acc.: 50.00%] [G loss: -0.000378]\n",
            "700 [D loss: 8.059233, acc.: 50.00%] [G loss: -0.000402]\n",
            "701 [D loss: 8.059248, acc.: 50.00%] [G loss: -0.000351]\n",
            "702 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000356]\n",
            "703 [D loss: 8.059238, acc.: 50.00%] [G loss: -0.000383]\n",
            "704 [D loss: 8.059225, acc.: 50.00%] [G loss: -0.000359]\n",
            "705 [D loss: 8.059239, acc.: 50.00%] [G loss: -0.000365]\n",
            "706 [D loss: 8.059239, acc.: 50.00%] [G loss: -0.000381]\n",
            "707 [D loss: 8.059251, acc.: 50.00%] [G loss: -0.000383]\n",
            "708 [D loss: 8.059234, acc.: 50.00%] [G loss: -0.000389]\n",
            "709 [D loss: 8.059238, acc.: 50.00%] [G loss: -0.000366]\n",
            "710 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000385]\n",
            "711 [D loss: 8.059221, acc.: 50.00%] [G loss: -0.000386]\n",
            "712 [D loss: 8.059247, acc.: 50.00%] [G loss: -0.000388]\n",
            "713 [D loss: 8.059231, acc.: 50.00%] [G loss: -0.000364]\n",
            "714 [D loss: 8.059218, acc.: 50.00%] [G loss: -0.000365]\n",
            "715 [D loss: 8.059221, acc.: 50.00%] [G loss: -0.000394]\n",
            "716 [D loss: 8.059247, acc.: 50.00%] [G loss: -0.000353]\n",
            "717 [D loss: 8.059240, acc.: 50.00%] [G loss: -0.000375]\n",
            "718 [D loss: 8.059245, acc.: 50.00%] [G loss: -0.000390]\n",
            "719 [D loss: 8.059231, acc.: 50.00%] [G loss: -0.000331]\n",
            "720 [D loss: 8.059227, acc.: 50.00%] [G loss: -0.000371]\n",
            "721 [D loss: 8.059230, acc.: 50.00%] [G loss: -0.000363]\n",
            "722 [D loss: 8.059247, acc.: 50.00%] [G loss: -0.000362]\n",
            "723 [D loss: 8.059240, acc.: 50.00%] [G loss: -0.000401]\n",
            "724 [D loss: 8.059222, acc.: 50.00%] [G loss: -0.000365]\n",
            "725 [D loss: 8.059235, acc.: 50.00%] [G loss: -0.000391]\n",
            "726 [D loss: 8.059235, acc.: 50.00%] [G loss: -0.000373]\n",
            "727 [D loss: 8.059242, acc.: 50.00%] [G loss: -0.000367]\n",
            "728 [D loss: 8.059246, acc.: 50.00%] [G loss: -0.000379]\n",
            "729 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000356]\n",
            "730 [D loss: 8.059231, acc.: 50.00%] [G loss: -0.000372]\n",
            "731 [D loss: 8.059233, acc.: 50.00%] [G loss: -0.000368]\n",
            "732 [D loss: 8.059254, acc.: 50.00%] [G loss: -0.000374]\n",
            "733 [D loss: 8.059241, acc.: 50.00%] [G loss: -0.000370]\n",
            "734 [D loss: 8.059238, acc.: 50.00%] [G loss: -0.000408]\n",
            "735 [D loss: 8.059248, acc.: 50.00%] [G loss: -0.000358]\n",
            "736 [D loss: 8.059231, acc.: 50.00%] [G loss: -0.000371]\n",
            "737 [D loss: 8.059232, acc.: 50.00%] [G loss: -0.000365]\n",
            "738 [D loss: 8.059232, acc.: 50.00%] [G loss: -0.000385]\n",
            "739 [D loss: 8.059229, acc.: 50.00%] [G loss: -0.000325]\n",
            "740 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000358]\n",
            "741 [D loss: 8.059226, acc.: 50.00%] [G loss: -0.000343]\n",
            "742 [D loss: 8.059245, acc.: 50.00%] [G loss: -0.000376]\n",
            "743 [D loss: 8.059245, acc.: 50.00%] [G loss: -0.000393]\n",
            "744 [D loss: 8.059243, acc.: 50.00%] [G loss: -0.000359]\n",
            "745 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000374]\n",
            "746 [D loss: 8.059226, acc.: 50.00%] [G loss: -0.000363]\n",
            "747 [D loss: 8.059241, acc.: 50.00%] [G loss: -0.000388]\n",
            "748 [D loss: 8.059237, acc.: 50.00%] [G loss: -0.000380]\n",
            "749 [D loss: 8.059251, acc.: 50.00%] [G loss: -0.000376]\n",
            "750 [D loss: 8.059220, acc.: 50.00%] [G loss: -0.000354]\n",
            "751 [D loss: 8.059231, acc.: 50.00%] [G loss: -0.000391]\n",
            "752 [D loss: 8.059229, acc.: 50.00%] [G loss: -0.000370]\n",
            "753 [D loss: 8.059240, acc.: 50.00%] [G loss: -0.000378]\n",
            "754 [D loss: 8.059222, acc.: 50.00%] [G loss: -0.000393]\n",
            "755 [D loss: 8.059234, acc.: 50.00%] [G loss: -0.000355]\n",
            "756 [D loss: 8.059249, acc.: 50.00%] [G loss: -0.000347]\n",
            "757 [D loss: 8.059244, acc.: 50.00%] [G loss: -0.000347]\n",
            "758 [D loss: 8.059239, acc.: 50.00%] [G loss: -0.000380]\n",
            "759 [D loss: 8.059244, acc.: 50.00%] [G loss: -0.000361]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2ffc740d0849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/test_images.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ca3a37ecf569>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# Generate a batch of new (fake) images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;31m# START INSERT CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "po1Im1Qg1NWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gan.sample_images('images/test_images_5.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_9cesADtg2V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gan.sample_images('images/test_images.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}