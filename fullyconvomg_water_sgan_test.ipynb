{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fatineetiennefast_water_sgan_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FHffsU9Qrs-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d8155f0-deb7-4a5d-eb11-c12653cf71e6"
      },
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import pickle\n",
        "import copy\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.ConfigProto()\n",
        "#config.gpu_options.allow_growth = True\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "session = tf.Session(config=config)\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Activation, ZeroPadding2D, MaxPooling2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.backend.tensorflow_backend import set_session\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "r1ZL-FDtx67z",
        "colab_type": "code",
        "outputId": "5dc72b10-2697-4898-a31a-6f84b3b9a2bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "py4crw0errbL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GAN():\n",
        "    def __init__(self,dataset_name='',load_model_name=''):\n",
        "        \n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        \n",
        "        if (load_model_name == ''):\n",
        "            #X_train = self.load_gan_data(dataset_name)\n",
        "            \n",
        "            #print(\"Loading dataset...\")\n",
        "            \n",
        "            ndata = 200\n",
        "            patch_size = 128\n",
        "            self.patch_size = patch_size\n",
        "            \n",
        "            self.X_tr = np.zeros((ndata, patch_size, patch_size, 3))\n",
        "            \n",
        "            for i in range(ndata):\n",
        "              #if(i%10==0):\n",
        "                #print(\"Image \",i+1)\n",
        "              im = Image.open(\"drive/My Drive/texture/patchset1/patchno\"+str(i)+\".jpg\")\n",
        "              self.X_tr[i,:,:,:] = np.array(im)/255\n",
        "            \n",
        "            print(\"Dataset loaded.\")\n",
        "            \n",
        "            # default parameters for mnist \n",
        "            self.img_rows = self.X_tr.shape[1]\n",
        "            self.img_cols = self.X_tr.shape[2]\n",
        "            self.img_channels = self.X_tr.shape[3]\n",
        "            self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
        "            \n",
        "            self.k = 4\n",
        "            \n",
        "            self.l = self.patch_size // 2**self.k\n",
        "            self.m = self.patch_size // 2**self.k\n",
        "            \n",
        "            self.z_depth = 16\n",
        "            \n",
        "            self.sigma = 0.02\n",
        "            \n",
        "            self.batch_size = 16\n",
        "            self.iter_count = 0\n",
        "            self.dataset_name = dataset_name\n",
        "            self.model_file = 'models/'+self.dataset_name+'_gan_model.pickle'#\n",
        "\n",
        "            # Build and compile the discriminator and discriminator loss\n",
        "            self.discriminator = self.build_discriminator()\n",
        "            # set discriminator loss\n",
        "            # BEGIN INSERT CODE\n",
        "            self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            # END INSERT CODE\n",
        "\n",
        "            # Build the generator\n",
        "            self.generator = self.build_generator()\n",
        "\n",
        "        else:\n",
        "            #load gan class and models (generator, discriminator and stacked model)\n",
        "            self.load_gan_model(load_model_name)\n",
        "\n",
        "        # Create the stacked model\n",
        "        #first, create the random vector z in the latent space\n",
        "        z = Input(shape=(self.l,self.m,self.z_depth))\n",
        "        #z = Input(shape=(self.z_dim,))\n",
        "        #create generated (fake) image\n",
        "        img = self.generator(z)\n",
        "\n",
        "        #indicate that for the stacked model, the weights are not trained\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and gives a probability of whether it is a true or\n",
        "        #false image\n",
        "        print(\"img : \", img.shape)\n",
        "        p_true = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # In this model, we train the generator only\n",
        "        self.stacked_gen_disc = Model(z, p_true)\n",
        "\n",
        "        # loss\n",
        "        # START INSERT CODE HERE\n",
        "        generator_loss = K.mean(K.log(1 - p_true))\n",
        "        # END INSERT CODE HERE\n",
        "        self.stacked_gen_disc.add_loss(generator_loss)\n",
        "        self.stacked_gen_disc.compile(optimizer=optimizer)\n",
        "\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        \n",
        "        model.add(Conv2D(64, kernel_size=5, input_shape=(self.l,self.m,self.z_depth), padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(self.img_channels, kernel_size=3, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.l,self.m,self.z_depth))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        \n",
        "        model.add(Conv2D(64, kernel_size=5, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        #model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\"))\n",
        "        #model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        #model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(256, kernel_size=5, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        #model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(1, kernel_size=5, strides=2, padding=\"same\"))\n",
        "        model.add(Activation('sigmoid'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def load_gan_data(self,dataset_name):\n",
        "        # Load the dataset\n",
        "        if(dataset_name == 'mnist'):\n",
        "            (X_train, _), (_, _) = mnist.load_data()\n",
        "        elif(dataset_name == 'cifar'):\n",
        "            from keras.datasets import cifar10\n",
        "            (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "        else:\n",
        "            print('Error, unknown database')\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        #add a channel dimension, if need be (for mnist data)\n",
        "        if(X_train.ndim ==3):\n",
        "            X_train = np.expand_dims(X_train, axis=3)\n",
        "        return X_train\n",
        "\n",
        "    def save_gan_model(self, model_file):\n",
        "\n",
        "        #save the GAN class instance\n",
        "        gan_temp = GAN(self.dataset_name,'')\n",
        "        gan_temp.generator = self.generator\n",
        "        gan_temp.discriminator = self.discriminator\n",
        "        gan_temp.stacked_gen_disc = []\n",
        "        gan_temp.iter_count = self.iter_count\n",
        "        with open(model_file,'wb') as file_class:\n",
        "            pickle.dump(gan_temp,file_class,-1)\n",
        "\n",
        "    def load_gan_model(self, model_file):\n",
        "\n",
        "        #load GAN class instance\n",
        "        gan_temp = pickle.load(open(model_file,\"rb\",-1))\n",
        "        #copy parameters\n",
        "        self.img_rows = gan_temp.img_rows \n",
        "        self.img_cols = gan_temp.img_cols \n",
        "        self.img_channels = gan_temp.img_channels \n",
        "        self.img_shape = gan_temp.img_shape\n",
        "        self.z_dim = gan_temp.z_dim\n",
        "        self.iter_count = gan_temp.iter_count\n",
        "        self.model_file = gan_temp.model_file\n",
        "        self.dataset_name = gan_temp.dataset_name\n",
        "\n",
        "        #copy models\n",
        "        self.generator = gan_temp.generator\n",
        "        self.discriminator = gan_temp.discriminator\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        k=1    #number of internal loops\n",
        "\n",
        "        #load dataset\n",
        "        X_train = self.X_tr\n",
        "        # Adversarial ground truths\n",
        "        d_output_true = np.ones((batch_size, self.l, self.m, 1))+(2*np.random.rand(batch_size, self.l, self.m, 1)-1)*0.2\n",
        "        d_output_false = np.zeros((batch_size, self.l, self.m, 1))+0.25*np.random.rand(batch_size, self.l, self.m, 1)\n",
        "\n",
        "        first_iter =self.iter_count\n",
        "\n",
        "        for epoch in range(first_iter,epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the discriminator\n",
        "            for i in range(0,k):\n",
        "                # Select a random batch of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "                imgs = X_train[idx]\n",
        "\n",
        "                z_random = np.random.normal(0, self.sigma, (batch_size, self.l, self.m, self.z_depth))\n",
        "\n",
        "                # Generate a batch of new (fake) images\n",
        "                gen_imgs = self.generator.predict(z_random)\n",
        "                \n",
        "                # START INSERT CODE\n",
        "                d_loss_real = self.discriminator.train_on_batch(imgs, d_output_true)\n",
        "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, d_output_false)\n",
        "                d_loss_real2 = np.mean(self.discriminator.predict(imgs))\n",
        "                d_loss_fake2 = 1-np.mean(self.discriminator.predict(gen_imgs))\n",
        "                # END INSERT CODE\n",
        "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "                d_loss2 = 0.5*(d_loss_real2 + d_loss_fake2)\n",
        "\n",
        "        \n",
        "            \n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            z_random = np.random.normal(0, self.sigma, (batch_size, self.l, self.m, self.z_depth))\n",
        "\n",
        "            # Generate a batch of new (fake) images\n",
        "            gen_imgs = self.generator.predict(z_random)\n",
        "            # Generator training : try to make generated images be classified as true by the discriminator\n",
        "            g_loss = self.stacked_gen_disc.train_on_batch(z_random,None)\n",
        "\n",
        "            # increase epoch counter\n",
        "            self.iter_count = self.iter_count+1\n",
        "            # Plot the losses\n",
        "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss2, g_loss))\n",
        "\n",
        "            # Save some random generated images and the models at every sample_interval iterations\n",
        "            if (epoch % sample_interval == 0):\n",
        "                self.sample_images('images/'+self.dataset_name+'_sample_%06d.png' % epoch)\n",
        "                #self.save_gan_model(self.model_file)\n",
        "\n",
        "    def sample_images(self, image_filename, rand_seed=30):\n",
        "        np.random.seed(rand_seed)\n",
        "\n",
        "        r, c = 2, 2\n",
        "        z_random = np.random.normal(0, self.sigma, (r * c, self.l, self.m, self.z_depth))\n",
        "        gen_imgs = self.generator.predict(z_random)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                #black and white images\n",
        "                if(gen_imgs.shape[3] == 1):\n",
        "                    axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                elif(gen_imgs.shape[3] == 3):   #colour images\n",
        "                    axs[i,j].imshow(gen_imgs[cnt, :,:], vmin=0, vmax=1)\n",
        "                else:\n",
        "                    print('Error, unsupported channel size. Dude, I don''t know what you want me to do.\\\n",
        "                            I can''t handle this data. You''ve made me very sad ...')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(image_filename)\n",
        "        plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQHAcAS-r3Fq",
        "colab_type": "code",
        "outputId": "885df70d-5e8f-40ab-b411-46775a4d36cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33680
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    #create the output image and model directories\n",
        "    if (os.path.isdir('images')==0):\n",
        "        os.mkdir('images')\n",
        "    if (os.path.isdir('models')==0):\n",
        "        os.mkdir('models')\n",
        "\n",
        "    #choose dataset\n",
        "    dataset_name = 'mnist'#\n",
        "\n",
        "    #create GAN model\n",
        "    set_session(session)\n",
        "\n",
        "    #create GAN model\n",
        "    model_file = '_gan_model.pickle'\n",
        "    gan = GAN()#,\n",
        "    is_training = 1\n",
        "\n",
        "    if (is_training ==1):\n",
        "        gan.train(epochs=10001, batch_size=32, sample_interval=10)\n",
        "    else:\n",
        "        gan.sample_images('images/test_images.png')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset loaded.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_163 (Conv2D)          (None, 64, 64, 64)        4864      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_74 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_164 (Conv2D)          (None, 32, 32, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_115 (Bat (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_75 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_165 (Conv2D)          (None, 16, 16, 256)       819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_116 (Bat (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_76 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_166 (Conv2D)          (None, 8, 8, 1)           6401      \n",
            "_________________________________________________________________\n",
            "activation_98 (Activation)   (None, 8, 8, 1)           0         \n",
            "=================================================================\n",
            "Total params: 1,037,185\n",
            "Trainable params: 1,036,417\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_167 (Conv2D)          (None, 8, 8, 64)          25664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_117 (Bat (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation_99 (Activation)   (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_52 (UpSampling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_168 (Conv2D)          (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_118 (Bat (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_100 (Activation)  (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_53 (UpSampling (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_169 (Conv2D)          (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_119 (Bat (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_101 (Activation)  (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_54 (UpSampling (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_170 (Conv2D)          (None, 64, 64, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_120 (Bat (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_102 (Activation)  (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_55 (UpSampling (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_171 (Conv2D)          (None, 128, 128, 3)       1731      \n",
            "_________________________________________________________________\n",
            "activation_103 (Activation)  (None, 128, 128, 3)       0         \n",
            "=================================================================\n",
            "Total params: 213,251\n",
            "Trainable params: 212,611\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n",
            "img :  (?, 128, 128, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.671640, acc.: 53.30%] [G loss: -1.143552]\n",
            "1 [D loss: 0.631495, acc.: 52.59%] [G loss: -0.782524]\n",
            "2 [D loss: 0.391743, acc.: 53.02%] [G loss: -0.426236]\n",
            "3 [D loss: 0.285184, acc.: 55.07%] [G loss: -0.281956]\n",
            "4 [D loss: 0.247653, acc.: 55.04%] [G loss: -0.251154]\n",
            "5 [D loss: 0.231679, acc.: 56.09%] [G loss: -0.287921]\n",
            "6 [D loss: 0.222244, acc.: 56.26%] [G loss: -0.349907]\n",
            "7 [D loss: 0.222254, acc.: 57.03%] [G loss: -0.562753]\n",
            "8 [D loss: 0.277860, acc.: 55.81%] [G loss: -0.279724]\n",
            "9 [D loss: 0.375746, acc.: 57.66%] [G loss: -0.270955]\n",
            "10 [D loss: 0.257827, acc.: 59.02%] [G loss: -0.356938]\n",
            "11 [D loss: 0.232868, acc.: 59.19%] [G loss: -0.380247]\n",
            "12 [D loss: 0.215745, acc.: 57.15%] [G loss: -0.394927]\n",
            "13 [D loss: 0.208774, acc.: 57.22%] [G loss: -0.457419]\n",
            "14 [D loss: 0.211217, acc.: 56.73%] [G loss: -0.451108]\n",
            "15 [D loss: 0.216512, acc.: 57.12%] [G loss: -0.559764]\n",
            "16 [D loss: 0.261086, acc.: 55.03%] [G loss: -0.331593]\n",
            "17 [D loss: 0.267642, acc.: 58.58%] [G loss: -0.470414]\n",
            "18 [D loss: 0.278181, acc.: 57.80%] [G loss: -0.348449]\n",
            "19 [D loss: 0.230476, acc.: 55.43%] [G loss: -0.376926]\n",
            "20 [D loss: 0.212443, acc.: 54.11%] [G loss: -0.459282]\n",
            "21 [D loss: 0.221164, acc.: 52.90%] [G loss: -0.383123]\n",
            "22 [D loss: 0.220264, acc.: 52.43%] [G loss: -0.307694]\n",
            "23 [D loss: 0.352667, acc.: 51.45%] [G loss: -0.148442]\n",
            "24 [D loss: 0.976225, acc.: 48.85%] [G loss: -0.155655]\n",
            "25 [D loss: 0.412425, acc.: 50.19%] [G loss: -0.109054]\n",
            "26 [D loss: 0.435334, acc.: 49.69%] [G loss: -0.192734]\n",
            "27 [D loss: 0.279513, acc.: 50.31%] [G loss: -0.285356]\n",
            "28 [D loss: 0.307191, acc.: 49.86%] [G loss: -0.643279]\n",
            "29 [D loss: 0.249347, acc.: 49.80%] [G loss: -0.942491]\n",
            "30 [D loss: 0.305158, acc.: 49.97%] [G loss: -0.292251]\n",
            "31 [D loss: 0.457269, acc.: 50.21%] [G loss: -0.271135]\n",
            "32 [D loss: 0.273770, acc.: 50.04%] [G loss: -0.306444]\n",
            "33 [D loss: 0.270155, acc.: 49.37%] [G loss: -0.459099]\n",
            "34 [D loss: 0.260656, acc.: 51.14%] [G loss: -0.673407]\n",
            "35 [D loss: 0.230265, acc.: 53.40%] [G loss: -1.367001]\n",
            "36 [D loss: 0.225642, acc.: 52.85%] [G loss: -2.160320]\n",
            "37 [D loss: 0.291095, acc.: 49.13%] [G loss: -0.293414]\n",
            "38 [D loss: 0.281063, acc.: 48.91%] [G loss: -0.344803]\n",
            "39 [D loss: 0.223901, acc.: 48.99%] [G loss: -0.593771]\n",
            "40 [D loss: 0.236812, acc.: 49.31%] [G loss: -0.480550]\n",
            "41 [D loss: 0.220521, acc.: 50.73%] [G loss: -0.568142]\n",
            "42 [D loss: 0.238857, acc.: 52.46%] [G loss: -0.748957]\n",
            "43 [D loss: 0.313289, acc.: 49.45%] [G loss: -0.100392]\n",
            "44 [D loss: 0.360450, acc.: 50.07%] [G loss: -1.390451]\n",
            "45 [D loss: 0.523511, acc.: 50.50%] [G loss: -0.302939]\n",
            "46 [D loss: 0.387336, acc.: 50.80%] [G loss: -0.493218]\n",
            "47 [D loss: 0.317044, acc.: 51.09%] [G loss: -2.093705]\n",
            "48 [D loss: 0.389191, acc.: 48.37%] [G loss: -0.115751]\n",
            "49 [D loss: 0.352417, acc.: 48.43%] [G loss: -0.102916]\n",
            "50 [D loss: 0.298585, acc.: 48.44%] [G loss: -0.324199]\n",
            "51 [D loss: 0.208818, acc.: 49.06%] [G loss: -0.582494]\n",
            "52 [D loss: 0.209732, acc.: 48.91%] [G loss: -0.654583]\n",
            "53 [D loss: 0.202150, acc.: 48.69%] [G loss: -0.540083]\n",
            "54 [D loss: 0.206449, acc.: 48.65%] [G loss: -0.600108]\n",
            "55 [D loss: 0.205442, acc.: 48.57%] [G loss: -0.556486]\n",
            "56 [D loss: 0.208109, acc.: 48.61%] [G loss: -0.538395]\n",
            "57 [D loss: 0.211772, acc.: 48.69%] [G loss: -0.492479]\n",
            "58 [D loss: 0.231031, acc.: 49.04%] [G loss: -0.312822]\n",
            "59 [D loss: 0.214009, acc.: 49.04%] [G loss: -0.269139]\n",
            "60 [D loss: 0.209819, acc.: 49.21%] [G loss: -0.302010]\n",
            "61 [D loss: 0.220818, acc.: 49.42%] [G loss: -0.358163]\n",
            "62 [D loss: 0.210653, acc.: 49.45%] [G loss: -0.341587]\n",
            "63 [D loss: 0.205405, acc.: 49.57%] [G loss: -0.279031]\n",
            "64 [D loss: 0.224811, acc.: 49.67%] [G loss: -0.404981]\n",
            "65 [D loss: 0.211458, acc.: 49.67%] [G loss: -0.394208]\n",
            "66 [D loss: 0.216816, acc.: 49.77%] [G loss: -0.551049]\n",
            "67 [D loss: 0.214453, acc.: 49.89%] [G loss: -0.682109]\n",
            "68 [D loss: 0.214778, acc.: 49.91%] [G loss: -0.820252]\n",
            "69 [D loss: 0.216553, acc.: 50.01%] [G loss: -0.428587]\n",
            "70 [D loss: 0.530919, acc.: 48.97%] [G loss: -0.036749]\n",
            "71 [D loss: 1.228547, acc.: 49.88%] [G loss: -0.412276]\n",
            "72 [D loss: 0.369880, acc.: 49.34%] [G loss: -0.360388]\n",
            "73 [D loss: 0.317940, acc.: 49.58%] [G loss: -0.175888]\n",
            "74 [D loss: 0.637489, acc.: 50.57%] [G loss: -0.214370]\n",
            "75 [D loss: 0.442399, acc.: 50.00%] [G loss: -0.653861]\n",
            "76 [D loss: 0.315841, acc.: 50.47%] [G loss: -0.353099]\n",
            "77 [D loss: 0.325186, acc.: 50.51%] [G loss: -0.276314]\n",
            "78 [D loss: 0.256546, acc.: 50.12%] [G loss: -1.277245]\n",
            "79 [D loss: 0.250140, acc.: 49.67%] [G loss: -0.477724]\n",
            "80 [D loss: 0.207921, acc.: 49.85%] [G loss: -0.607115]\n",
            "81 [D loss: 0.426573, acc.: 50.63%] [G loss: -0.267454]\n",
            "82 [D loss: 0.282965, acc.: 51.38%] [G loss: -0.457148]\n",
            "83 [D loss: 0.264517, acc.: 50.71%] [G loss: -0.343147]\n",
            "84 [D loss: 0.358181, acc.: 51.62%] [G loss: -1.480290]\n",
            "85 [D loss: 0.292911, acc.: 50.39%] [G loss: -0.062174]\n",
            "86 [D loss: 0.291106, acc.: 50.22%] [G loss: -0.107257]\n",
            "87 [D loss: 0.237880, acc.: 50.18%] [G loss: -0.167780]\n",
            "88 [D loss: 0.206967, acc.: 50.46%] [G loss: -0.196201]\n",
            "89 [D loss: 0.202653, acc.: 50.38%] [G loss: -0.242454]\n",
            "90 [D loss: 0.199854, acc.: 50.31%] [G loss: -0.232338]\n",
            "91 [D loss: 0.199546, acc.: 50.54%] [G loss: -0.243419]\n",
            "92 [D loss: 0.199366, acc.: 50.65%] [G loss: -0.240811]\n",
            "93 [D loss: 0.196997, acc.: 50.59%] [G loss: -0.205200]\n",
            "94 [D loss: 0.199487, acc.: 50.67%] [G loss: -0.235780]\n",
            "95 [D loss: 0.198307, acc.: 50.80%] [G loss: -0.188025]\n",
            "96 [D loss: 0.199340, acc.: 50.86%] [G loss: -0.209649]\n",
            "97 [D loss: 0.198923, acc.: 50.77%] [G loss: -0.217632]\n",
            "98 [D loss: 0.209162, acc.: 51.29%] [G loss: -0.174422]\n",
            "99 [D loss: 0.199282, acc.: 51.27%] [G loss: -0.179855]\n",
            "100 [D loss: 0.199558, acc.: 51.07%] [G loss: -0.159806]\n",
            "101 [D loss: 0.200593, acc.: 51.21%] [G loss: -0.180962]\n",
            "102 [D loss: 0.198776, acc.: 51.19%] [G loss: -0.191033]\n",
            "103 [D loss: 0.197170, acc.: 51.13%] [G loss: -0.160599]\n",
            "104 [D loss: 0.200797, acc.: 51.18%] [G loss: -0.196320]\n",
            "105 [D loss: 0.198986, acc.: 51.23%] [G loss: -0.155152]\n",
            "106 [D loss: 0.199942, acc.: 51.27%] [G loss: -0.178160]\n",
            "107 [D loss: 0.199361, acc.: 51.14%] [G loss: -0.184327]\n",
            "108 [D loss: 0.211116, acc.: 51.47%] [G loss: -0.147033]\n",
            "109 [D loss: 0.199538, acc.: 51.41%] [G loss: -0.150753]\n",
            "110 [D loss: 0.199815, acc.: 51.25%] [G loss: -0.132132]\n",
            "111 [D loss: 0.201102, acc.: 51.24%] [G loss: -0.155751]\n",
            "112 [D loss: 0.198571, acc.: 51.13%] [G loss: -0.167227]\n",
            "113 [D loss: 0.196822, acc.: 51.08%] [G loss: -0.141066]\n",
            "114 [D loss: 0.201345, acc.: 51.10%] [G loss: -0.179979]\n",
            "115 [D loss: 0.198739, acc.: 51.07%] [G loss: -0.141303]\n",
            "116 [D loss: 0.199815, acc.: 51.07%] [G loss: -0.169730]\n",
            "117 [D loss: 0.199362, acc.: 50.96%] [G loss: -0.173872]\n",
            "118 [D loss: 0.211018, acc.: 51.13%] [G loss: -0.135442]\n",
            "119 [D loss: 0.199361, acc.: 51.07%] [G loss: -0.138213]\n",
            "120 [D loss: 0.199934, acc.: 50.99%] [G loss: -0.121719]\n",
            "121 [D loss: 0.200963, acc.: 50.94%] [G loss: -0.148759]\n",
            "122 [D loss: 0.197976, acc.: 50.83%] [G loss: -0.162799]\n",
            "123 [D loss: 0.196493, acc.: 50.79%] [G loss: -0.134378]\n",
            "124 [D loss: 0.201507, acc.: 50.82%] [G loss: -0.182107]\n",
            "125 [D loss: 0.198516, acc.: 50.78%] [G loss: -0.135515]\n",
            "126 [D loss: 0.199833, acc.: 50.78%] [G loss: -0.172269]\n",
            "127 [D loss: 0.199558, acc.: 50.71%] [G loss: -0.173231]\n",
            "128 [D loss: 0.214478, acc.: 50.79%] [G loss: -0.125704]\n",
            "129 [D loss: 0.200005, acc.: 50.75%] [G loss: -0.126470]\n",
            "130 [D loss: 0.201162, acc.: 50.70%] [G loss: -0.113860]\n",
            "131 [D loss: 0.202355, acc.: 50.64%] [G loss: -0.147715]\n",
            "132 [D loss: 0.198401, acc.: 50.54%] [G loss: -0.163788]\n",
            "133 [D loss: 0.196666, acc.: 50.50%] [G loss: -0.129375]\n",
            "134 [D loss: 0.203466, acc.: 50.55%] [G loss: -0.190058]\n",
            "135 [D loss: 0.199099, acc.: 50.51%] [G loss: -0.131574]\n",
            "136 [D loss: 0.201247, acc.: 50.48%] [G loss: -0.184418]\n",
            "137 [D loss: 0.201228, acc.: 50.45%] [G loss: -0.179061]\n",
            "138 [D loss: 0.220596, acc.: 50.49%] [G loss: -0.115375]\n",
            "139 [D loss: 0.202987, acc.: 50.46%] [G loss: -0.113828]\n",
            "140 [D loss: 0.204036, acc.: 50.43%] [G loss: -0.111844]\n",
            "141 [D loss: 0.206331, acc.: 50.36%] [G loss: -0.155352]\n",
            "142 [D loss: 0.200760, acc.: 50.27%] [G loss: -0.167189]\n",
            "143 [D loss: 0.197679, acc.: 50.22%] [G loss: -0.120615]\n",
            "144 [D loss: 0.209257, acc.: 50.29%] [G loss: -0.199155]\n",
            "145 [D loss: 0.201039, acc.: 50.26%] [G loss: -0.127074]\n",
            "146 [D loss: 0.205000, acc.: 50.22%] [G loss: -0.202423]\n",
            "147 [D loss: 0.204873, acc.: 50.21%] [G loss: -0.184782]\n",
            "148 [D loss: 0.224103, acc.: 50.20%] [G loss: -0.105959]\n",
            "149 [D loss: 0.209583, acc.: 50.21%] [G loss: -0.110576]\n",
            "150 [D loss: 0.206829, acc.: 50.23%] [G loss: -0.129044]\n",
            "151 [D loss: 0.213605, acc.: 50.14%] [G loss: -0.167939]\n",
            "152 [D loss: 0.204485, acc.: 50.07%] [G loss: -0.155472]\n",
            "153 [D loss: 0.200043, acc.: 50.07%] [G loss: -0.111889]\n",
            "154 [D loss: 0.227131, acc.: 50.19%] [G loss: -0.221681]\n",
            "155 [D loss: 0.213619, acc.: 50.13%] [G loss: -0.106643]\n",
            "156 [D loss: 0.224978, acc.: 50.13%] [G loss: -0.177296]\n",
            "157 [D loss: 0.226363, acc.: 50.17%] [G loss: -0.120458]\n",
            "158 [D loss: 0.228423, acc.: 50.16%] [G loss: -0.081822]\n",
            "159 [D loss: 0.246911, acc.: 50.33%] [G loss: -0.140813]\n",
            "160 [D loss: 0.213703, acc.: 50.26%] [G loss: -0.201019]\n",
            "161 [D loss: 0.280368, acc.: 50.35%] [G loss: -0.085115]\n",
            "162 [D loss: 0.286815, acc.: 50.36%] [G loss: -0.186000]\n",
            "163 [D loss: 0.400805, acc.: 50.85%] [G loss: -0.358361]\n",
            "164 [D loss: 1.975729, acc.: 50.41%] [G loss: -0.501211]\n",
            "165 [D loss: 0.400733, acc.: 50.30%] [G loss: -0.186077]\n",
            "166 [D loss: 0.711229, acc.: 49.63%] [G loss: -0.543323]\n",
            "167 [D loss: 0.315862, acc.: 48.98%] [G loss: -0.606502]\n",
            "168 [D loss: 0.241796, acc.: 49.45%] [G loss: -0.188093]\n",
            "169 [D loss: 0.229639, acc.: 49.40%] [G loss: -0.236147]\n",
            "170 [D loss: 0.212823, acc.: 49.62%] [G loss: -0.243306]\n",
            "171 [D loss: 0.207255, acc.: 49.61%] [G loss: -0.305441]\n",
            "172 [D loss: 0.208997, acc.: 49.71%] [G loss: -0.301786]\n",
            "173 [D loss: 0.202270, acc.: 49.81%] [G loss: -0.210220]\n",
            "174 [D loss: 0.221728, acc.: 49.82%] [G loss: -0.323324]\n",
            "175 [D loss: 0.212191, acc.: 49.81%] [G loss: -0.193607]\n",
            "176 [D loss: 0.228013, acc.: 49.85%] [G loss: -0.297993]\n",
            "177 [D loss: 0.237040, acc.: 49.87%] [G loss: -0.220220]\n",
            "178 [D loss: 0.273198, acc.: 49.99%] [G loss: -0.122596]\n",
            "179 [D loss: 0.314997, acc.: 50.11%] [G loss: -0.188712]\n",
            "180 [D loss: 0.228835, acc.: 50.29%] [G loss: -0.232180]\n",
            "181 [D loss: 0.315654, acc.: 50.31%] [G loss: -0.181879]\n",
            "182 [D loss: 0.238428, acc.: 50.38%] [G loss: -0.207276]\n",
            "183 [D loss: 0.245753, acc.: 50.46%] [G loss: -0.414072]\n",
            "184 [D loss: 0.293620, acc.: 50.15%] [G loss: -0.547972]\n",
            "185 [D loss: 0.549435, acc.: 51.41%] [G loss: -0.023067]\n",
            "186 [D loss: 1.272302, acc.: 51.82%] [G loss: -0.190133]\n",
            "187 [D loss: 2.114751, acc.: 48.59%] [G loss: -1.307608]\n",
            "188 [D loss: 0.684910, acc.: 50.80%] [G loss: -1.000175]\n",
            "189 [D loss: 0.428136, acc.: 50.37%] [G loss: -0.144712]\n",
            "190 [D loss: 0.252942, acc.: 50.18%] [G loss: -0.126610]\n",
            "191 [D loss: 0.231671, acc.: 49.82%] [G loss: -0.166353]\n",
            "192 [D loss: 0.213336, acc.: 49.80%] [G loss: -0.184623]\n",
            "193 [D loss: 0.205050, acc.: 49.90%] [G loss: -0.176397]\n",
            "194 [D loss: 0.211373, acc.: 49.95%] [G loss: -0.223577]\n",
            "195 [D loss: 0.212084, acc.: 50.08%] [G loss: -0.230636]\n",
            "196 [D loss: 0.247519, acc.: 50.33%] [G loss: -0.320551]\n",
            "197 [D loss: 0.488861, acc.: 50.12%] [G loss: -0.720157]\n",
            "198 [D loss: 0.344827, acc.: 49.89%] [G loss: -0.119885]\n",
            "199 [D loss: 0.245338, acc.: 49.86%] [G loss: -0.174207]\n",
            "200 [D loss: 0.214475, acc.: 49.86%] [G loss: -0.191186]\n",
            "201 [D loss: 0.204728, acc.: 49.69%] [G loss: -0.246914]\n",
            "202 [D loss: 0.203826, acc.: 49.76%] [G loss: -0.266780]\n",
            "203 [D loss: 0.199583, acc.: 49.83%] [G loss: -0.203661]\n",
            "204 [D loss: 0.206669, acc.: 49.86%] [G loss: -0.294583]\n",
            "205 [D loss: 0.205474, acc.: 49.90%] [G loss: -0.202117]\n",
            "206 [D loss: 0.208275, acc.: 49.93%] [G loss: -0.246321]\n",
            "207 [D loss: 0.213580, acc.: 49.99%] [G loss: -0.230979]\n",
            "208 [D loss: 0.260313, acc.: 50.02%] [G loss: -0.128500]\n",
            "209 [D loss: 0.224119, acc.: 50.08%] [G loss: -0.121414]\n",
            "210 [D loss: 0.216942, acc.: 50.15%] [G loss: -0.143958]\n",
            "211 [D loss: 0.254605, acc.: 50.14%] [G loss: -0.151132]\n",
            "212 [D loss: 0.225974, acc.: 50.19%] [G loss: -0.134271]\n",
            "213 [D loss: 0.212380, acc.: 50.32%] [G loss: -0.118975]\n",
            "214 [D loss: 0.295921, acc.: 50.45%] [G loss: -0.263573]\n",
            "215 [D loss: 0.240698, acc.: 50.83%] [G loss: -0.259721]\n",
            "216 [D loss: 0.260580, acc.: 51.09%] [G loss: -0.860675]\n",
            "217 [D loss: 0.235593, acc.: 51.95%] [G loss: -0.419691]\n",
            "218 [D loss: 0.607584, acc.: 50.96%] [G loss: -0.092292]\n",
            "219 [D loss: 0.761810, acc.: 49.95%] [G loss: -0.010910]\n",
            "220 [D loss: 1.876150, acc.: 50.18%] [G loss: -0.151757]\n",
            "221 [D loss: 0.901354, acc.: 50.36%] [G loss: -0.058967]\n",
            "222 [D loss: 0.517950, acc.: 50.03%] [G loss: -0.113964]\n",
            "223 [D loss: 0.398424, acc.: 50.03%] [G loss: -0.191057]\n",
            "224 [D loss: 0.408746, acc.: 50.10%] [G loss: -0.255419]\n",
            "225 [D loss: 0.278402, acc.: 50.13%] [G loss: -0.133616]\n",
            "226 [D loss: 0.360441, acc.: 49.98%] [G loss: -0.242672]\n",
            "227 [D loss: 0.245870, acc.: 49.92%] [G loss: -0.323166]\n",
            "228 [D loss: 0.347275, acc.: 49.84%] [G loss: -0.149159]\n",
            "229 [D loss: 0.397817, acc.: 50.14%] [G loss: -0.215178]\n",
            "230 [D loss: 0.300165, acc.: 50.09%] [G loss: -0.318580]\n",
            "231 [D loss: 0.272425, acc.: 49.87%] [G loss: -0.163895]\n",
            "232 [D loss: 0.213242, acc.: 49.84%] [G loss: -0.162314]\n",
            "233 [D loss: 0.207802, acc.: 49.75%] [G loss: -0.141135]\n",
            "234 [D loss: 0.252958, acc.: 49.93%] [G loss: -0.200112]\n",
            "235 [D loss: 0.439465, acc.: 49.83%] [G loss: -0.137108]\n",
            "236 [D loss: 0.730828, acc.: 50.53%] [G loss: -0.766171]\n",
            "237 [D loss: 0.325982, acc.: 51.03%] [G loss: -0.111574]\n",
            "238 [D loss: 0.231176, acc.: 50.36%] [G loss: -0.102148]\n",
            "239 [D loss: 0.212901, acc.: 50.46%] [G loss: -0.118981]\n",
            "240 [D loss: 0.209143, acc.: 50.47%] [G loss: -0.120992]\n",
            "241 [D loss: 0.202378, acc.: 50.31%] [G loss: -0.140548]\n",
            "242 [D loss: 0.198133, acc.: 50.29%] [G loss: -0.167672]\n",
            "243 [D loss: 0.196641, acc.: 50.22%] [G loss: -0.134253]\n",
            "244 [D loss: 0.201869, acc.: 50.18%] [G loss: -0.201385]\n",
            "245 [D loss: 0.199323, acc.: 50.14%] [G loss: -0.156934]\n",
            "246 [D loss: 0.201828, acc.: 50.16%] [G loss: -0.176490]\n",
            "247 [D loss: 0.211074, acc.: 50.11%] [G loss: -0.206052]\n",
            "248 [D loss: 0.278029, acc.: 50.01%] [G loss: -0.161463]\n",
            "249 [D loss: 0.238721, acc.: 49.98%] [G loss: -0.133650]\n",
            "250 [D loss: 0.215041, acc.: 49.84%] [G loss: -0.118310]\n",
            "251 [D loss: 0.233701, acc.: 49.76%] [G loss: -0.214843]\n",
            "252 [D loss: 0.221901, acc.: 50.01%] [G loss: -0.210067]\n",
            "253 [D loss: 0.205194, acc.: 49.78%] [G loss: -0.483309]\n",
            "254 [D loss: 0.233889, acc.: 50.14%] [G loss: -0.123219]\n",
            "255 [D loss: 0.211810, acc.: 50.93%] [G loss: -0.075547]\n",
            "256 [D loss: 0.220282, acc.: 50.92%] [G loss: -0.087457]\n",
            "257 [D loss: 0.213071, acc.: 50.80%] [G loss: -0.112450]\n",
            "258 [D loss: 0.197694, acc.: 50.47%] [G loss: -0.121219]\n",
            "259 [D loss: 0.198566, acc.: 50.47%] [G loss: -0.141487]\n",
            "260 [D loss: 0.197553, acc.: 50.51%] [G loss: -0.146873]\n",
            "261 [D loss: 0.200281, acc.: 50.38%] [G loss: -0.170405]\n",
            "262 [D loss: 0.208856, acc.: 50.45%] [G loss: -0.209650]\n",
            "263 [D loss: 0.260676, acc.: 50.95%] [G loss: -0.135654]\n",
            "264 [D loss: 0.599940, acc.: 53.57%] [G loss: -0.046134]\n",
            "265 [D loss: 0.686333, acc.: 50.51%] [G loss: -0.107998]\n",
            "266 [D loss: 1.042060, acc.: 50.95%] [G loss: -1.117395]\n",
            "267 [D loss: 0.274685, acc.: 52.15%] [G loss: -0.074617]\n",
            "268 [D loss: 0.245416, acc.: 51.25%] [G loss: -0.074646]\n",
            "269 [D loss: 0.216664, acc.: 51.91%] [G loss: -0.083409]\n",
            "270 [D loss: 0.211889, acc.: 52.87%] [G loss: -0.084456]\n",
            "271 [D loss: 0.202492, acc.: 52.21%] [G loss: -0.101022]\n",
            "272 [D loss: 0.201350, acc.: 52.16%] [G loss: -0.111169]\n",
            "273 [D loss: 0.200079, acc.: 52.46%] [G loss: -0.110441]\n",
            "274 [D loss: 0.200041, acc.: 52.53%] [G loss: -0.122379]\n",
            "275 [D loss: 0.196187, acc.: 52.53%] [G loss: -0.122674]\n",
            "276 [D loss: 0.197060, acc.: 52.68%] [G loss: -0.127231]\n",
            "277 [D loss: 0.197941, acc.: 52.94%] [G loss: -0.145631]\n",
            "278 [D loss: 0.194061, acc.: 51.98%] [G loss: -0.133572]\n",
            "279 [D loss: 0.195990, acc.: 52.38%] [G loss: -0.143365]\n",
            "280 [D loss: 0.196353, acc.: 53.19%] [G loss: -0.132368]\n",
            "281 [D loss: 0.193727, acc.: 52.43%] [G loss: -0.141796]\n",
            "282 [D loss: 0.195183, acc.: 52.29%] [G loss: -0.144878]\n",
            "283 [D loss: 0.194659, acc.: 52.54%] [G loss: -0.134501]\n",
            "284 [D loss: 0.195722, acc.: 52.54%] [G loss: -0.144728]\n",
            "285 [D loss: 0.193714, acc.: 52.45%] [G loss: -0.138804]\n",
            "286 [D loss: 0.194600, acc.: 52.51%] [G loss: -0.139096]\n",
            "287 [D loss: 0.195375, acc.: 52.73%] [G loss: -0.158061]\n",
            "288 [D loss: 0.193678, acc.: 51.78%] [G loss: -0.142206]\n",
            "289 [D loss: 0.194893, acc.: 52.12%] [G loss: -0.150938]\n",
            "290 [D loss: 0.194971, acc.: 52.87%] [G loss: -0.137237]\n",
            "291 [D loss: 0.192918, acc.: 52.15%] [G loss: -0.146430]\n",
            "292 [D loss: 0.194125, acc.: 51.98%] [G loss: -0.148388]\n",
            "293 [D loss: 0.193631, acc.: 52.20%] [G loss: -0.136365]\n",
            "294 [D loss: 0.194793, acc.: 52.21%] [G loss: -0.147451]\n",
            "295 [D loss: 0.193010, acc.: 52.11%] [G loss: -0.140491]\n",
            "296 [D loss: 0.193747, acc.: 52.13%] [G loss: -0.140274]\n",
            "297 [D loss: 0.194502, acc.: 52.35%] [G loss: -0.160179]\n",
            "298 [D loss: 0.193467, acc.: 51.51%] [G loss: -0.143433]\n",
            "299 [D loss: 0.194265, acc.: 51.81%] [G loss: -0.152196]\n",
            "300 [D loss: 0.194226, acc.: 52.49%] [G loss: -0.137634]\n",
            "301 [D loss: 0.192320, acc.: 51.84%] [G loss: -0.147561]\n",
            "302 [D loss: 0.193388, acc.: 51.68%] [G loss: -0.149533]\n",
            "303 [D loss: 0.192888, acc.: 51.86%] [G loss: -0.136385]\n",
            "304 [D loss: 0.194151, acc.: 51.88%] [G loss: -0.148995]\n",
            "305 [D loss: 0.192443, acc.: 51.78%] [G loss: -0.141221]\n",
            "306 [D loss: 0.193107, acc.: 51.78%] [G loss: -0.140727]\n",
            "307 [D loss: 0.193857, acc.: 51.99%] [G loss: -0.161627]\n",
            "308 [D loss: 0.193378, acc.: 51.25%] [G loss: -0.144094]\n",
            "309 [D loss: 0.193791, acc.: 51.51%] [G loss: -0.152919]\n",
            "310 [D loss: 0.193596, acc.: 52.14%] [G loss: -0.137577]\n",
            "311 [D loss: 0.191815, acc.: 51.55%] [G loss: -0.148130]\n",
            "312 [D loss: 0.192784, acc.: 51.40%] [G loss: -0.150330]\n",
            "313 [D loss: 0.192246, acc.: 51.57%] [G loss: -0.135971]\n",
            "314 [D loss: 0.193635, acc.: 51.59%] [G loss: -0.150556]\n",
            "315 [D loss: 0.191927, acc.: 51.50%] [G loss: -0.141857]\n",
            "316 [D loss: 0.192563, acc.: 51.48%] [G loss: -0.141002]\n",
            "317 [D loss: 0.193331, acc.: 51.69%] [G loss: -0.162786]\n",
            "318 [D loss: 0.193351, acc.: 51.02%] [G loss: -0.144536]\n",
            "319 [D loss: 0.193373, acc.: 51.27%] [G loss: -0.153448]\n",
            "320 [D loss: 0.193034, acc.: 51.85%] [G loss: -0.137558]\n",
            "321 [D loss: 0.191380, acc.: 51.31%] [G loss: -0.148433]\n",
            "322 [D loss: 0.192271, acc.: 51.17%] [G loss: -0.151018]\n",
            "323 [D loss: 0.191677, acc.: 51.31%] [G loss: -0.135337]\n",
            "324 [D loss: 0.193178, acc.: 51.33%] [G loss: -0.152379]\n",
            "325 [D loss: 0.191479, acc.: 51.24%] [G loss: -0.142391]\n",
            "326 [D loss: 0.192089, acc.: 51.21%] [G loss: -0.141153]\n",
            "327 [D loss: 0.192875, acc.: 51.42%] [G loss: -0.163568]\n",
            "328 [D loss: 0.193459, acc.: 50.82%] [G loss: -0.144520]\n",
            "329 [D loss: 0.193018, acc.: 51.05%] [G loss: -0.153632]\n",
            "330 [D loss: 0.192521, acc.: 51.58%] [G loss: -0.137417]\n",
            "331 [D loss: 0.190995, acc.: 51.10%] [G loss: -0.148470]\n",
            "332 [D loss: 0.191813, acc.: 50.96%] [G loss: -0.151983]\n",
            "333 [D loss: 0.191195, acc.: 51.09%] [G loss: -0.134609]\n",
            "334 [D loss: 0.192808, acc.: 51.11%] [G loss: -0.155093]\n",
            "335 [D loss: 0.191109, acc.: 51.04%] [G loss: -0.143303]\n",
            "336 [D loss: 0.191699, acc.: 50.99%] [G loss: -0.141919]\n",
            "337 [D loss: 0.192486, acc.: 51.18%] [G loss: -0.164906]\n",
            "338 [D loss: 0.193962, acc.: 50.65%] [G loss: -0.144971]\n",
            "339 [D loss: 0.192884, acc.: 50.86%] [G loss: -0.153908]\n",
            "340 [D loss: 0.192024, acc.: 51.33%] [G loss: -0.137238]\n",
            "341 [D loss: 0.190759, acc.: 50.90%] [G loss: -0.147830]\n",
            "342 [D loss: 0.191470, acc.: 50.76%] [G loss: -0.152493]\n",
            "343 [D loss: 0.190812, acc.: 50.86%] [G loss: -0.132406]\n",
            "344 [D loss: 0.192557, acc.: 50.89%] [G loss: -0.157466]\n",
            "345 [D loss: 0.190857, acc.: 50.83%] [G loss: -0.142995]\n",
            "346 [D loss: 0.191406, acc.: 50.77%] [G loss: -0.141511]\n",
            "347 [D loss: 0.192181, acc.: 50.94%] [G loss: -0.164776]\n",
            "348 [D loss: 0.195079, acc.: 50.48%] [G loss: -0.143866]\n",
            "349 [D loss: 0.193098, acc.: 50.67%] [G loss: -0.152158]\n",
            "350 [D loss: 0.191592, acc.: 51.05%] [G loss: -0.135808]\n",
            "351 [D loss: 0.190817, acc.: 50.70%] [G loss: -0.145485]\n",
            "352 [D loss: 0.191431, acc.: 50.56%] [G loss: -0.151726]\n",
            "353 [D loss: 0.190672, acc.: 50.64%] [G loss: -0.127961]\n",
            "354 [D loss: 0.192562, acc.: 50.67%] [G loss: -0.160217]\n",
            "355 [D loss: 0.190809, acc.: 50.64%] [G loss: -0.141559]\n",
            "356 [D loss: 0.191406, acc.: 50.55%] [G loss: -0.141033]\n",
            "357 [D loss: 0.192204, acc.: 50.69%] [G loss: -0.164473]\n",
            "358 [D loss: 0.198243, acc.: 50.37%] [G loss: -0.142112]\n",
            "359 [D loss: 0.194695, acc.: 50.55%] [G loss: -0.148571]\n",
            "360 [D loss: 0.191419, acc.: 50.82%] [G loss: -0.132654]\n",
            "361 [D loss: 0.192488, acc.: 50.57%] [G loss: -0.139369]\n",
            "362 [D loss: 0.193437, acc.: 50.46%] [G loss: -0.148079]\n",
            "363 [D loss: 0.192731, acc.: 50.53%] [G loss: -0.116877]\n",
            "364 [D loss: 0.195359, acc.: 50.58%] [G loss: -0.159476]\n",
            "365 [D loss: 0.193055, acc.: 50.58%] [G loss: -0.132123]\n",
            "366 [D loss: 0.196280, acc.: 50.49%] [G loss: -0.134573]\n",
            "367 [D loss: 0.202292, acc.: 50.74%] [G loss: -0.151425]\n",
            "368 [D loss: 0.246181, acc.: 50.80%] [G loss: -0.115514]\n",
            "369 [D loss: 0.222193, acc.: 51.18%] [G loss: -0.102067]\n",
            "370 [D loss: 0.208635, acc.: 51.46%] [G loss: -0.088092]\n",
            "371 [D loss: 0.243014, acc.: 51.49%] [G loss: -0.085900]\n",
            "372 [D loss: 0.224061, acc.: 51.56%] [G loss: -0.096801]\n",
            "373 [D loss: 0.226816, acc.: 51.57%] [G loss: -0.076768]\n",
            "374 [D loss: 0.288657, acc.: 51.67%] [G loss: -0.085269]\n",
            "375 [D loss: 0.259715, acc.: 51.67%] [G loss: -0.083750]\n",
            "376 [D loss: 0.329415, acc.: 51.80%] [G loss: -0.110047]\n",
            "377 [D loss: 0.296474, acc.: 50.95%] [G loss: -0.461843]\n",
            "378 [D loss: 0.242541, acc.: 50.47%] [G loss: -0.157566]\n",
            "379 [D loss: 0.208205, acc.: 49.87%] [G loss: -0.424378]\n",
            "380 [D loss: 0.208493, acc.: 49.84%] [G loss: -0.293761]\n",
            "381 [D loss: 0.199171, acc.: 49.77%] [G loss: -0.224523]\n",
            "382 [D loss: 0.197111, acc.: 49.82%] [G loss: -0.308188]\n",
            "383 [D loss: 0.202617, acc.: 49.87%] [G loss: -0.194279]\n",
            "384 [D loss: 0.210041, acc.: 49.91%] [G loss: -0.341890]\n",
            "385 [D loss: 0.206612, acc.: 49.84%] [G loss: -0.177765]\n",
            "386 [D loss: 0.213375, acc.: 49.75%] [G loss: -0.191179]\n",
            "387 [D loss: 0.212238, acc.: 49.86%] [G loss: -0.244397]\n",
            "388 [D loss: 0.275598, acc.: 49.70%] [G loss: -0.137642]\n",
            "389 [D loss: 0.255278, acc.: 49.76%] [G loss: -0.132469]\n",
            "390 [D loss: 0.224541, acc.: 49.88%] [G loss: -0.180376]\n",
            "391 [D loss: 0.255190, acc.: 49.75%] [G loss: -0.144699]\n",
            "392 [D loss: 0.216263, acc.: 49.64%] [G loss: -0.140365]\n",
            "393 [D loss: 0.213954, acc.: 49.64%] [G loss: -0.114576]\n",
            "394 [D loss: 0.274801, acc.: 49.75%] [G loss: -0.164353]\n",
            "395 [D loss: 0.227273, acc.: 49.80%] [G loss: -0.089646]\n",
            "396 [D loss: 0.299380, acc.: 50.02%] [G loss: -0.152985]\n",
            "397 [D loss: 0.237529, acc.: 49.76%] [G loss: -0.139020]\n",
            "398 [D loss: 0.295396, acc.: 50.01%] [G loss: -0.045499]\n",
            "399 [D loss: 0.644466, acc.: 49.14%] [G loss: -0.055881]\n",
            "400 [D loss: 0.513211, acc.: 50.18%] [G loss: -0.326883]\n",
            "401 [D loss: 1.237060, acc.: 50.73%] [G loss: -0.201742]\n",
            "402 [D loss: 0.269163, acc.: 50.96%] [G loss: -0.128498]\n",
            "403 [D loss: 0.533430, acc.: 49.92%] [G loss: -0.667958]\n",
            "404 [D loss: 0.288776, acc.: 49.68%] [G loss: -0.290894]\n",
            "405 [D loss: 0.212257, acc.: 49.60%] [G loss: -0.340052]\n",
            "406 [D loss: 0.251025, acc.: 49.68%] [G loss: -0.324359]\n",
            "407 [D loss: 0.249508, acc.: 49.61%] [G loss: -0.199848]\n",
            "408 [D loss: 0.475431, acc.: 49.88%] [G loss: -0.065038]\n",
            "409 [D loss: 0.697477, acc.: 50.23%] [G loss: -0.278674]\n",
            "410 [D loss: 0.633759, acc.: 50.67%] [G loss: -0.332258]\n",
            "411 [D loss: 0.690194, acc.: 50.87%] [G loss: -0.114971]\n",
            "412 [D loss: 0.311363, acc.: 50.90%] [G loss: -0.034624]\n",
            "413 [D loss: 0.317004, acc.: 50.68%] [G loss: -0.046160]\n",
            "414 [D loss: 0.221396, acc.: 50.59%] [G loss: -0.134170]\n",
            "415 [D loss: 0.215514, acc.: 50.76%] [G loss: -0.142687]\n",
            "416 [D loss: 0.258352, acc.: 50.85%] [G loss: -0.134881]\n",
            "417 [D loss: 0.730477, acc.: 50.89%] [G loss: -0.045105]\n",
            "418 [D loss: 1.265589, acc.: 50.40%] [G loss: -0.412836]\n",
            "419 [D loss: 1.118335, acc.: 49.16%] [G loss: -0.845423]\n",
            "420 [D loss: 0.302268, acc.: 48.62%] [G loss: -0.082214]\n",
            "421 [D loss: 0.301189, acc.: 47.97%] [G loss: -0.092780]\n",
            "422 [D loss: 0.222200, acc.: 47.46%] [G loss: -0.126321]\n",
            "423 [D loss: 0.208951, acc.: 47.25%] [G loss: -0.109019]\n",
            "424 [D loss: 0.212371, acc.: 47.24%] [G loss: -0.180743]\n",
            "425 [D loss: 0.201282, acc.: 47.36%] [G loss: -0.154364]\n",
            "426 [D loss: 0.202665, acc.: 47.33%] [G loss: -0.171007]\n",
            "427 [D loss: 0.200252, acc.: 47.47%] [G loss: -0.185546]\n",
            "428 [D loss: 0.205355, acc.: 47.81%] [G loss: -0.149551]\n",
            "429 [D loss: 0.201184, acc.: 47.78%] [G loss: -0.149735]\n",
            "430 [D loss: 0.196237, acc.: 47.85%] [G loss: -0.149688]\n",
            "431 [D loss: 0.198442, acc.: 47.95%] [G loss: -0.149113]\n",
            "432 [D loss: 0.202266, acc.: 48.01%] [G loss: -0.175408]\n",
            "433 [D loss: 0.197724, acc.: 48.02%] [G loss: -0.119944]\n",
            "434 [D loss: 0.202549, acc.: 48.06%] [G loss: -0.203719]\n",
            "435 [D loss: 0.200594, acc.: 48.28%] [G loss: -0.141983]\n",
            "436 [D loss: 0.201863, acc.: 48.18%] [G loss: -0.155869]\n",
            "437 [D loss: 0.203741, acc.: 48.44%] [G loss: -0.153900]\n",
            "438 [D loss: 0.236088, acc.: 48.62%] [G loss: -0.113520]\n",
            "439 [D loss: 0.211698, acc.: 48.75%] [G loss: -0.104313]\n",
            "440 [D loss: 0.202925, acc.: 49.03%] [G loss: -0.101589]\n",
            "441 [D loss: 0.214798, acc.: 49.05%] [G loss: -0.106210]\n",
            "442 [D loss: 0.210260, acc.: 49.16%] [G loss: -0.136347]\n",
            "443 [D loss: 0.217803, acc.: 49.45%] [G loss: -0.074640]\n",
            "444 [D loss: 0.231968, acc.: 49.68%] [G loss: -0.168714]\n",
            "445 [D loss: 0.246183, acc.: 50.16%] [G loss: -0.090108]\n",
            "446 [D loss: 0.256453, acc.: 50.43%] [G loss: -0.170604]\n",
            "447 [D loss: 0.223504, acc.: 50.31%] [G loss: -0.746525]\n",
            "448 [D loss: 0.272113, acc.: 49.88%] [G loss: -0.066959]\n",
            "449 [D loss: 0.259099, acc.: 49.85%] [G loss: -0.101255]\n",
            "450 [D loss: 0.239788, acc.: 50.05%] [G loss: -0.136728]\n",
            "451 [D loss: 0.209175, acc.: 49.61%] [G loss: -0.206623]\n",
            "452 [D loss: 0.199707, acc.: 49.58%] [G loss: -0.299931]\n",
            "453 [D loss: 0.196266, acc.: 49.69%] [G loss: -0.230291]\n",
            "454 [D loss: 0.199445, acc.: 49.73%] [G loss: -0.371427]\n",
            "455 [D loss: 0.199789, acc.: 49.79%] [G loss: -0.240514]\n",
            "456 [D loss: 0.202941, acc.: 49.72%] [G loss: -0.246392]\n",
            "457 [D loss: 0.198329, acc.: 49.81%] [G loss: -0.339516]\n",
            "458 [D loss: 0.235007, acc.: 49.71%] [G loss: -0.139983]\n",
            "459 [D loss: 0.202768, acc.: 49.66%] [G loss: -0.131472]\n",
            "460 [D loss: 0.201373, acc.: 49.70%] [G loss: -0.128829]\n",
            "461 [D loss: 0.203299, acc.: 49.66%] [G loss: -0.154365]\n",
            "462 [D loss: 0.203499, acc.: 49.61%] [G loss: -0.216124]\n",
            "463 [D loss: 0.198320, acc.: 49.52%] [G loss: -0.128084]\n",
            "464 [D loss: 0.208957, acc.: 49.53%] [G loss: -0.307615]\n",
            "465 [D loss: 0.200668, acc.: 49.51%] [G loss: -0.182283]\n",
            "466 [D loss: 0.208315, acc.: 49.43%] [G loss: -0.217159]\n",
            "467 [D loss: 0.207312, acc.: 49.45%] [G loss: -0.322760]\n",
            "468 [D loss: 0.258055, acc.: 49.34%] [G loss: -0.104912]\n",
            "469 [D loss: 0.214842, acc.: 49.37%] [G loss: -0.096742]\n",
            "470 [D loss: 0.211231, acc.: 49.65%] [G loss: -0.112592]\n",
            "471 [D loss: 0.223473, acc.: 49.80%] [G loss: -0.142423]\n",
            "472 [D loss: 0.217356, acc.: 49.68%] [G loss: -0.169506]\n",
            "473 [D loss: 0.201621, acc.: 49.56%] [G loss: -0.080734]\n",
            "474 [D loss: 0.234451, acc.: 49.75%] [G loss: -0.307665]\n",
            "475 [D loss: 0.205436, acc.: 49.74%] [G loss: -0.144441]\n",
            "476 [D loss: 0.213740, acc.: 49.70%] [G loss: -0.186707]\n",
            "477 [D loss: 0.209713, acc.: 49.86%] [G loss: -0.278636]\n",
            "478 [D loss: 0.251012, acc.: 50.12%] [G loss: -0.060382]\n",
            "479 [D loss: 0.235260, acc.: 50.10%] [G loss: -0.073483]\n",
            "480 [D loss: 0.207525, acc.: 50.04%] [G loss: -0.116971]\n",
            "481 [D loss: 0.226413, acc.: 50.03%] [G loss: -0.115206]\n",
            "482 [D loss: 0.203638, acc.: 50.06%] [G loss: -0.139697]\n",
            "483 [D loss: 0.203878, acc.: 49.95%] [G loss: -0.070788]\n",
            "484 [D loss: 0.235713, acc.: 49.75%] [G loss: -0.350617]\n",
            "485 [D loss: 0.203381, acc.: 49.59%] [G loss: -0.136875]\n",
            "486 [D loss: 0.215088, acc.: 49.65%] [G loss: -0.264754]\n",
            "487 [D loss: 0.210369, acc.: 50.19%] [G loss: -0.442199]\n",
            "488 [D loss: 0.217250, acc.: 49.81%] [G loss: -0.081029]\n",
            "489 [D loss: 0.218858, acc.: 49.31%] [G loss: -0.096883]\n",
            "490 [D loss: 0.200500, acc.: 49.44%] [G loss: -0.209214]\n",
            "491 [D loss: 0.290149, acc.: 50.31%] [G loss: -0.127588]\n",
            "492 [D loss: 0.274550, acc.: 51.42%] [G loss: -0.071412]\n",
            "493 [D loss: 0.275388, acc.: 50.59%] [G loss: -0.111612]\n",
            "494 [D loss: 0.223136, acc.: 50.23%] [G loss: -0.755215]\n",
            "495 [D loss: 0.212702, acc.: 51.11%] [G loss: -0.104714]\n",
            "496 [D loss: 0.229951, acc.: 51.82%] [G loss: -0.073892]\n",
            "497 [D loss: 0.219770, acc.: 52.20%] [G loss: -0.144918]\n",
            "498 [D loss: 0.203620, acc.: 51.05%] [G loss: -0.116294]\n",
            "499 [D loss: 0.203101, acc.: 51.01%] [G loss: -0.114448]\n",
            "500 [D loss: 0.199117, acc.: 50.65%] [G loss: -0.118440]\n",
            "501 [D loss: 0.220115, acc.: 49.14%] [G loss: -0.117269]\n",
            "502 [D loss: 0.203506, acc.: 48.43%] [G loss: -0.115869]\n",
            "503 [D loss: 0.202979, acc.: 48.26%] [G loss: -0.085260]\n",
            "504 [D loss: 0.233140, acc.: 48.08%] [G loss: -0.220336]\n",
            "505 [D loss: 0.208175, acc.: 48.36%] [G loss: -0.134105]\n",
            "506 [D loss: 0.247039, acc.: 48.87%] [G loss: -0.224025]\n",
            "507 [D loss: 0.225894, acc.: 49.53%] [G loss: -0.175451]\n",
            "508 [D loss: 0.218765, acc.: 49.79%] [G loss: -0.049011]\n",
            "509 [D loss: 0.225472, acc.: 49.38%] [G loss: -0.083198]\n",
            "510 [D loss: 0.203964, acc.: 49.10%] [G loss: -0.098740]\n",
            "511 [D loss: 0.218568, acc.: 49.41%] [G loss: -0.073702]\n",
            "512 [D loss: 0.202713, acc.: 49.53%] [G loss: -0.098269]\n",
            "513 [D loss: 0.200766, acc.: 49.33%] [G loss: -0.078253]\n",
            "514 [D loss: 0.223877, acc.: 49.20%] [G loss: -0.179296]\n",
            "515 [D loss: 0.204845, acc.: 49.06%] [G loss: -0.099208]\n",
            "516 [D loss: 0.219054, acc.: 49.02%] [G loss: -0.148446]\n",
            "517 [D loss: 0.206588, acc.: 49.13%] [G loss: -0.147281]\n",
            "518 [D loss: 0.222930, acc.: 49.34%] [G loss: -0.032691]\n",
            "519 [D loss: 0.235971, acc.: 49.07%] [G loss: -0.067617]\n",
            "520 [D loss: 0.201611, acc.: 49.00%] [G loss: -0.096576]\n",
            "521 [D loss: 0.208432, acc.: 49.14%] [G loss: -0.072033]\n",
            "522 [D loss: 0.198441, acc.: 49.14%] [G loss: -0.095315]\n",
            "523 [D loss: 0.197967, acc.: 49.05%] [G loss: -0.078242]\n",
            "524 [D loss: 0.211184, acc.: 49.08%] [G loss: -0.179779]\n",
            "525 [D loss: 0.199428, acc.: 49.09%] [G loss: -0.098087]\n",
            "526 [D loss: 0.209399, acc.: 49.02%] [G loss: -0.148256]\n",
            "527 [D loss: 0.206769, acc.: 49.29%] [G loss: -0.129377]\n",
            "528 [D loss: 0.213513, acc.: 49.58%] [G loss: -0.040524]\n",
            "529 [D loss: 0.216223, acc.: 49.10%] [G loss: -0.078255]\n",
            "530 [D loss: 0.205650, acc.: 49.31%] [G loss: -0.076835]\n",
            "531 [D loss: 0.207952, acc.: 49.37%] [G loss: -0.078829]\n",
            "532 [D loss: 0.200061, acc.: 49.43%] [G loss: -0.082056]\n",
            "533 [D loss: 0.202413, acc.: 49.24%] [G loss: -0.072979]\n",
            "534 [D loss: 0.215046, acc.: 49.36%] [G loss: -0.149437]\n",
            "535 [D loss: 0.200459, acc.: 49.43%] [G loss: -0.094553]\n",
            "536 [D loss: 0.208819, acc.: 49.25%] [G loss: -0.153816]\n",
            "537 [D loss: 0.205515, acc.: 49.57%] [G loss: -0.125644]\n",
            "538 [D loss: 0.208441, acc.: 49.84%] [G loss: -0.046773]\n",
            "539 [D loss: 0.210566, acc.: 49.36%] [G loss: -0.088094]\n",
            "540 [D loss: 0.208369, acc.: 49.73%] [G loss: -0.066019]\n",
            "541 [D loss: 0.212080, acc.: 49.57%] [G loss: -0.092215]\n",
            "542 [D loss: 0.207592, acc.: 49.78%] [G loss: -0.067977]\n",
            "543 [D loss: 0.217805, acc.: 49.63%] [G loss: -0.061853]\n",
            "544 [D loss: 0.221934, acc.: 49.53%] [G loss: -0.182598]\n",
            "545 [D loss: 0.199725, acc.: 49.70%] [G loss: -0.093763]\n",
            "546 [D loss: 0.212173, acc.: 49.48%] [G loss: -0.180845]\n",
            "547 [D loss: 0.212190, acc.: 49.86%] [G loss: -0.106267]\n",
            "548 [D loss: 0.207623, acc.: 49.86%] [G loss: -0.057430]\n",
            "549 [D loss: 0.205707, acc.: 49.59%] [G loss: -0.098463]\n",
            "550 [D loss: 0.212282, acc.: 49.92%] [G loss: -0.050229]\n",
            "551 [D loss: 0.235460, acc.: 49.78%] [G loss: -0.087192]\n",
            "552 [D loss: 0.199061, acc.: 49.84%] [G loss: -0.077952]\n",
            "553 [D loss: 0.203401, acc.: 49.77%] [G loss: -0.075693]\n",
            "554 [D loss: 0.231599, acc.: 49.84%] [G loss: -0.095486]\n",
            "555 [D loss: 0.217305, acc.: 49.83%] [G loss: -0.101090]\n",
            "556 [D loss: 0.234269, acc.: 49.76%] [G loss: -0.148221]\n",
            "557 [D loss: 0.210218, acc.: 49.92%] [G loss: -0.095040]\n",
            "558 [D loss: 0.243851, acc.: 50.02%] [G loss: -0.021107]\n",
            "559 [D loss: 0.727958, acc.: 49.77%] [G loss: -0.038068]\n",
            "560 [D loss: 0.540549, acc.: 51.23%] [G loss: -0.103393]\n",
            "561 [D loss: 0.580169, acc.: 50.61%] [G loss: -0.088654]\n",
            "562 [D loss: 0.260571, acc.: 51.01%] [G loss: -0.084868]\n",
            "563 [D loss: 0.248835, acc.: 50.46%] [G loss: -0.089039]\n",
            "564 [D loss: 0.296019, acc.: 50.32%] [G loss: -0.129941]\n",
            "565 [D loss: 0.234645, acc.: 50.20%] [G loss: -0.065010]\n",
            "566 [D loss: 0.364725, acc.: 50.10%] [G loss: -0.211737]\n",
            "567 [D loss: 0.311974, acc.: 50.07%] [G loss: -0.040128]\n",
            "568 [D loss: 0.353370, acc.: 50.33%] [G loss: -0.061573]\n",
            "569 [D loss: 0.453104, acc.: 50.26%] [G loss: -0.046259]\n",
            "570 [D loss: 0.560743, acc.: 49.62%] [G loss: -0.036033]\n",
            "571 [D loss: 0.467848, acc.: 50.21%] [G loss: -0.074906]\n",
            "572 [D loss: 0.252640, acc.: 50.28%] [G loss: -0.087177]\n",
            "573 [D loss: 0.226456, acc.: 50.23%] [G loss: -0.097547]\n",
            "574 [D loss: 0.569057, acc.: 50.22%] [G loss: -0.027791]\n",
            "575 [D loss: 1.119033, acc.: 50.50%] [G loss: -0.145822]\n",
            "576 [D loss: 0.717350, acc.: 50.47%] [G loss: -0.191205]\n",
            "577 [D loss: 0.313743, acc.: 50.68%] [G loss: -0.169528]\n",
            "578 [D loss: 0.486789, acc.: 50.17%] [G loss: -0.074056]\n",
            "579 [D loss: 0.478682, acc.: 50.38%] [G loss: -0.198456]\n",
            "580 [D loss: 0.288971, acc.: 49.77%] [G loss: -0.144970]\n",
            "581 [D loss: 0.441541, acc.: 49.81%] [G loss: -0.128266]\n",
            "582 [D loss: 0.301578, acc.: 50.09%] [G loss: -0.193621]\n",
            "583 [D loss: 0.390768, acc.: 50.43%] [G loss: -0.045127]\n",
            "584 [D loss: 0.577728, acc.: 50.21%] [G loss: -0.231951]\n",
            "585 [D loss: 0.249940, acc.: 49.94%] [G loss: -0.381895]\n",
            "586 [D loss: 0.269019, acc.: 49.49%] [G loss: -0.618076]\n",
            "587 [D loss: 0.332191, acc.: 49.97%] [G loss: -0.175289]\n",
            "588 [D loss: 0.284266, acc.: 49.88%] [G loss: -0.178807]\n",
            "589 [D loss: 0.242077, acc.: 49.80%] [G loss: -0.558784]\n",
            "590 [D loss: 0.220324, acc.: 50.25%] [G loss: -0.935025]\n",
            "591 [D loss: 0.276847, acc.: 50.38%] [G loss: -1.399889]\n",
            "592 [D loss: 0.262933, acc.: 49.72%] [G loss: -0.808466]\n",
            "593 [D loss: 0.260068, acc.: 50.34%] [G loss: -0.116592]\n",
            "594 [D loss: 0.275836, acc.: 49.99%] [G loss: -0.142513]\n",
            "595 [D loss: 0.244343, acc.: 49.58%] [G loss: -0.137186]\n",
            "596 [D loss: 0.215673, acc.: 49.57%] [G loss: -0.366923]\n",
            "597 [D loss: 0.207664, acc.: 49.54%] [G loss: -0.653379]\n",
            "598 [D loss: 0.432665, acc.: 50.30%] [G loss: -0.076759]\n",
            "599 [D loss: 0.246355, acc.: 50.36%] [G loss: -0.059664]\n",
            "600 [D loss: 0.268165, acc.: 50.31%] [G loss: -0.154007]\n",
            "601 [D loss: 0.246631, acc.: 50.13%] [G loss: -0.175425]\n",
            "602 [D loss: 0.248942, acc.: 50.19%] [G loss: -0.129134]\n",
            "603 [D loss: 0.209322, acc.: 50.23%] [G loss: -0.079763]\n",
            "604 [D loss: 0.305547, acc.: 50.12%] [G loss: -0.262601]\n",
            "605 [D loss: 0.265339, acc.: 50.03%] [G loss: -0.139398]\n",
            "606 [D loss: 0.280517, acc.: 49.93%] [G loss: -0.256803]\n",
            "607 [D loss: 0.268167, acc.: 49.82%] [G loss: -0.167752]\n",
            "608 [D loss: 0.311630, acc.: 49.92%] [G loss: -0.045205]\n",
            "609 [D loss: 0.435702, acc.: 49.98%] [G loss: -0.098546]\n",
            "610 [D loss: 0.291949, acc.: 49.82%] [G loss: -0.063190]\n",
            "611 [D loss: 0.481672, acc.: 50.23%] [G loss: -0.283339]\n",
            "612 [D loss: 0.369750, acc.: 50.45%] [G loss: -0.109456]\n",
            "613 [D loss: 0.639775, acc.: 50.17%] [G loss: -0.065352]\n",
            "614 [D loss: 1.221999, acc.: 50.92%] [G loss: -0.395332]\n",
            "615 [D loss: 0.415510, acc.: 51.07%] [G loss: -0.175896]\n",
            "616 [D loss: 0.246861, acc.: 50.45%] [G loss: -0.158058]\n",
            "617 [D loss: 0.213347, acc.: 50.79%] [G loss: -0.170117]\n",
            "618 [D loss: 0.221366, acc.: 50.68%] [G loss: -0.175489]\n",
            "619 [D loss: 0.258113, acc.: 50.89%] [G loss: -0.151427]\n",
            "620 [D loss: 0.578372, acc.: 52.79%] [G loss: -0.066358]\n",
            "621 [D loss: 1.557856, acc.: 50.28%] [G loss: -0.036454]\n",
            "622 [D loss: 1.751263, acc.: 49.82%] [G loss: -1.348290]\n",
            "623 [D loss: 0.263475, acc.: 50.97%] [G loss: -0.082682]\n",
            "624 [D loss: 0.247849, acc.: 50.12%] [G loss: -0.115580]\n",
            "625 [D loss: 0.219095, acc.: 50.22%] [G loss: -0.108603]\n",
            "626 [D loss: 0.214348, acc.: 50.05%] [G loss: -0.136059]\n",
            "627 [D loss: 0.209043, acc.: 50.00%] [G loss: -0.163234]\n",
            "628 [D loss: 0.199223, acc.: 49.90%] [G loss: -0.128672]\n",
            "629 [D loss: 0.200196, acc.: 49.93%] [G loss: -0.132199]\n",
            "630 [D loss: 0.198562, acc.: 50.08%] [G loss: -0.144291]\n",
            "631 [D loss: 0.197337, acc.: 50.01%] [G loss: -0.137260]\n",
            "632 [D loss: 0.197805, acc.: 49.67%] [G loss: -0.153988]\n",
            "633 [D loss: 0.195471, acc.: 49.75%] [G loss: -0.127030]\n",
            "634 [D loss: 0.199275, acc.: 49.70%] [G loss: -0.181355]\n",
            "635 [D loss: 0.195710, acc.: 49.83%] [G loss: -0.153829]\n",
            "636 [D loss: 0.198097, acc.: 49.69%] [G loss: -0.178746]\n",
            "637 [D loss: 0.197687, acc.: 49.85%] [G loss: -0.194907]\n",
            "638 [D loss: 0.197117, acc.: 49.65%] [G loss: -0.145934]\n",
            "639 [D loss: 0.197376, acc.: 49.62%] [G loss: -0.142233]\n",
            "640 [D loss: 0.195139, acc.: 49.84%] [G loss: -0.153574]\n",
            "641 [D loss: 0.194190, acc.: 49.83%] [G loss: -0.142305]\n",
            "642 [D loss: 0.196557, acc.: 49.48%] [G loss: -0.160960]\n",
            "643 [D loss: 0.194122, acc.: 49.51%] [G loss: -0.126709]\n",
            "644 [D loss: 0.197534, acc.: 49.46%] [G loss: -0.190431]\n",
            "645 [D loss: 0.194178, acc.: 49.55%] [G loss: -0.156083]\n",
            "646 [D loss: 0.196409, acc.: 49.41%] [G loss: -0.182842]\n",
            "647 [D loss: 0.196137, acc.: 49.48%] [G loss: -0.192207]\n",
            "648 [D loss: 0.196951, acc.: 49.35%] [G loss: -0.143469]\n",
            "649 [D loss: 0.196568, acc.: 49.30%] [G loss: -0.138039]\n",
            "650 [D loss: 0.194245, acc.: 49.46%] [G loss: -0.152622]\n",
            "651 [D loss: 0.193459, acc.: 49.43%] [G loss: -0.139630]\n",
            "652 [D loss: 0.196081, acc.: 49.13%] [G loss: -0.162355]\n",
            "653 [D loss: 0.193504, acc.: 49.15%] [G loss: -0.120842]\n",
            "654 [D loss: 0.196730, acc.: 49.13%] [G loss: -0.194420]\n",
            "655 [D loss: 0.193712, acc.: 49.22%] [G loss: -0.153227]\n",
            "656 [D loss: 0.195857, acc.: 49.06%] [G loss: -0.180080]\n",
            "657 [D loss: 0.195820, acc.: 49.29%] [G loss: -0.181066]\n",
            "658 [D loss: 0.199226, acc.: 49.12%] [G loss: -0.134294]\n",
            "659 [D loss: 0.196455, acc.: 49.05%] [G loss: -0.127497]\n",
            "660 [D loss: 0.193985, acc.: 49.27%] [G loss: -0.143277]\n",
            "661 [D loss: 0.193855, acc.: 49.21%] [G loss: -0.130440]\n",
            "662 [D loss: 0.196683, acc.: 48.98%] [G loss: -0.157641]\n",
            "663 [D loss: 0.193462, acc.: 49.15%] [G loss: -0.107505]\n",
            "664 [D loss: 0.196652, acc.: 49.11%] [G loss: -0.192203]\n",
            "665 [D loss: 0.194407, acc.: 49.29%] [G loss: -0.143327]\n",
            "666 [D loss: 0.197005, acc.: 49.08%] [G loss: -0.171134]\n",
            "667 [D loss: 0.196659, acc.: 49.36%] [G loss: -0.157038]\n",
            "668 [D loss: 0.201729, acc.: 49.25%] [G loss: -0.117393]\n",
            "669 [D loss: 0.196500, acc.: 49.17%] [G loss: -0.111669]\n",
            "670 [D loss: 0.194673, acc.: 49.30%] [G loss: -0.122957]\n",
            "671 [D loss: 0.194954, acc.: 49.28%] [G loss: -0.117353]\n",
            "672 [D loss: 0.198054, acc.: 49.10%] [G loss: -0.148495]\n",
            "673 [D loss: 0.193499, acc.: 49.16%] [G loss: -0.091053]\n",
            "674 [D loss: 0.196605, acc.: 49.25%] [G loss: -0.180026]\n",
            "675 [D loss: 0.193997, acc.: 49.37%] [G loss: -0.133941]\n",
            "676 [D loss: 0.196057, acc.: 49.15%] [G loss: -0.165623]\n",
            "677 [D loss: 0.196646, acc.: 49.40%] [G loss: -0.142125]\n",
            "678 [D loss: 0.198155, acc.: 49.25%] [G loss: -0.113552]\n",
            "679 [D loss: 0.195400, acc.: 49.30%] [G loss: -0.109555]\n",
            "680 [D loss: 0.194414, acc.: 49.39%] [G loss: -0.117838]\n",
            "681 [D loss: 0.193983, acc.: 49.34%] [G loss: -0.125642]\n",
            "682 [D loss: 0.196063, acc.: 49.17%] [G loss: -0.167266]\n",
            "683 [D loss: 0.193030, acc.: 49.18%] [G loss: -0.094765]\n",
            "684 [D loss: 0.195990, acc.: 49.26%] [G loss: -0.202295]\n",
            "685 [D loss: 0.193695, acc.: 49.44%] [G loss: -0.148150]\n",
            "686 [D loss: 0.195530, acc.: 49.23%] [G loss: -0.185783]\n",
            "687 [D loss: 0.195671, acc.: 49.33%] [G loss: -0.147671]\n",
            "688 [D loss: 0.199673, acc.: 49.20%] [G loss: -0.122859]\n",
            "689 [D loss: 0.195561, acc.: 49.26%] [G loss: -0.117523]\n",
            "690 [D loss: 0.195058, acc.: 49.33%] [G loss: -0.117460]\n",
            "691 [D loss: 0.194511, acc.: 49.30%] [G loss: -0.131472]\n",
            "692 [D loss: 0.196414, acc.: 49.16%] [G loss: -0.178662]\n",
            "693 [D loss: 0.193197, acc.: 49.21%] [G loss: -0.092996]\n",
            "694 [D loss: 0.196103, acc.: 49.29%] [G loss: -0.207707]\n",
            "695 [D loss: 0.193851, acc.: 49.42%] [G loss: -0.148698]\n",
            "696 [D loss: 0.195167, acc.: 49.23%] [G loss: -0.188334]\n",
            "697 [D loss: 0.195861, acc.: 49.35%] [G loss: -0.138323]\n",
            "698 [D loss: 0.208937, acc.: 49.30%] [G loss: -0.113480]\n",
            "699 [D loss: 0.196285, acc.: 49.25%] [G loss: -0.109423]\n",
            "700 [D loss: 0.196096, acc.: 49.37%] [G loss: -0.100529]\n",
            "701 [D loss: 0.195753, acc.: 49.51%] [G loss: -0.115828]\n",
            "702 [D loss: 0.196572, acc.: 49.33%] [G loss: -0.154377]\n",
            "703 [D loss: 0.193767, acc.: 49.35%] [G loss: -0.079874]\n",
            "704 [D loss: 0.197155, acc.: 49.50%] [G loss: -0.181671]\n",
            "705 [D loss: 0.194284, acc.: 49.48%] [G loss: -0.128924]\n",
            "706 [D loss: 0.195297, acc.: 49.41%] [G loss: -0.168627]\n",
            "707 [D loss: 0.196121, acc.: 49.44%] [G loss: -0.118426]\n",
            "708 [D loss: 0.218298, acc.: 49.47%] [G loss: -0.093028]\n",
            "709 [D loss: 0.196199, acc.: 49.43%] [G loss: -0.091445]\n",
            "710 [D loss: 0.195991, acc.: 49.47%] [G loss: -0.083911]\n",
            "711 [D loss: 0.195665, acc.: 49.68%] [G loss: -0.103392]\n",
            "712 [D loss: 0.195595, acc.: 49.53%] [G loss: -0.144900]\n",
            "713 [D loss: 0.194341, acc.: 49.44%] [G loss: -0.074963]\n",
            "714 [D loss: 0.198215, acc.: 49.51%] [G loss: -0.172838]\n",
            "715 [D loss: 0.197854, acc.: 49.50%] [G loss: -0.121156]\n",
            "716 [D loss: 0.196099, acc.: 49.43%] [G loss: -0.158472]\n",
            "717 [D loss: 0.198323, acc.: 49.46%] [G loss: -0.110935]\n",
            "718 [D loss: 0.249141, acc.: 49.94%] [G loss: -0.074227]\n",
            "719 [D loss: 0.202985, acc.: 49.94%] [G loss: -0.071614]\n",
            "720 [D loss: 0.202105, acc.: 49.57%] [G loss: -0.071919]\n",
            "721 [D loss: 0.200705, acc.: 49.78%] [G loss: -0.101142]\n",
            "722 [D loss: 0.196680, acc.: 49.74%] [G loss: -0.156103]\n",
            "723 [D loss: 0.196777, acc.: 49.51%] [G loss: -0.079357]\n",
            "724 [D loss: 0.200138, acc.: 49.42%] [G loss: -0.196003]\n",
            "725 [D loss: 0.222627, acc.: 49.76%] [G loss: -0.124890]\n",
            "726 [D loss: 0.204335, acc.: 49.89%] [G loss: -0.182471]\n",
            "727 [D loss: 0.219843, acc.: 49.79%] [G loss: -0.146501]\n",
            "728 [D loss: 0.207539, acc.: 50.56%] [G loss: -0.111900]\n",
            "729 [D loss: 0.213619, acc.: 50.97%] [G loss: -0.190270]\n",
            "730 [D loss: 0.208114, acc.: 50.87%] [G loss: -0.541388]\n",
            "731 [D loss: 0.283863, acc.: 50.74%] [G loss: -0.088245]\n",
            "732 [D loss: 0.268555, acc.: 50.45%] [G loss: -0.090025]\n",
            "733 [D loss: 0.251561, acc.: 49.99%] [G loss: -0.096567]\n",
            "734 [D loss: 0.231849, acc.: 49.60%] [G loss: -0.148092]\n",
            "735 [D loss: 0.219177, acc.: 49.72%] [G loss: -0.195926]\n",
            "736 [D loss: 0.203266, acc.: 49.44%] [G loss: -0.260303]\n",
            "737 [D loss: 0.202520, acc.: 49.84%] [G loss: -0.294329]\n",
            "738 [D loss: 0.259392, acc.: 50.69%] [G loss: -0.267387]\n",
            "739 [D loss: 0.200684, acc.: 51.15%] [G loss: -0.387072]\n",
            "740 [D loss: 0.205439, acc.: 50.80%] [G loss: -0.802414]\n",
            "741 [D loss: 0.251522, acc.: 53.18%] [G loss: -1.600495]\n",
            "742 [D loss: 0.198683, acc.: 49.83%] [G loss: -0.756963]\n",
            "743 [D loss: 0.309917, acc.: 50.17%] [G loss: -0.133724]\n",
            "744 [D loss: 0.221673, acc.: 50.54%] [G loss: -0.144152]\n",
            "745 [D loss: 0.219227, acc.: 50.43%] [G loss: -0.162442]\n",
            "746 [D loss: 0.222079, acc.: 50.28%] [G loss: -0.295500]\n",
            "747 [D loss: 0.296209, acc.: 50.30%] [G loss: -0.828535]\n",
            "748 [D loss: 0.271708, acc.: 52.70%] [G loss: -0.099377]\n",
            "749 [D loss: 0.286090, acc.: 52.50%] [G loss: -0.080931]\n",
            "750 [D loss: 0.251307, acc.: 51.43%] [G loss: -0.107673]\n",
            "751 [D loss: 0.229039, acc.: 50.10%] [G loss: -0.100560]\n",
            "752 [D loss: 0.207417, acc.: 49.74%] [G loss: -0.164868]\n",
            "753 [D loss: 0.198688, acc.: 49.67%] [G loss: -0.124808]\n",
            "754 [D loss: 0.201187, acc.: 49.59%] [G loss: -0.184492]\n",
            "755 [D loss: 0.196078, acc.: 49.65%] [G loss: -0.175313]\n",
            "756 [D loss: 0.197509, acc.: 49.59%] [G loss: -0.203301]\n",
            "757 [D loss: 0.198143, acc.: 49.66%] [G loss: -0.192603]\n",
            "758 [D loss: 0.198603, acc.: 49.85%] [G loss: -0.185565]\n",
            "759 [D loss: 0.195984, acc.: 49.86%] [G loss: -0.173274]\n",
            "760 [D loss: 0.195558, acc.: 49.88%] [G loss: -0.189004]\n",
            "761 [D loss: 0.194624, acc.: 49.91%] [G loss: -0.157483]\n",
            "762 [D loss: 0.196115, acc.: 49.76%] [G loss: -0.218805]\n",
            "763 [D loss: 0.193734, acc.: 49.79%] [G loss: -0.141679]\n",
            "764 [D loss: 0.196956, acc.: 49.79%] [G loss: -0.201481]\n",
            "765 [D loss: 0.193472, acc.: 49.98%] [G loss: -0.173500]\n",
            "766 [D loss: 0.195565, acc.: 49.90%] [G loss: -0.202201]\n",
            "767 [D loss: 0.195598, acc.: 49.92%] [G loss: -0.167644]\n",
            "768 [D loss: 0.200503, acc.: 49.98%] [G loss: -0.161247]\n",
            "769 [D loss: 0.195998, acc.: 49.98%] [G loss: -0.144700]\n",
            "770 [D loss: 0.194335, acc.: 49.98%] [G loss: -0.149939]\n",
            "771 [D loss: 0.196768, acc.: 50.02%] [G loss: -0.120307]\n",
            "772 [D loss: 0.197595, acc.: 49.92%] [G loss: -0.189306]\n",
            "773 [D loss: 0.196259, acc.: 50.00%] [G loss: -0.101603]\n",
            "774 [D loss: 0.199440, acc.: 50.03%] [G loss: -0.169892]\n",
            "775 [D loss: 0.211270, acc.: 50.06%] [G loss: -0.125868]\n",
            "776 [D loss: 0.199857, acc.: 50.19%] [G loss: -0.154762]\n",
            "777 [D loss: 0.202331, acc.: 50.14%] [G loss: -0.115270]\n",
            "778 [D loss: 0.214033, acc.: 50.23%] [G loss: -0.109647]\n",
            "779 [D loss: 0.200086, acc.: 50.21%] [G loss: -0.099843]\n",
            "780 [D loss: 0.200258, acc.: 50.19%] [G loss: -0.101252]\n",
            "781 [D loss: 0.211124, acc.: 50.23%] [G loss: -0.077103]\n",
            "782 [D loss: 0.202108, acc.: 50.19%] [G loss: -0.142623]\n",
            "783 [D loss: 0.210114, acc.: 50.26%] [G loss: -0.065319]\n",
            "784 [D loss: 0.221980, acc.: 50.23%] [G loss: -0.134615]\n",
            "785 [D loss: 0.257577, acc.: 50.30%] [G loss: -0.074935]\n",
            "786 [D loss: 0.262888, acc.: 50.33%] [G loss: -0.127199]\n",
            "787 [D loss: 0.211495, acc.: 50.27%] [G loss: -0.107983]\n",
            "788 [D loss: 0.243228, acc.: 50.40%] [G loss: -0.094555]\n",
            "789 [D loss: 0.208184, acc.: 50.34%] [G loss: -0.087676]\n",
            "790 [D loss: 0.205994, acc.: 50.28%] [G loss: -0.090760]\n",
            "791 [D loss: 0.225796, acc.: 50.30%] [G loss: -0.066723]\n",
            "792 [D loss: 0.207896, acc.: 50.25%] [G loss: -0.138109]\n",
            "793 [D loss: 0.450686, acc.: 50.62%] [G loss: -0.017257]\n",
            "794 [D loss: 1.230701, acc.: 50.06%] [G loss: -0.272463]\n",
            "795 [D loss: 0.511853, acc.: 50.12%] [G loss: -0.110394]\n",
            "796 [D loss: 0.759018, acc.: 50.45%] [G loss: -0.614913]\n",
            "797 [D loss: 0.347253, acc.: 50.43%] [G loss: -0.289701]\n",
            "798 [D loss: 0.522807, acc.: 50.44%] [G loss: -0.247007]\n",
            "799 [D loss: 0.333903, acc.: 50.20%] [G loss: -0.357331]\n",
            "800 [D loss: 0.231225, acc.: 50.03%] [G loss: -0.682549]\n",
            "801 [D loss: 0.528052, acc.: 50.14%] [G loss: -0.248554]\n",
            "802 [D loss: 0.490087, acc.: 49.86%] [G loss: -0.734449]\n",
            "803 [D loss: 0.291018, acc.: 49.92%] [G loss: -0.198064]\n",
            "804 [D loss: 0.301263, acc.: 50.02%] [G loss: -0.599498]\n",
            "805 [D loss: 0.285453, acc.: 49.74%] [G loss: -0.316015]\n",
            "806 [D loss: 0.521286, acc.: 50.08%] [G loss: -0.169145]\n",
            "807 [D loss: 0.609314, acc.: 48.66%] [G loss: -0.371221]\n",
            "808 [D loss: 0.963398, acc.: 48.90%] [G loss: -0.315390]\n",
            "809 [D loss: 0.815639, acc.: 51.54%] [G loss: -0.098247]\n",
            "810 [D loss: 0.509619, acc.: 50.42%] [G loss: -0.145368]\n",
            "811 [D loss: 0.292984, acc.: 49.67%] [G loss: -0.136020]\n",
            "812 [D loss: 0.442003, acc.: 51.43%] [G loss: -0.191506]\n",
            "813 [D loss: 0.493294, acc.: 50.21%] [G loss: -0.212243]\n",
            "814 [D loss: 0.358727, acc.: 50.16%] [G loss: -0.206997]\n",
            "815 [D loss: 0.371782, acc.: 51.75%] [G loss: -0.159122]\n",
            "816 [D loss: 0.604308, acc.: 49.08%] [G loss: -0.750474]\n",
            "817 [D loss: 0.715399, acc.: 50.64%] [G loss: -0.239327]\n",
            "818 [D loss: 0.469514, acc.: 50.70%] [G loss: -0.597374]\n",
            "819 [D loss: 0.324168, acc.: 49.04%] [G loss: -0.103729]\n",
            "820 [D loss: 0.225573, acc.: 48.55%] [G loss: -0.104821]\n",
            "821 [D loss: 0.207776, acc.: 47.77%] [G loss: -0.090742]\n",
            "822 [D loss: 0.200501, acc.: 47.69%] [G loss: -0.195934]\n",
            "823 [D loss: 0.196431, acc.: 47.97%] [G loss: -0.127285]\n",
            "824 [D loss: 0.203169, acc.: 48.25%] [G loss: -0.785795]\n",
            "825 [D loss: 0.775828, acc.: 49.45%] [G loss: -0.190024]\n",
            "826 [D loss: 0.340581, acc.: 50.11%] [G loss: -0.141011]\n",
            "827 [D loss: 0.325106, acc.: 49.52%] [G loss: -0.900933]\n",
            "828 [D loss: 0.226919, acc.: 49.36%] [G loss: -0.098788]\n",
            "829 [D loss: 0.224763, acc.: 48.94%] [G loss: -0.118376]\n",
            "830 [D loss: 0.216946, acc.: 48.30%] [G loss: -0.142650]\n",
            "831 [D loss: 0.204393, acc.: 47.65%] [G loss: -0.142300]\n",
            "832 [D loss: 0.201379, acc.: 47.80%] [G loss: -0.348146]\n",
            "833 [D loss: 0.207883, acc.: 48.69%] [G loss: -0.195921]\n",
            "834 [D loss: 0.227787, acc.: 49.46%] [G loss: -1.042974]\n",
            "835 [D loss: 0.472993, acc.: 50.41%] [G loss: -0.197298]\n",
            "836 [D loss: 0.322075, acc.: 50.40%] [G loss: -0.283585]\n",
            "837 [D loss: 0.332071, acc.: 50.34%] [G loss: -0.465816]\n",
            "838 [D loss: 0.269615, acc.: 50.47%] [G loss: -0.273586]\n",
            "839 [D loss: 0.294951, acc.: 49.76%] [G loss: -1.052819]\n",
            "840 [D loss: 0.253759, acc.: 49.40%] [G loss: -0.255281]\n",
            "841 [D loss: 0.239859, acc.: 48.91%] [G loss: -0.382613]\n",
            "842 [D loss: 0.399954, acc.: 50.20%] [G loss: -0.458069]\n",
            "843 [D loss: 0.249896, acc.: 51.02%] [G loss: -0.129402]\n",
            "844 [D loss: 0.444758, acc.: 50.25%] [G loss: -0.129608]\n",
            "845 [D loss: 0.220371, acc.: 50.46%] [G loss: -0.230488]\n",
            "846 [D loss: 0.445266, acc.: 50.65%] [G loss: -0.157243]\n",
            "847 [D loss: 0.814272, acc.: 51.00%] [G loss: -0.238106]\n",
            "848 [D loss: 1.607302, acc.: 51.15%] [G loss: -0.123966]\n",
            "849 [D loss: 0.367465, acc.: 48.85%] [G loss: -0.109922]\n",
            "850 [D loss: 0.235048, acc.: 48.35%] [G loss: -0.300264]\n",
            "851 [D loss: 0.614906, acc.: 50.29%] [G loss: -0.155464]\n",
            "852 [D loss: 1.137120, acc.: 51.35%] [G loss: -0.156075]\n",
            "853 [D loss: 0.712986, acc.: 49.90%] [G loss: -0.503710]\n",
            "854 [D loss: 0.437948, acc.: 49.01%] [G loss: -0.890294]\n",
            "855 [D loss: 0.289747, acc.: 49.65%] [G loss: -0.203245]\n",
            "856 [D loss: 0.266548, acc.: 49.71%] [G loss: -0.187985]\n",
            "857 [D loss: 0.256918, acc.: 49.70%] [G loss: -0.116129]\n",
            "858 [D loss: 0.287048, acc.: 49.72%] [G loss: -0.076778]\n",
            "859 [D loss: 0.243147, acc.: 49.61%] [G loss: -0.066694]\n",
            "860 [D loss: 0.223556, acc.: 49.60%] [G loss: -0.119252]\n",
            "861 [D loss: 0.286218, acc.: 49.56%] [G loss: -0.119579]\n",
            "862 [D loss: 0.261665, acc.: 49.68%] [G loss: -0.117567]\n",
            "863 [D loss: 0.225842, acc.: 49.68%] [G loss: -0.081394]\n",
            "864 [D loss: 0.440629, acc.: 49.50%] [G loss: -0.270416]\n",
            "865 [D loss: 0.304889, acc.: 49.61%] [G loss: -0.167242]\n",
            "866 [D loss: 0.415582, acc.: 49.56%] [G loss: -0.317483]\n",
            "867 [D loss: 0.406549, acc.: 49.84%] [G loss: -0.093926]\n",
            "868 [D loss: 0.414200, acc.: 49.91%] [G loss: -0.069255]\n",
            "869 [D loss: 0.449121, acc.: 49.56%] [G loss: -0.110156]\n",
            "870 [D loss: 0.312514, acc.: 49.79%] [G loss: -0.096842]\n",
            "871 [D loss: 0.602719, acc.: 49.87%] [G loss: -0.079199]\n",
            "872 [D loss: 0.443070, acc.: 49.63%] [G loss: -0.089977]\n",
            "873 [D loss: 0.550939, acc.: 49.74%] [G loss: -0.045668]\n",
            "874 [D loss: 1.342150, acc.: 50.03%] [G loss: -0.320144]\n",
            "875 [D loss: 0.544456, acc.: 49.99%] [G loss: -0.131486]\n",
            "876 [D loss: 0.874087, acc.: 49.93%] [G loss: -0.458515]\n",
            "877 [D loss: 0.900686, acc.: 49.86%] [G loss: -0.132412]\n",
            "878 [D loss: 0.603968, acc.: 49.90%] [G loss: -0.229210]\n",
            "879 [D loss: 0.610246, acc.: 49.48%] [G loss: -0.985661]\n",
            "880 [D loss: 0.273102, acc.: 51.27%] [G loss: -0.036090]\n",
            "881 [D loss: 0.294601, acc.: 51.51%] [G loss: -0.015624]\n",
            "882 [D loss: 0.225896, acc.: 51.73%] [G loss: -0.014807]\n",
            "883 [D loss: 0.208471, acc.: 51.80%] [G loss: -0.041712]\n",
            "884 [D loss: 0.250702, acc.: 51.27%] [G loss: -0.029246]\n",
            "885 [D loss: 0.215969, acc.: 50.17%] [G loss: -0.050687]\n",
            "886 [D loss: 0.211989, acc.: 49.84%] [G loss: -0.108313]\n",
            "887 [D loss: 0.206412, acc.: 49.86%] [G loss: -0.071570]\n",
            "888 [D loss: 0.201507, acc.: 49.72%] [G loss: -0.037783]\n",
            "889 [D loss: 0.203602, acc.: 49.91%] [G loss: -0.031064]\n",
            "890 [D loss: 0.201558, acc.: 49.87%] [G loss: -0.110433]\n",
            "891 [D loss: 0.202591, acc.: 49.48%] [G loss: -0.124613]\n",
            "892 [D loss: 0.196968, acc.: 49.19%] [G loss: -0.179308]\n",
            "893 [D loss: 0.197552, acc.: 48.84%] [G loss: -0.140169]\n",
            "894 [D loss: 0.207618, acc.: 48.70%] [G loss: -0.625791]\n",
            "895 [D loss: 0.204236, acc.: 48.89%] [G loss: -1.026601]\n",
            "896 [D loss: 0.221396, acc.: 49.04%] [G loss: -1.874229]\n",
            "897 [D loss: 0.246830, acc.: 50.17%] [G loss: -0.249026]\n",
            "898 [D loss: 0.242801, acc.: 50.61%] [G loss: -0.164434]\n",
            "899 [D loss: 0.230203, acc.: 50.41%] [G loss: -0.131069]\n",
            "900 [D loss: 0.222129, acc.: 50.01%] [G loss: -0.250132]\n",
            "901 [D loss: 0.203150, acc.: 49.11%] [G loss: -0.298120]\n",
            "902 [D loss: 0.198986, acc.: 48.76%] [G loss: -0.447238]\n",
            "903 [D loss: 0.504926, acc.: 49.38%] [G loss: -0.525725]\n",
            "904 [D loss: 0.241569, acc.: 49.90%] [G loss: -1.404112]\n",
            "905 [D loss: 0.258428, acc.: 49.77%] [G loss: -0.439991]\n",
            "906 [D loss: 0.231668, acc.: 49.77%] [G loss: -0.665674]\n",
            "907 [D loss: 0.217937, acc.: 49.50%] [G loss: -0.521390]\n",
            "908 [D loss: 0.289493, acc.: 49.39%] [G loss: -0.252653]\n",
            "909 [D loss: 0.210482, acc.: 49.10%] [G loss: -0.134172]\n",
            "910 [D loss: 0.206240, acc.: 49.05%] [G loss: -0.202985]\n",
            "911 [D loss: 0.228627, acc.: 49.12%] [G loss: -0.191353]\n",
            "912 [D loss: 0.214267, acc.: 49.15%] [G loss: -0.170349]\n",
            "913 [D loss: 0.209721, acc.: 49.24%] [G loss: -0.196035]\n",
            "914 [D loss: 0.271846, acc.: 49.41%] [G loss: -0.263171]\n",
            "915 [D loss: 0.232302, acc.: 49.51%] [G loss: -0.348894]\n",
            "916 [D loss: 0.280192, acc.: 49.61%] [G loss: -0.607819]\n",
            "917 [D loss: 0.621084, acc.: 49.83%] [G loss: -0.090431]\n",
            "918 [D loss: 0.860338, acc.: 50.02%] [G loss: -0.118122]\n",
            "919 [D loss: 0.483504, acc.: 50.15%] [G loss: -0.174625]\n",
            "920 [D loss: 0.288413, acc.: 50.12%] [G loss: -0.212662]\n",
            "921 [D loss: 0.315170, acc.: 50.08%] [G loss: -0.356554]\n",
            "922 [D loss: 1.221940, acc.: 50.16%] [G loss: -0.131954]\n",
            "923 [D loss: 0.571311, acc.: 49.85%] [G loss: -0.079978]\n",
            "924 [D loss: 0.885288, acc.: 49.76%] [G loss: -0.706989]\n",
            "925 [D loss: 0.608309, acc.: 49.73%] [G loss: -0.271809]\n",
            "926 [D loss: 0.845893, acc.: 49.55%] [G loss: -0.460955]\n",
            "927 [D loss: 1.057030, acc.: 49.57%] [G loss: -0.332668]\n",
            "928 [D loss: 1.373105, acc.: 49.77%] [G loss: -0.263821]\n",
            "929 [D loss: 1.093585, acc.: 50.26%] [G loss: -0.243100]\n",
            "930 [D loss: 0.756324, acc.: 50.55%] [G loss: -0.189798]\n",
            "931 [D loss: 0.543044, acc.: 50.33%] [G loss: -0.107523]\n",
            "932 [D loss: 0.476714, acc.: 50.19%] [G loss: -0.242896]\n",
            "933 [D loss: 0.950091, acc.: 50.14%] [G loss: -0.148728]\n",
            "934 [D loss: 0.895446, acc.: 50.24%] [G loss: -0.321284]\n",
            "935 [D loss: 0.535784, acc.: 50.10%] [G loss: -0.352610]\n",
            "936 [D loss: 0.709588, acc.: 50.04%] [G loss: -0.496899]\n",
            "937 [D loss: 0.519469, acc.: 49.86%] [G loss: -0.246850]\n",
            "938 [D loss: 0.822023, acc.: 49.81%] [G loss: -0.266366]\n",
            "939 [D loss: 0.645077, acc.: 49.92%] [G loss: -0.109879]\n",
            "940 [D loss: 0.322259, acc.: 49.38%] [G loss: -0.169948]\n",
            "941 [D loss: 0.254145, acc.: 49.61%] [G loss: -0.187283]\n",
            "942 [D loss: 0.221430, acc.: 49.52%] [G loss: -0.215729]\n",
            "943 [D loss: 0.212372, acc.: 49.54%] [G loss: -0.200340]\n",
            "944 [D loss: 0.229439, acc.: 49.54%] [G loss: -0.268627]\n",
            "945 [D loss: 0.217921, acc.: 49.87%] [G loss: -0.264153]\n",
            "946 [D loss: 0.241744, acc.: 49.88%] [G loss: -0.330871]\n",
            "947 [D loss: 0.822358, acc.: 49.99%] [G loss: -0.493172]\n",
            "948 [D loss: 0.526340, acc.: 49.92%] [G loss: -0.082231]\n",
            "949 [D loss: 0.416257, acc.: 50.10%] [G loss: -0.074426]\n",
            "950 [D loss: 0.232355, acc.: 49.80%] [G loss: -0.156913]\n",
            "951 [D loss: 0.226265, acc.: 49.73%] [G loss: -0.146407]\n",
            "952 [D loss: 0.210272, acc.: 49.89%] [G loss: -0.309775]\n",
            "953 [D loss: 0.203637, acc.: 50.08%] [G loss: -0.268290]\n",
            "954 [D loss: 0.218624, acc.: 50.14%] [G loss: -1.326386]\n",
            "955 [D loss: 0.253737, acc.: 50.13%] [G loss: -0.808424]\n",
            "956 [D loss: 0.294837, acc.: 49.84%] [G loss: -0.830517]\n",
            "957 [D loss: 0.335294, acc.: 49.70%] [G loss: -0.232814]\n",
            "958 [D loss: 0.386630, acc.: 49.78%] [G loss: -0.113684]\n",
            "959 [D loss: 0.324444, acc.: 49.88%] [G loss: -0.098262]\n",
            "960 [D loss: 0.303286, acc.: 49.97%] [G loss: -0.226154]\n",
            "961 [D loss: 0.364511, acc.: 50.00%] [G loss: -0.193142]\n",
            "962 [D loss: 0.295848, acc.: 50.00%] [G loss: -0.212415]\n",
            "963 [D loss: 0.283991, acc.: 50.04%] [G loss: -0.132322]\n",
            "964 [D loss: 0.549552, acc.: 50.02%] [G loss: -0.314829]\n",
            "965 [D loss: 0.348869, acc.: 50.06%] [G loss: -0.266310]\n",
            "966 [D loss: 0.407715, acc.: 50.02%] [G loss: -0.601969]\n",
            "967 [D loss: 0.410432, acc.: 49.90%] [G loss: -0.135764]\n",
            "968 [D loss: 0.310521, acc.: 49.84%] [G loss: -0.059385]\n",
            "969 [D loss: 0.319525, acc.: 49.91%] [G loss: -0.064772]\n",
            "970 [D loss: 0.283449, acc.: 50.03%] [G loss: -0.179496]\n",
            "971 [D loss: 0.447116, acc.: 50.02%] [G loss: -0.132017]\n",
            "972 [D loss: 0.319815, acc.: 50.00%] [G loss: -0.172558]\n",
            "973 [D loss: 0.306575, acc.: 50.05%] [G loss: -0.160028]\n",
            "974 [D loss: 0.518944, acc.: 50.20%] [G loss: -0.582244]\n",
            "975 [D loss: 0.564844, acc.: 49.48%] [G loss: -0.142091]\n",
            "976 [D loss: 0.557879, acc.: 49.21%] [G loss: -0.316879]\n",
            "977 [D loss: 0.332297, acc.: 49.59%] [G loss: -0.886593]\n",
            "978 [D loss: 0.221880, acc.: 49.95%] [G loss: -0.361841]\n",
            "979 [D loss: 0.218166, acc.: 49.38%] [G loss: -0.306339]\n",
            "980 [D loss: 0.725843, acc.: 49.49%] [G loss: -0.360855]\n",
            "981 [D loss: 0.423886, acc.: 50.18%] [G loss: -0.562006]\n",
            "982 [D loss: 0.330499, acc.: 49.58%] [G loss: -0.433851]\n",
            "983 [D loss: 0.226378, acc.: 49.77%] [G loss: -0.241025]\n",
            "984 [D loss: 0.283119, acc.: 49.80%] [G loss: -1.249511]\n",
            "985 [D loss: 0.600900, acc.: 49.70%] [G loss: -0.400403]\n",
            "986 [D loss: 0.841502, acc.: 50.12%] [G loss: -0.441652]\n",
            "987 [D loss: 0.771855, acc.: 49.94%] [G loss: -0.231375]\n",
            "988 [D loss: 0.865587, acc.: 50.26%] [G loss: -0.368688]\n",
            "989 [D loss: 0.490907, acc.: 49.84%] [G loss: -0.078378]\n",
            "990 [D loss: 0.274406, acc.: 49.62%] [G loss: -0.214987]\n",
            "991 [D loss: 0.233254, acc.: 49.70%] [G loss: -0.154885]\n",
            "992 [D loss: 0.207450, acc.: 49.53%] [G loss: -0.754122]\n",
            "993 [D loss: 0.290320, acc.: 49.86%] [G loss: -0.300264]\n",
            "994 [D loss: 0.318461, acc.: 49.91%] [G loss: -0.664616]\n",
            "995 [D loss: 0.284369, acc.: 49.83%] [G loss: -0.281998]\n",
            "996 [D loss: 0.397583, acc.: 49.94%] [G loss: -0.688173]\n",
            "997 [D loss: 0.542543, acc.: 49.84%] [G loss: -0.144489]\n",
            "998 [D loss: 0.530162, acc.: 49.98%] [G loss: -0.110697]\n",
            "999 [D loss: 0.487749, acc.: 50.11%] [G loss: -0.085651]\n",
            "1000 [D loss: 0.445302, acc.: 50.08%] [G loss: -0.115503]\n",
            "1001 [D loss: 0.496218, acc.: 50.06%] [G loss: -0.128364]\n",
            "1002 [D loss: 0.356634, acc.: 50.28%] [G loss: -0.197467]\n",
            "1003 [D loss: 0.405457, acc.: 50.05%] [G loss: -0.104488]\n",
            "1004 [D loss: 0.779552, acc.: 50.04%] [G loss: -0.307565]\n",
            "1005 [D loss: 0.726993, acc.: 49.92%] [G loss: -0.114624]\n",
            "1006 [D loss: 0.831337, acc.: 49.78%] [G loss: -0.335860]\n",
            "1007 [D loss: 0.715690, acc.: 49.82%] [G loss: -0.236822]\n",
            "1008 [D loss: 0.639614, acc.: 49.83%] [G loss: -0.156730]\n",
            "1009 [D loss: 0.560890, acc.: 50.10%] [G loss: -0.151024]\n",
            "1010 [D loss: 0.548344, acc.: 50.32%] [G loss: -0.179366]\n",
            "1011 [D loss: 0.556298, acc.: 50.36%] [G loss: -0.347292]\n",
            "1012 [D loss: 0.589830, acc.: 50.69%] [G loss: -0.234854]\n",
            "1013 [D loss: 0.950069, acc.: 50.31%] [G loss: -0.113401]\n",
            "1014 [D loss: 0.978595, acc.: 50.39%] [G loss: -0.198669]\n",
            "1015 [D loss: 0.868835, acc.: 50.36%] [G loss: -0.406015]\n",
            "1016 [D loss: 0.677693, acc.: 50.55%] [G loss: -0.391214]\n",
            "1017 [D loss: 0.447197, acc.: 50.19%] [G loss: -0.140822]\n",
            "1018 [D loss: 0.277890, acc.: 50.15%] [G loss: -0.142395]\n",
            "1019 [D loss: 0.254449, acc.: 50.09%] [G loss: -0.105492]\n",
            "1020 [D loss: 0.230143, acc.: 50.06%] [G loss: -0.367609]\n",
            "1021 [D loss: 0.271541, acc.: 50.00%] [G loss: -0.163129]\n",
            "1022 [D loss: 0.243495, acc.: 49.95%] [G loss: -0.450769]\n",
            "1023 [D loss: 0.230347, acc.: 49.95%] [G loss: -0.368582]\n",
            "1024 [D loss: 0.322642, acc.: 50.00%] [G loss: -0.745338]\n",
            "1025 [D loss: 0.414688, acc.: 49.80%] [G loss: -0.313075]\n",
            "1026 [D loss: 0.531795, acc.: 49.94%] [G loss: -0.635094]\n",
            "1027 [D loss: 0.607889, acc.: 50.01%] [G loss: -0.216926]\n",
            "1028 [D loss: 0.742880, acc.: 50.02%] [G loss: -0.115199]\n",
            "1029 [D loss: 0.944511, acc.: 50.02%] [G loss: -0.183248]\n",
            "1030 [D loss: 0.536758, acc.: 50.26%] [G loss: -0.357272]\n",
            "1031 [D loss: 0.505792, acc.: 50.13%] [G loss: -0.407918]\n",
            "1032 [D loss: 0.284514, acc.: 50.08%] [G loss: -0.558164]\n",
            "1033 [D loss: 0.256477, acc.: 50.27%] [G loss: -0.471279]\n",
            "1034 [D loss: 0.235165, acc.: 50.51%] [G loss: -1.765228]\n",
            "1035 [D loss: 0.212857, acc.: 50.82%] [G loss: -1.117974]\n",
            "1036 [D loss: 0.251799, acc.: 50.65%] [G loss: -1.932808]\n",
            "1037 [D loss: 0.292201, acc.: 50.47%] [G loss: -0.461019]\n",
            "1038 [D loss: 0.251431, acc.: 50.07%] [G loss: -0.248332]\n",
            "1039 [D loss: 0.254550, acc.: 49.84%] [G loss: -0.121950]\n",
            "1040 [D loss: 0.250873, acc.: 49.82%] [G loss: -0.185316]\n",
            "1041 [D loss: 0.290316, acc.: 50.09%] [G loss: -0.215204]\n",
            "1042 [D loss: 0.323344, acc.: 49.90%] [G loss: -0.275582]\n",
            "1043 [D loss: 0.296800, acc.: 49.58%] [G loss: -0.078791]\n",
            "1044 [D loss: 0.569706, acc.: 49.55%] [G loss: -0.467711]\n",
            "1045 [D loss: 0.392854, acc.: 49.40%] [G loss: -0.351738]\n",
            "1046 [D loss: 0.389428, acc.: 49.80%] [G loss: -0.587888]\n",
            "1047 [D loss: 0.343026, acc.: 49.90%] [G loss: -0.260316]\n",
            "1048 [D loss: 0.306480, acc.: 49.49%] [G loss: -0.145994]\n",
            "1049 [D loss: 0.287521, acc.: 49.29%] [G loss: -0.088056]\n",
            "1050 [D loss: 0.304828, acc.: 49.33%] [G loss: -0.163427]\n",
            "1051 [D loss: 0.352625, acc.: 49.66%] [G loss: -0.217300]\n",
            "1052 [D loss: 0.326343, acc.: 49.76%] [G loss: -0.248173]\n",
            "1053 [D loss: 0.292232, acc.: 49.87%] [G loss: -0.080896]\n",
            "1054 [D loss: 0.830312, acc.: 50.75%] [G loss: -0.338333]\n",
            "1055 [D loss: 0.632540, acc.: 49.90%] [G loss: -0.164970]\n",
            "1056 [D loss: 0.643673, acc.: 48.97%] [G loss: -0.348898]\n",
            "1057 [D loss: 0.529586, acc.: 48.80%] [G loss: -0.223357]\n",
            "1058 [D loss: 0.537409, acc.: 49.05%] [G loss: -0.135800]\n",
            "1059 [D loss: 0.430902, acc.: 49.51%] [G loss: -0.166914]\n",
            "1060 [D loss: 0.507623, acc.: 49.39%] [G loss: -0.126140]\n",
            "1061 [D loss: 0.630555, acc.: 49.68%] [G loss: -0.104947]\n",
            "1062 [D loss: 0.721814, acc.: 49.38%] [G loss: -0.078860]\n",
            "1063 [D loss: 0.842775, acc.: 49.16%] [G loss: -0.075449]\n",
            "1064 [D loss: 1.078436, acc.: 49.35%] [G loss: -0.241508]\n",
            "1065 [D loss: 1.030451, acc.: 49.47%] [G loss: -0.298726]\n",
            "1066 [D loss: 0.582361, acc.: 49.69%] [G loss: -0.343585]\n",
            "1067 [D loss: 0.608457, acc.: 49.41%] [G loss: -0.259990]\n",
            "1068 [D loss: 0.631079, acc.: 49.27%] [G loss: -0.196516]\n",
            "1069 [D loss: 0.705906, acc.: 49.53%] [G loss: -0.261500]\n",
            "1070 [D loss: 0.669521, acc.: 49.85%] [G loss: -0.304115]\n",
            "1071 [D loss: 0.530815, acc.: 50.07%] [G loss: -0.382622]\n",
            "1072 [D loss: 0.730238, acc.: 50.42%] [G loss: -0.301793]\n",
            "1073 [D loss: 0.637905, acc.: 50.65%] [G loss: -0.238580]\n",
            "1074 [D loss: 0.749889, acc.: 50.79%] [G loss: -0.459790]\n",
            "1075 [D loss: 0.645824, acc.: 50.85%] [G loss: -0.448854]\n",
            "1076 [D loss: 0.636575, acc.: 50.93%] [G loss: -0.518929]\n",
            "1077 [D loss: 0.643851, acc.: 51.11%] [G loss: -0.365777]\n",
            "1078 [D loss: 0.589766, acc.: 51.28%] [G loss: -0.445336]\n",
            "1079 [D loss: 0.626207, acc.: 51.23%] [G loss: -0.488030]\n",
            "1080 [D loss: 0.637153, acc.: 50.92%] [G loss: -0.581564]\n",
            "1081 [D loss: 0.512984, acc.: 51.09%] [G loss: -0.436850]\n",
            "1082 [D loss: 0.419749, acc.: 50.79%] [G loss: -0.770624]\n",
            "1083 [D loss: 0.312131, acc.: 50.83%] [G loss: -0.717634]\n",
            "1084 [D loss: 0.408160, acc.: 51.46%] [G loss: -1.400472]\n",
            "1085 [D loss: 0.547267, acc.: 51.81%] [G loss: -0.406549]\n",
            "1086 [D loss: 0.508584, acc.: 50.93%] [G loss: -1.482537]\n",
            "1087 [D loss: 0.330512, acc.: 50.42%] [G loss: -0.634906]\n",
            "1088 [D loss: 0.759999, acc.: 52.88%] [G loss: -0.236711]\n",
            "1089 [D loss: 0.475053, acc.: 52.94%] [G loss: -0.203036]\n",
            "1090 [D loss: 0.425424, acc.: 51.23%] [G loss: -1.038540]\n",
            "1091 [D loss: 0.562916, acc.: 52.90%] [G loss: -0.407335]\n",
            "1092 [D loss: 0.371091, acc.: 50.75%] [G loss: -0.697264]\n",
            "1093 [D loss: 0.303951, acc.: 50.49%] [G loss: -1.072255]\n",
            "1094 [D loss: 0.285457, acc.: 50.55%] [G loss: -1.943486]\n",
            "1095 [D loss: 0.235194, acc.: 50.33%] [G loss: -1.182809]\n",
            "1096 [D loss: 0.339037, acc.: 50.14%] [G loss: -1.411735]\n",
            "1097 [D loss: 0.329344, acc.: 49.97%] [G loss: -0.321132]\n",
            "1098 [D loss: 0.536019, acc.: 50.16%] [G loss: -0.132182]\n",
            "1099 [D loss: 0.577240, acc.: 50.20%] [G loss: -0.135265]\n",
            "1100 [D loss: 0.498825, acc.: 50.38%] [G loss: -0.131835]\n",
            "1101 [D loss: 0.513818, acc.: 50.30%] [G loss: -0.172308]\n",
            "1102 [D loss: 0.495261, acc.: 50.39%] [G loss: -0.116071]\n",
            "1103 [D loss: 0.742494, acc.: 50.07%] [G loss: -0.084255]\n",
            "1104 [D loss: 0.759352, acc.: 49.94%] [G loss: -0.155190]\n",
            "1105 [D loss: 0.628720, acc.: 49.89%] [G loss: -0.113060]\n",
            "1106 [D loss: 0.528987, acc.: 49.77%] [G loss: -0.235793]\n",
            "1107 [D loss: 0.486090, acc.: 49.57%] [G loss: -0.239099]\n",
            "1108 [D loss: 0.630173, acc.: 49.70%] [G loss: -0.186917]\n",
            "1109 [D loss: 0.843876, acc.: 49.88%] [G loss: -0.350832]\n",
            "1110 [D loss: 0.615668, acc.: 49.98%] [G loss: -0.329209]\n",
            "1111 [D loss: 0.716610, acc.: 49.99%] [G loss: -0.359424]\n",
            "1112 [D loss: 0.769472, acc.: 50.07%] [G loss: -0.369479]\n",
            "1113 [D loss: 0.781384, acc.: 50.04%] [G loss: -0.347592]\n",
            "1114 [D loss: 0.804963, acc.: 50.07%] [G loss: -0.404902]\n",
            "1115 [D loss: 0.706932, acc.: 50.04%] [G loss: -0.426500]\n",
            "1116 [D loss: 0.667542, acc.: 50.06%] [G loss: -0.624223]\n",
            "1117 [D loss: 0.651531, acc.: 50.00%] [G loss: -0.673939]\n",
            "1118 [D loss: 0.617342, acc.: 50.22%] [G loss: -0.352789]\n",
            "1119 [D loss: 0.644027, acc.: 50.44%] [G loss: -0.319983]\n",
            "1120 [D loss: 0.681034, acc.: 50.56%] [G loss: -0.358298]\n",
            "1121 [D loss: 0.561317, acc.: 50.60%] [G loss: -0.294493]\n",
            "1122 [D loss: 0.600911, acc.: 50.71%] [G loss: -0.306284]\n",
            "1123 [D loss: 0.542447, acc.: 50.67%] [G loss: -0.298743]\n",
            "1124 [D loss: 0.528156, acc.: 50.56%] [G loss: -0.335321]\n",
            "1125 [D loss: 0.547301, acc.: 50.45%] [G loss: -0.292317]\n",
            "1126 [D loss: 0.567691, acc.: 50.26%] [G loss: -0.448740]\n",
            "1127 [D loss: 0.552384, acc.: 50.11%] [G loss: -0.351097]\n",
            "1128 [D loss: 0.580961, acc.: 50.06%] [G loss: -0.282614]\n",
            "1129 [D loss: 0.591489, acc.: 50.01%] [G loss: -0.432331]\n",
            "1130 [D loss: 0.507832, acc.: 50.22%] [G loss: -0.488122]\n",
            "1131 [D loss: 0.424365, acc.: 50.30%] [G loss: -0.655438]\n",
            "1132 [D loss: 0.473617, acc.: 50.43%] [G loss: -0.433839]\n",
            "1133 [D loss: 0.369091, acc.: 49.96%] [G loss: -0.835175]\n",
            "1134 [D loss: 0.506156, acc.: 50.53%] [G loss: -0.273623]\n",
            "1135 [D loss: 0.402177, acc.: 50.20%] [G loss: -1.092859]\n",
            "1136 [D loss: 0.377014, acc.: 48.94%] [G loss: -1.067027]\n",
            "1137 [D loss: 0.272997, acc.: 50.82%] [G loss: -0.667656]\n",
            "1138 [D loss: 0.294709, acc.: 49.83%] [G loss: -1.262557]\n",
            "1139 [D loss: 0.269603, acc.: 49.61%] [G loss: -0.733160]\n",
            "1140 [D loss: 0.612423, acc.: 50.61%] [G loss: -0.964622]\n",
            "1141 [D loss: 0.546711, acc.: 54.05%] [G loss: -0.078408]\n",
            "1142 [D loss: 0.417447, acc.: 55.38%] [G loss: -0.077605]\n",
            "1143 [D loss: 0.325323, acc.: 54.32%] [G loss: -0.084506]\n",
            "1144 [D loss: 0.296299, acc.: 53.61%] [G loss: -0.116313]\n",
            "1145 [D loss: 0.255886, acc.: 52.78%] [G loss: -0.133890]\n",
            "1146 [D loss: 0.257563, acc.: 54.50%] [G loss: -0.184890]\n",
            "1147 [D loss: 0.258424, acc.: 53.29%] [G loss: -0.207957]\n",
            "1148 [D loss: 0.216330, acc.: 51.97%] [G loss: -0.160536]\n",
            "1149 [D loss: 0.214709, acc.: 51.81%] [G loss: -0.208973]\n",
            "1150 [D loss: 0.208335, acc.: 51.75%] [G loss: -0.118824]\n",
            "1151 [D loss: 0.203253, acc.: 51.58%] [G loss: -0.181846]\n",
            "1152 [D loss: 0.203608, acc.: 52.03%] [G loss: -0.252750]\n",
            "1153 [D loss: 0.203042, acc.: 51.41%] [G loss: -0.244541]\n",
            "1154 [D loss: 0.206323, acc.: 51.51%] [G loss: -0.308906]\n",
            "1155 [D loss: 0.205573, acc.: 51.26%] [G loss: -0.296557]\n",
            "1156 [D loss: 0.204594, acc.: 51.48%] [G loss: -0.309478]\n",
            "1157 [D loss: 0.203250, acc.: 51.25%] [G loss: -0.291479]\n",
            "1158 [D loss: 0.202246, acc.: 51.06%] [G loss: -0.236699]\n",
            "1159 [D loss: 0.207875, acc.: 50.90%] [G loss: -0.263042]\n",
            "1160 [D loss: 0.205972, acc.: 50.97%] [G loss: -0.141748]\n",
            "1161 [D loss: 0.202442, acc.: 51.13%] [G loss: -0.187933]\n",
            "1162 [D loss: 0.199518, acc.: 51.37%] [G loss: -0.246097]\n",
            "1163 [D loss: 0.200401, acc.: 51.15%] [G loss: -0.216827]\n",
            "1164 [D loss: 0.203541, acc.: 51.00%] [G loss: -0.287146]\n",
            "1165 [D loss: 0.207662, acc.: 50.99%] [G loss: -0.267917]\n",
            "1166 [D loss: 0.204392, acc.: 51.06%] [G loss: -0.305770]\n",
            "1167 [D loss: 0.202967, acc.: 50.99%] [G loss: -0.235852]\n",
            "1168 [D loss: 0.212508, acc.: 50.89%] [G loss: -0.207490]\n",
            "1169 [D loss: 0.214380, acc.: 50.84%] [G loss: -0.225777]\n",
            "1170 [D loss: 0.239376, acc.: 50.93%] [G loss: -0.126924]\n",
            "1171 [D loss: 0.206447, acc.: 51.27%] [G loss: -0.143624]\n",
            "1172 [D loss: 0.205538, acc.: 51.32%] [G loss: -0.190100]\n",
            "1173 [D loss: 0.203510, acc.: 51.14%] [G loss: -0.159226]\n",
            "1174 [D loss: 0.211713, acc.: 51.01%] [G loss: -0.215199]\n",
            "1175 [D loss: 0.224088, acc.: 51.01%] [G loss: -0.197779]\n",
            "1176 [D loss: 0.213714, acc.: 50.96%] [G loss: -0.238499]\n",
            "1177 [D loss: 0.208251, acc.: 50.84%] [G loss: -0.172070]\n",
            "1178 [D loss: 0.222836, acc.: 50.84%] [G loss: -0.164993]\n",
            "1179 [D loss: 0.214186, acc.: 50.94%] [G loss: -0.180001]\n",
            "1180 [D loss: 0.250741, acc.: 50.97%] [G loss: -0.104461]\n",
            "1181 [D loss: 0.223889, acc.: 51.35%] [G loss: -0.111047]\n",
            "1182 [D loss: 0.216692, acc.: 51.43%] [G loss: -0.152595]\n",
            "1183 [D loss: 0.208555, acc.: 51.25%] [G loss: -0.134988]\n",
            "1184 [D loss: 0.211640, acc.: 51.06%] [G loss: -0.171508]\n",
            "1185 [D loss: 0.226439, acc.: 51.07%] [G loss: -0.158445]\n",
            "1186 [D loss: 0.217538, acc.: 50.97%] [G loss: -0.191265]\n",
            "1187 [D loss: 0.209358, acc.: 50.82%] [G loss: -0.146767]\n",
            "1188 [D loss: 0.210486, acc.: 50.80%] [G loss: -0.150927]\n",
            "1189 [D loss: 0.206729, acc.: 50.86%] [G loss: -0.170129]\n",
            "1190 [D loss: 0.220942, acc.: 50.83%] [G loss: -0.104010]\n",
            "1191 [D loss: 0.208648, acc.: 51.12%] [G loss: -0.111839]\n",
            "1192 [D loss: 0.203755, acc.: 51.18%] [G loss: -0.156764]\n",
            "1193 [D loss: 0.203463, acc.: 51.01%] [G loss: -0.140180]\n",
            "1194 [D loss: 0.208203, acc.: 50.85%] [G loss: -0.170159]\n",
            "1195 [D loss: 0.228320, acc.: 50.87%] [G loss: -0.157353]\n",
            "1196 [D loss: 0.219707, acc.: 50.82%] [G loss: -0.185831]\n",
            "1197 [D loss: 0.208571, acc.: 50.66%] [G loss: -0.147333]\n",
            "1198 [D loss: 0.222772, acc.: 50.66%] [G loss: -0.148041]\n",
            "1199 [D loss: 0.210064, acc.: 50.73%] [G loss: -0.159606]\n",
            "1200 [D loss: 0.227824, acc.: 50.66%] [G loss: -0.097995]\n",
            "1201 [D loss: 0.219560, acc.: 50.95%] [G loss: -0.105118]\n",
            "1202 [D loss: 0.210812, acc.: 51.03%] [G loss: -0.143196]\n",
            "1203 [D loss: 0.212234, acc.: 50.90%] [G loss: -0.124729]\n",
            "1204 [D loss: 0.217695, acc.: 50.77%] [G loss: -0.145323]\n",
            "1205 [D loss: 0.257977, acc.: 50.72%] [G loss: -0.128537]\n",
            "1206 [D loss: 0.253655, acc.: 50.69%] [G loss: -0.151572]\n",
            "1207 [D loss: 0.220825, acc.: 50.55%] [G loss: -0.129469]\n",
            "1208 [D loss: 0.253391, acc.: 50.54%] [G loss: -0.122449]\n",
            "1209 [D loss: 0.223884, acc.: 50.61%] [G loss: -0.130624]\n",
            "1210 [D loss: 0.231063, acc.: 50.49%] [G loss: -0.086614]\n",
            "1211 [D loss: 0.225471, acc.: 50.79%] [G loss: -0.096336]\n",
            "1212 [D loss: 0.221195, acc.: 50.91%] [G loss: -0.125326]\n",
            "1213 [D loss: 0.223386, acc.: 50.78%] [G loss: -0.107340]\n",
            "1214 [D loss: 0.229040, acc.: 50.66%] [G loss: -0.124388]\n",
            "1215 [D loss: 0.258767, acc.: 50.55%] [G loss: -0.115291]\n",
            "1216 [D loss: 0.275143, acc.: 50.52%] [G loss: -0.139569]\n",
            "1217 [D loss: 0.235722, acc.: 50.35%] [G loss: -0.118659]\n",
            "1218 [D loss: 0.257887, acc.: 50.36%] [G loss: -0.108762]\n",
            "1219 [D loss: 0.284190, acc.: 50.51%] [G loss: -0.122356]\n",
            "1220 [D loss: 0.275732, acc.: 50.38%] [G loss: -0.080309]\n",
            "1221 [D loss: 0.250550, acc.: 50.63%] [G loss: -0.087906]\n",
            "1222 [D loss: 0.240369, acc.: 50.79%] [G loss: -0.110210]\n",
            "1223 [D loss: 0.242548, acc.: 50.67%] [G loss: -0.096648]\n",
            "1224 [D loss: 0.246345, acc.: 50.55%] [G loss: -0.106789]\n",
            "1225 [D loss: 0.304865, acc.: 50.42%] [G loss: -0.099808]\n",
            "1226 [D loss: 0.334901, acc.: 50.34%] [G loss: -0.130081]\n",
            "1227 [D loss: 0.291758, acc.: 50.21%] [G loss: -0.105071]\n",
            "1228 [D loss: 0.297591, acc.: 50.25%] [G loss: -0.092083]\n",
            "1229 [D loss: 0.378637, acc.: 50.39%] [G loss: -0.103222]\n",
            "1230 [D loss: 0.334624, acc.: 50.28%] [G loss: -0.065942]\n",
            "1231 [D loss: 0.348631, acc.: 50.62%] [G loss: -0.081674]\n",
            "1232 [D loss: 0.295334, acc.: 50.73%] [G loss: -0.087314]\n",
            "1233 [D loss: 0.251048, acc.: 50.63%] [G loss: -0.081592]\n",
            "1234 [D loss: 0.271642, acc.: 50.47%] [G loss: -0.093325]\n",
            "1235 [D loss: 0.293956, acc.: 50.40%] [G loss: -0.095517]\n",
            "1236 [D loss: 0.290016, acc.: 50.32%] [G loss: -0.127828]\n",
            "1237 [D loss: 0.432881, acc.: 50.15%] [G loss: -0.083860]\n",
            "1238 [D loss: 0.788709, acc.: 50.30%] [G loss: -0.094545]\n",
            "1239 [D loss: 0.476101, acc.: 50.27%] [G loss: -0.288239]\n",
            "1240 [D loss: 0.793049, acc.: 50.21%] [G loss: -0.119644]\n",
            "1241 [D loss: 0.424408, acc.: 50.73%] [G loss: -0.142555]\n",
            "1242 [D loss: 0.523520, acc.: 50.59%] [G loss: -0.323552]\n",
            "1243 [D loss: 0.345131, acc.: 50.54%] [G loss: -0.756631]\n",
            "1244 [D loss: 1.151144, acc.: 50.48%] [G loss: -0.506310]\n",
            "1245 [D loss: 0.572401, acc.: 50.05%] [G loss: -0.775890]\n",
            "1246 [D loss: 0.410621, acc.: 49.91%] [G loss: -0.459883]\n",
            "1247 [D loss: 0.310963, acc.: 49.06%] [G loss: -0.454444]\n",
            "1248 [D loss: 0.243005, acc.: 48.42%] [G loss: -0.613057]\n",
            "1249 [D loss: 0.291889, acc.: 48.84%] [G loss: -0.469994]\n",
            "1250 [D loss: 0.239056, acc.: 48.94%] [G loss: -0.199295]\n",
            "1251 [D loss: 0.258670, acc.: 49.06%] [G loss: -0.147546]\n",
            "1252 [D loss: 0.226506, acc.: 49.02%] [G loss: -0.286724]\n",
            "1253 [D loss: 0.224313, acc.: 48.94%] [G loss: -0.278990]\n",
            "1254 [D loss: 0.272176, acc.: 48.88%] [G loss: -0.686746]\n",
            "1255 [D loss: 0.332895, acc.: 48.99%] [G loss: -0.303461]\n",
            "1256 [D loss: 0.383880, acc.: 48.72%] [G loss: -0.514970]\n",
            "1257 [D loss: 0.329718, acc.: 48.88%] [G loss: -0.219354]\n",
            "1258 [D loss: 0.347538, acc.: 49.17%] [G loss: -0.098030]\n",
            "1259 [D loss: 0.310880, acc.: 49.10%] [G loss: -0.143646]\n",
            "1260 [D loss: 0.267599, acc.: 49.12%] [G loss: -0.098122]\n",
            "1261 [D loss: 0.258193, acc.: 49.22%] [G loss: -0.117369]\n",
            "1262 [D loss: 0.215526, acc.: 49.21%] [G loss: -0.290298]\n",
            "1263 [D loss: 0.210893, acc.: 49.15%] [G loss: -0.231674]\n",
            "1264 [D loss: 0.248164, acc.: 48.96%] [G loss: -0.569396]\n",
            "1265 [D loss: 0.243838, acc.: 48.98%] [G loss: -0.500906]\n",
            "1266 [D loss: 0.241162, acc.: 48.82%] [G loss: -0.900230]\n",
            "1267 [D loss: 0.234806, acc.: 48.87%] [G loss: -0.432504]\n",
            "1268 [D loss: 0.250144, acc.: 49.00%] [G loss: -0.279677]\n",
            "1269 [D loss: 0.240046, acc.: 49.11%] [G loss: -0.409256]\n",
            "1270 [D loss: 0.233135, acc.: 49.24%] [G loss: -0.255990]\n",
            "1271 [D loss: 0.252484, acc.: 49.49%] [G loss: -0.286805]\n",
            "1272 [D loss: 0.241132, acc.: 49.60%] [G loss: -0.396271]\n",
            "1273 [D loss: 0.274838, acc.: 49.62%] [G loss: -0.153659]\n",
            "1274 [D loss: 0.703881, acc.: 49.76%] [G loss: -0.361770]\n",
            "1275 [D loss: 0.901717, acc.: 49.73%] [G loss: -0.174476]\n",
            "1276 [D loss: 0.852385, acc.: 49.75%] [G loss: -0.661635]\n",
            "1277 [D loss: 0.617070, acc.: 49.57%] [G loss: -0.251746]\n",
            "1278 [D loss: 0.850798, acc.: 49.70%] [G loss: -0.152162]\n",
            "1279 [D loss: 0.784209, acc.: 49.78%] [G loss: -0.227441]\n",
            "1280 [D loss: 0.715025, acc.: 49.81%] [G loss: -0.121148]\n",
            "1281 [D loss: 0.593287, acc.: 50.15%] [G loss: -0.197508]\n",
            "1282 [D loss: 0.532607, acc.: 50.25%] [G loss: -0.177775]\n",
            "1283 [D loss: 0.573059, acc.: 50.33%] [G loss: -0.097425]\n",
            "1284 [D loss: 0.901424, acc.: 50.26%] [G loss: -0.287373]\n",
            "1285 [D loss: 0.854863, acc.: 50.40%] [G loss: -0.252013]\n",
            "1286 [D loss: 0.580544, acc.: 50.29%] [G loss: -0.348491]\n",
            "1287 [D loss: 0.560548, acc.: 50.35%] [G loss: -0.295918]\n",
            "1288 [D loss: 0.562136, acc.: 50.29%] [G loss: -0.277421]\n",
            "1289 [D loss: 0.524753, acc.: 50.00%] [G loss: -0.352555]\n",
            "1290 [D loss: 0.457491, acc.: 49.75%] [G loss: -0.239885]\n",
            "1291 [D loss: 0.359728, acc.: 49.57%] [G loss: -0.320866]\n",
            "1292 [D loss: 0.391608, acc.: 49.54%] [G loss: -0.410066]\n",
            "1293 [D loss: 0.407653, acc.: 49.68%] [G loss: -0.150722]\n",
            "1294 [D loss: 0.637920, acc.: 49.78%] [G loss: -0.368292]\n",
            "1295 [D loss: 0.677237, acc.: 49.66%] [G loss: -0.323835]\n",
            "1296 [D loss: 0.479644, acc.: 49.58%] [G loss: -0.556752]\n",
            "1297 [D loss: 0.497593, acc.: 49.76%] [G loss: -0.432865]\n",
            "1298 [D loss: 0.726589, acc.: 50.08%] [G loss: -0.237627]\n",
            "1299 [D loss: 0.604318, acc.: 50.18%] [G loss: -0.295389]\n",
            "1300 [D loss: 0.655883, acc.: 50.25%] [G loss: -0.312124]\n",
            "1301 [D loss: 0.795224, acc.: 50.43%] [G loss: -0.370790]\n",
            "1302 [D loss: 0.607074, acc.: 50.54%] [G loss: -0.324477]\n",
            "1303 [D loss: 0.639967, acc.: 50.70%] [G loss: -0.300752]\n",
            "1304 [D loss: 0.604732, acc.: 50.79%] [G loss: -0.211411]\n",
            "1305 [D loss: 0.463575, acc.: 50.59%] [G loss: -0.410801]\n",
            "1306 [D loss: 0.364335, acc.: 50.12%] [G loss: -0.982074]\n",
            "1307 [D loss: 0.438772, acc.: 50.18%] [G loss: -0.250823]\n",
            "1308 [D loss: 0.368141, acc.: 50.33%] [G loss: -0.187476]\n",
            "1309 [D loss: 0.316986, acc.: 49.73%] [G loss: -0.688617]\n",
            "1310 [D loss: 0.259995, acc.: 49.66%] [G loss: -0.430451]\n",
            "1311 [D loss: 0.337921, acc.: 49.96%] [G loss: -0.412944]\n",
            "1312 [D loss: 0.337315, acc.: 50.39%] [G loss: -0.254581]\n",
            "1313 [D loss: 0.514518, acc.: 50.42%] [G loss: -0.139846]\n",
            "1314 [D loss: 0.787432, acc.: 50.17%] [G loss: -0.346558]\n",
            "1315 [D loss: 0.715425, acc.: 49.96%] [G loss: -0.268713]\n",
            "1316 [D loss: 0.685556, acc.: 49.80%] [G loss: -0.333438]\n",
            "1317 [D loss: 0.505624, acc.: 49.81%] [G loss: -0.373788]\n",
            "1318 [D loss: 0.454001, acc.: 50.00%] [G loss: -0.310531]\n",
            "1319 [D loss: 0.642819, acc.: 50.14%] [G loss: -0.270378]\n",
            "1320 [D loss: 0.568271, acc.: 50.40%] [G loss: -0.200608]\n",
            "1321 [D loss: 0.554221, acc.: 50.49%] [G loss: -0.315402]\n",
            "1322 [D loss: 0.564478, acc.: 50.78%] [G loss: -0.326978]\n",
            "1323 [D loss: 0.360245, acc.: 50.78%] [G loss: -0.288170]\n",
            "1324 [D loss: 0.303713, acc.: 50.28%] [G loss: -0.604326]\n",
            "1325 [D loss: 0.280136, acc.: 49.98%] [G loss: -0.773850]\n",
            "1326 [D loss: 0.281912, acc.: 49.84%] [G loss: -0.954421]\n",
            "1327 [D loss: 0.308727, acc.: 49.95%] [G loss: -0.264750]\n",
            "1328 [D loss: 0.401103, acc.: 50.23%] [G loss: -0.213873]\n",
            "1329 [D loss: 0.473973, acc.: 50.28%] [G loss: -0.273882]\n",
            "1330 [D loss: 0.537296, acc.: 50.17%] [G loss: -0.131476]\n",
            "1331 [D loss: 0.449473, acc.: 50.13%] [G loss: -0.183389]\n",
            "1332 [D loss: 0.384667, acc.: 50.28%] [G loss: -0.147016]\n",
            "1333 [D loss: 0.459305, acc.: 50.12%] [G loss: -0.122534]\n",
            "1334 [D loss: 0.544755, acc.: 50.05%] [G loss: -0.262548]\n",
            "1335 [D loss: 0.587668, acc.: 50.05%] [G loss: -0.194984]\n",
            "1336 [D loss: 0.512440, acc.: 49.99%] [G loss: -0.341737]\n",
            "1337 [D loss: 0.588011, acc.: 49.96%] [G loss: -0.293312]\n",
            "1338 [D loss: 0.542753, acc.: 50.05%] [G loss: -0.188476]\n",
            "1339 [D loss: 0.460848, acc.: 50.06%] [G loss: -0.247664]\n",
            "1340 [D loss: 0.510337, acc.: 50.10%] [G loss: -0.189512]\n",
            "1341 [D loss: 0.402692, acc.: 50.15%] [G loss: -0.273134]\n",
            "1342 [D loss: 0.478105, acc.: 50.35%] [G loss: -0.175888]\n",
            "1343 [D loss: 0.473177, acc.: 50.39%] [G loss: -0.181665]\n",
            "1344 [D loss: 0.486450, acc.: 50.38%] [G loss: -0.220193]\n",
            "1345 [D loss: 0.519655, acc.: 50.44%] [G loss: -0.258158]\n",
            "1346 [D loss: 0.483035, acc.: 50.22%] [G loss: -0.298364]\n",
            "1347 [D loss: 0.467772, acc.: 50.18%] [G loss: -0.277327]\n",
            "1348 [D loss: 0.423275, acc.: 50.34%] [G loss: -0.228470]\n",
            "1349 [D loss: 0.427628, acc.: 50.29%] [G loss: -0.279412]\n",
            "1350 [D loss: 0.431038, acc.: 50.25%] [G loss: -0.198763]\n",
            "1351 [D loss: 0.361987, acc.: 50.17%] [G loss: -0.223682]\n",
            "1352 [D loss: 0.356210, acc.: 50.10%] [G loss: -0.376853]\n",
            "1353 [D loss: 0.345628, acc.: 50.13%] [G loss: -0.279084]\n",
            "1354 [D loss: 0.338009, acc.: 50.07%] [G loss: -0.486362]\n",
            "1355 [D loss: 0.356751, acc.: 49.84%] [G loss: -0.681809]\n",
            "1356 [D loss: 0.308528, acc.: 49.51%] [G loss: -1.184781]\n",
            "1357 [D loss: 0.256281, acc.: 49.43%] [G loss: -1.151569]\n",
            "1358 [D loss: 0.331273, acc.: 49.84%] [G loss: -0.734080]\n",
            "1359 [D loss: 0.264620, acc.: 49.67%] [G loss: -1.007280]\n",
            "1360 [D loss: 0.286029, acc.: 49.49%] [G loss: -0.891024]\n",
            "1361 [D loss: 0.298268, acc.: 50.44%] [G loss: -0.099045]\n",
            "1362 [D loss: 0.291785, acc.: 51.07%] [G loss: -0.164736]\n",
            "1363 [D loss: 0.263274, acc.: 50.46%] [G loss: -0.179879]\n",
            "1364 [D loss: 0.230960, acc.: 50.00%] [G loss: -0.270289]\n",
            "1365 [D loss: 0.230153, acc.: 50.77%] [G loss: -0.349866]\n",
            "1366 [D loss: 0.327463, acc.: 50.45%] [G loss: -0.333362]\n",
            "1367 [D loss: 0.223209, acc.: 49.95%] [G loss: -0.237073]\n",
            "1368 [D loss: 0.302943, acc.: 50.15%] [G loss: -0.193357]\n",
            "1369 [D loss: 0.300184, acc.: 50.19%] [G loss: -0.149347]\n",
            "1370 [D loss: 0.433870, acc.: 50.03%] [G loss: -0.091695]\n",
            "1371 [D loss: 0.703810, acc.: 50.17%] [G loss: -0.057247]\n",
            "1372 [D loss: 0.690709, acc.: 50.34%] [G loss: -0.113379]\n",
            "1373 [D loss: 0.380464, acc.: 49.88%] [G loss: -0.249147]\n",
            "1374 [D loss: 0.490369, acc.: 50.06%] [G loss: -0.250703]\n",
            "1375 [D loss: 0.639458, acc.: 50.12%] [G loss: -0.126858]\n",
            "1376 [D loss: 0.612903, acc.: 50.12%] [G loss: -0.427582]\n",
            "1377 [D loss: 0.636697, acc.: 50.11%] [G loss: -0.100939]\n",
            "1378 [D loss: 0.577555, acc.: 50.17%] [G loss: -0.092350]\n",
            "1379 [D loss: 0.525229, acc.: 50.27%] [G loss: -0.132596]\n",
            "1380 [D loss: 0.410684, acc.: 50.26%] [G loss: -0.239783]\n",
            "1381 [D loss: 0.317093, acc.: 50.39%] [G loss: -0.234861]\n",
            "1382 [D loss: 0.491925, acc.: 50.33%] [G loss: -0.210467]\n",
            "1383 [D loss: 0.374124, acc.: 50.25%] [G loss: -0.165574]\n",
            "1384 [D loss: 0.361601, acc.: 50.16%] [G loss: -0.430606]\n",
            "1385 [D loss: 0.536125, acc.: 49.98%] [G loss: -0.223973]\n",
            "1386 [D loss: 0.452159, acc.: 49.93%] [G loss: -0.317522]\n",
            "1387 [D loss: 0.458035, acc.: 49.92%] [G loss: -0.255205]\n",
            "1388 [D loss: 0.499250, acc.: 49.85%] [G loss: -0.176781]\n",
            "1389 [D loss: 0.453809, acc.: 49.95%] [G loss: -0.317070]\n",
            "1390 [D loss: 0.456839, acc.: 49.95%] [G loss: -0.288244]\n",
            "1391 [D loss: 0.377666, acc.: 50.14%] [G loss: -0.304474]\n",
            "1392 [D loss: 0.479494, acc.: 50.21%] [G loss: -0.307129]\n",
            "1393 [D loss: 0.442412, acc.: 50.11%] [G loss: -0.335431]\n",
            "1394 [D loss: 0.407376, acc.: 50.12%] [G loss: -0.379460]\n",
            "1395 [D loss: 0.454383, acc.: 49.99%] [G loss: -0.372610]\n",
            "1396 [D loss: 0.423955, acc.: 49.93%] [G loss: -0.399222]\n",
            "1397 [D loss: 0.431199, acc.: 50.05%] [G loss: -0.349064]\n",
            "1398 [D loss: 0.458420, acc.: 50.25%] [G loss: -0.333361]\n",
            "1399 [D loss: 0.408362, acc.: 50.19%] [G loss: -0.318664]\n",
            "1400 [D loss: 0.393256, acc.: 50.11%] [G loss: -0.256571]\n",
            "1401 [D loss: 0.334004, acc.: 50.11%] [G loss: -0.227042]\n",
            "1402 [D loss: 0.327137, acc.: 49.87%] [G loss: -0.173079]\n",
            "1403 [D loss: 0.310383, acc.: 49.82%] [G loss: -0.169364]\n",
            "1404 [D loss: 0.304916, acc.: 49.74%] [G loss: -0.182276]\n",
            "1405 [D loss: 0.304423, acc.: 49.50%] [G loss: -0.184414]\n",
            "1406 [D loss: 0.288900, acc.: 49.47%] [G loss: -0.271523]\n",
            "1407 [D loss: 0.325682, acc.: 49.32%] [G loss: -0.302126]\n",
            "1408 [D loss: 0.361297, acc.: 49.73%] [G loss: -0.417311]\n",
            "1409 [D loss: 0.470378, acc.: 49.62%] [G loss: -0.518946]\n",
            "1410 [D loss: 0.330969, acc.: 49.10%] [G loss: -1.303180]\n",
            "1411 [D loss: 0.300056, acc.: 47.89%] [G loss: -2.476851]\n",
            "1412 [D loss: 0.218782, acc.: 47.61%] [G loss: -3.347951]\n",
            "1413 [D loss: 0.211179, acc.: 47.72%] [G loss: -3.088592]\n",
            "1414 [D loss: 0.333620, acc.: 48.30%] [G loss: -2.756165]\n",
            "1415 [D loss: 0.246587, acc.: 49.24%] [G loss: -2.190190]\n",
            "1416 [D loss: 0.247044, acc.: 49.41%] [G loss: -2.914871]\n",
            "1417 [D loss: 0.515968, acc.: 49.99%] [G loss: -0.585228]\n",
            "1418 [D loss: 0.388118, acc.: 50.16%] [G loss: -0.512510]\n",
            "1419 [D loss: 0.409743, acc.: 50.19%] [G loss: -0.550461]\n",
            "1420 [D loss: 0.348942, acc.: 50.98%] [G loss: -0.399395]\n",
            "1421 [D loss: 0.358943, acc.: 50.74%] [G loss: -0.437814]\n",
            "1422 [D loss: 0.376898, acc.: 49.39%] [G loss: -0.287689]\n",
            "1423 [D loss: 0.235256, acc.: 49.77%] [G loss: -0.763911]\n",
            "1424 [D loss: 0.325474, acc.: 49.93%] [G loss: -0.877420]\n",
            "1425 [D loss: 0.339167, acc.: 50.03%] [G loss: -0.260443]\n",
            "1426 [D loss: 0.399624, acc.: 50.00%] [G loss: -0.450428]\n",
            "1427 [D loss: 0.346776, acc.: 50.04%] [G loss: -0.293498]\n",
            "1428 [D loss: 0.382238, acc.: 50.17%] [G loss: -0.122113]\n",
            "1429 [D loss: 0.365459, acc.: 50.20%] [G loss: -0.161526]\n",
            "1430 [D loss: 0.360182, acc.: 50.19%] [G loss: -0.088310]\n",
            "1431 [D loss: 0.352973, acc.: 50.20%] [G loss: -0.101829]\n",
            "1432 [D loss: 0.302439, acc.: 50.24%] [G loss: -0.087192]\n",
            "1433 [D loss: 0.345176, acc.: 50.22%] [G loss: -0.045583]\n",
            "1434 [D loss: 0.467106, acc.: 50.12%] [G loss: -0.169363]\n",
            "1435 [D loss: 0.444302, acc.: 50.01%] [G loss: -0.121535]\n",
            "1436 [D loss: 0.337581, acc.: 50.00%] [G loss: -0.206466]\n",
            "1437 [D loss: 0.333833, acc.: 49.95%] [G loss: -0.175768]\n",
            "1438 [D loss: 0.344748, acc.: 50.03%] [G loss: -0.131510]\n",
            "1439 [D loss: 0.359794, acc.: 50.09%] [G loss: -0.182834]\n",
            "1440 [D loss: 0.378243, acc.: 50.08%] [G loss: -0.159182]\n",
            "1441 [D loss: 0.314610, acc.: 50.16%] [G loss: -0.238606]\n",
            "1442 [D loss: 0.407860, acc.: 50.18%] [G loss: -0.288523]\n",
            "1443 [D loss: 0.390343, acc.: 50.13%] [G loss: -0.319917]\n",
            "1444 [D loss: 0.432516, acc.: 49.98%] [G loss: -0.348238]\n",
            "1445 [D loss: 0.622723, acc.: 50.04%] [G loss: -0.437946]\n",
            "1446 [D loss: 0.823844, acc.: 50.01%] [G loss: -0.570236]\n",
            "1447 [D loss: 0.526296, acc.: 50.08%] [G loss: -0.705992]\n",
            "1448 [D loss: 0.527463, acc.: 50.15%] [G loss: -0.517067]\n",
            "1449 [D loss: 0.465844, acc.: 50.23%] [G loss: -0.509663]\n",
            "1450 [D loss: 0.430112, acc.: 50.20%] [G loss: -0.401077]\n",
            "1451 [D loss: 0.352817, acc.: 50.16%] [G loss: -0.331792]\n",
            "1452 [D loss: 0.414519, acc.: 50.05%] [G loss: -0.280572]\n",
            "1453 [D loss: 0.340418, acc.: 49.99%] [G loss: -0.287904]\n",
            "1454 [D loss: 0.345742, acc.: 49.94%] [G loss: -0.254865]\n",
            "1455 [D loss: 0.358196, acc.: 49.54%] [G loss: -0.231036]\n",
            "1456 [D loss: 0.340533, acc.: 49.52%] [G loss: -0.208215]\n",
            "1457 [D loss: 0.317526, acc.: 49.37%] [G loss: -0.196512]\n",
            "1458 [D loss: 0.268685, acc.: 49.27%] [G loss: -0.182897]\n",
            "1459 [D loss: 0.264925, acc.: 49.31%] [G loss: -0.210431]\n",
            "1460 [D loss: 0.253400, acc.: 49.31%] [G loss: -0.290571]\n",
            "1461 [D loss: 0.248870, acc.: 49.17%] [G loss: -0.485486]\n",
            "1462 [D loss: 0.233280, acc.: 48.93%] [G loss: -1.361970]\n",
            "1463 [D loss: 0.228797, acc.: 48.78%] [G loss: -2.269579]\n",
            "1464 [D loss: 0.226938, acc.: 48.83%] [G loss: -3.346118]\n",
            "1465 [D loss: 0.417426, acc.: 48.40%] [G loss: -inf]\n",
            "1466 [D loss: 7.096354, acc.: 50.00%] [G loss: -inf]\n",
            "1467 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1468 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1469 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1470 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1471 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1472 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1473 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1474 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1475 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1476 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1477 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1478 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1479 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1480 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1481 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1482 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1483 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1484 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1485 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1486 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1487 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1488 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1489 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1490 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1491 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1492 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1493 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1494 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1495 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1496 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1497 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1498 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1499 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1500 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1501 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1502 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1503 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1504 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1505 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1506 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1507 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1508 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1509 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1510 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1511 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1512 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1513 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1514 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1515 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1516 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1517 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1518 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1519 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1520 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1521 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1522 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1523 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1524 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1525 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1526 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1527 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1528 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1529 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1530 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1531 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1532 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1533 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1534 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1535 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1536 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1537 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1538 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1539 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1540 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1541 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1542 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1543 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1544 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1545 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1546 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1547 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1548 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1549 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1550 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1551 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1552 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1553 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1554 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1555 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1556 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1557 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1558 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1559 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1560 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1561 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1562 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1563 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1564 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1565 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1566 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1567 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1568 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1569 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1570 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1571 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1572 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1573 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1574 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1575 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1576 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1577 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1578 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1579 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1580 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1581 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1582 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1583 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1584 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1585 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1586 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1587 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1588 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1589 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1590 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1591 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1592 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1593 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1594 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1595 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1596 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1597 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1598 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1599 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1600 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1601 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1602 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1603 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1604 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1605 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1606 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1607 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1608 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1609 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1610 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1611 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1612 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1613 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1614 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1615 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1616 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1617 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1618 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1619 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1620 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1621 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1622 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1623 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1624 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1625 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1626 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1627 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1628 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1629 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1630 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1631 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1632 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1633 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1634 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1635 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1636 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1637 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1638 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1639 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1640 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1641 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1642 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1643 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1644 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1645 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1646 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1647 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1648 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1649 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1650 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1651 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1652 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1653 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1654 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1655 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1656 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1657 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1658 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1659 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1660 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1661 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1662 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1663 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1664 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1665 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1666 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1667 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1668 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1669 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1670 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1671 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1672 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1673 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1674 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1675 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1676 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1677 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1678 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1679 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1680 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1681 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1682 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1683 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1684 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1685 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1686 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1687 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1688 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1689 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1690 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1691 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1692 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1693 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1694 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1695 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1696 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1697 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1698 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1699 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1700 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1701 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1702 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1703 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1704 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1705 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1706 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1707 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1708 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1709 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1710 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1711 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1712 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1713 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1714 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1715 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1716 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1717 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1718 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1719 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1720 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1721 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1722 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1723 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1724 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1725 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1726 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1727 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1728 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1729 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1730 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1731 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1732 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1733 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1734 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1735 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1736 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1737 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1738 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1739 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1740 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1741 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1742 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1743 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1744 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1745 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1746 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1747 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1748 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1749 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1750 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1751 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1752 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1753 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1754 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1755 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1756 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1757 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1758 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1759 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1760 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1761 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1762 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1763 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1764 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1765 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1766 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1767 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1768 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1769 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1770 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1771 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1772 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1773 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1774 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1775 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1776 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1777 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1778 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1779 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1780 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1781 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1782 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1783 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1784 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1785 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1786 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1787 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1788 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n",
            "1789 [D loss: 6.966687, acc.: 50.00%] [G loss: -inf]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-7f5e7377ef94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/test_images.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-34cdf24fbc99>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# Generator training : try to make generated images be classified as true by the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstacked_gen_disc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_random\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;31m# increase epoch counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "po1Im1Qg1NWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gan.sample_images('images/test_images2_12.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kRuuKf9MFzgu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "im = Image.open(\"drive/My Drive/texture/patchset3/patchno\"+str(0)+\".jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o6uUaXwsF6Sl",
        "colab_type": "code",
        "outputId": "17b843fc-51fc-4487-fecf-3326095cfef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(im)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f408db00b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wGwrxWfLGLyR",
        "colab_type": "code",
        "outputId": "438b9f6e-f150-4680-d24e-4f53ac21db4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.array(im).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "daKr6S4XKvV3",
        "colab_type": "code",
        "outputId": "48d04ccc-826d-4e42-b2d8-ec5e3b42a131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.max(im)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "HShiehElZF98",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}